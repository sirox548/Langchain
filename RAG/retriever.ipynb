{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff57060e-7263-4946-9c06-d4025ebb58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Loder\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"machine-learning-elixir-nx-axon.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1945b2fd-2d09-482e-af3c-a47ed9c93e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 0}),\n",
       " Document(page_content='Machine Learning in Elixir\\nLearning to Learn with Nx and Axon\\nby Sean Moriarity\\nVersion: P1.0 (September 2024)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 1}),\n",
       " Document(page_content=\"Copyright © 2024 The Pragmatic Programmers, LLC. This book is licensed to the individual who purchased it. We don't copy-\\nprotect it because that would limit your ability to use it for your own purposes. Please don't break this trust—you can use this\\nacross all of your devices but please do not share this copy with other members of your team, with friends, or via file sharing\\nservices. Thanks.\\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those\\ndesignations appear in this book, and The Pragmatic Programmers, LLC was aware of a trademark claim, the designations have\\nbeen printed in initial capital letters or in all capitals. The Pragmatic Starter Kit, The Pragmatic Programmer, Pragmatic\\nProgramming, Pragmatic Bookshelf and the linking g device are trademarks of The Pragmatic Programmers, LLC.\\nEvery precaution was taken in the preparation of this book. However, the publisher assumes no responsibility for errors or\\nomissions, or for damages that may result from the use of information (including program listings) contained herein.\\nAbout the Pragmatic Bookshelf\\nThe Pragmatic Bookshelf is an agile publishing company. We’re here because we want to improve the lives of developers. We do\\nthis by creating timely, practical titles, written by programmers for programmers.\\nOur Pragmatic courses, workshops, and other products can help you and your team create better software and have more fun. For\\nmore information, as well as the latest Pragmatic titles, please visit us at http://pragprog.com.\\nOur ebooks do not contain any Digital Restrictions Management, and have always been DRM-free. We pioneered the beta book\\nconcept, where you can purchase and read a book while it’s still being written, and provide feedback to the author to help make a\\nbetter book for everyone. Free resources for all purchasers include source code downloads (if applicable), errata and discussion\\nforums, all available on the book's home page at pragprog.com. We’re here to make your life easier.\\nNew Book Announcements\\nWant to keep up on our latest titles and announcements, and occasional special offers? Just create an account on pragprog.com (an\\nemail address and a password is all it takes) and select the checkbox to receive newsletters. You can also follow us on twitter as\\n@pragprog.\\nAbout Ebook Formats\\nIf you buy directly from pragprog.com, you get ebooks in all available formats for one price. You can synch your ebooks amongst\\nall your devices (including iPhone/iPad, Android, laptops, etc.) via Dropbox. You get free updates for the life of the edition. And,\\nof course, you can always come back and re-download your books when needed. Ebooks bought from the Amazon Kindle store\\nare subject to Amazon's polices. Limitations in Amazon's file format may cause ebooks to display differently on different devices.\\nFor more information, please see our FAQ at pragprog.com/#about-ebooks. To learn more about this book and access the free\\nresources, go to https://pragprog.com/book/smelixir, the book's homepage.\", metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 2}),\n",
       " Document(page_content='Thanks for your continued support,\\nThe Pragmatic Bookshelf\\nThe team that produced this book includes: Dave Thomas (Publisher), Janet Furlow (COO),\\nSusannah Davidson (Executive Editor), Tammy Coron (Development Editor), Corina Lebegioara (Copy Editor),\\nPotomac Indexing, LLC (Indexing), Gilson Graphics (Layout)\\nFor customer support, please contact support@pragprog.com.\\nFor international rights, please contact rights@pragprog.com.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 3}),\n",
       " Document(page_content='To Riley, Weston, and Indy.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 4}),\n",
       " Document(page_content='Table of Contents\\n1. Disclaimer\\n2. Acknowledgments\\n3. Preface\\n1. Why Elixir for Machine Learning?\\n2. Who This Book Is For\\n3. What’s in This Book\\n4. How to Use This Book\\n4. Part I. Foundations of Machine Learning\\n1. 1. Make Machines That Learn\\n1. Classifying Flowers\\n2. Learning with Elixir\\n3. Wrapping Up\\n2. 2. Get Comfortable with Nx\\n1. Thinking in Tensors\\n2. Using Nx Operations', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 5}),\n",
       " Document(page_content='3. Representing the World\\n4. Going from def to defn\\n5. Wrapping Up\\n3. 3. Harness the Power of Math\\n1. Understanding Machine Learning Math\\n2. Speaking the Language of Data\\n3. Thinking Probabilistically\\n4. Tracking Change\\n5. Wrapping Up\\n4. 4. Optimize Everything\\n1. Learning with Optimization\\n2. Regularizing to Generalize\\n3. Descending Gradients\\n4. Peering into the Black Box\\n5. Wrapping Up\\n5. 5. Traditional Machine Learning\\n1. Learning Linearly\\n2. Learning from Your Surroundings\\n3. Using Clustering', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 6}),\n",
       " Document(page_content='4. Making Decisions\\n5. Wrapping Up\\n5. Part II. Deep Learning\\n1. 6. Go Deep with Axon\\n1. Understanding the Need for Deep Learning\\n2. Breaking Down a Neural Network\\n3. Creating Neural Networks with Axon\\n4. Wrapping Up\\n2. 7. Learn to See\\n1. Identifying Cats and Dogs\\n2. Introducing Convolutional Neural Networks\\n3. Improving the Training Process\\n4. Going Beyond Image Classification\\n5. Wrapping Up\\n3. 8. Stop Reinventing the Wheel\\n1. Identifying Cats and Dogs Again\\n2. Fine-Tuning Your Model\\n3. Understanding Transfer Learning', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 7}),\n",
       " Document(page_content='4. Taking Advantage of the Machine Learning Ecosystem\\n5. Wrapping Up\\n4. 9. Understand Text\\n1. Classifying Movie Reviews\\n2. Introducing Recurrent Neural Networks\\n3. Understanding Recurrent Neural Networks\\n4. Wrapping Up\\n5. 10. Forecast the Future\\n1. Predicting Stock Prices\\n2. Using CNNs for Single-Step Prediction\\n3. Using RNNs for Time-Series Prediction\\n4. Tempering Expectations\\n5. Wrapping Up\\n6. 11. Model Everything with Transformers\\n1. Paying Attention\\n2. Going from RNNs to Transformers\\n3. Using Transformers with Bumblebee\\n4. Wrapping Up', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 8}),\n",
       " Document(page_content='7. 12. Learn Without Supervision\\n1. Compressing Data with Autoencoders\\n2. Learning a Structured Latent\\n3. Generating with GANs\\n4. Learning Without Supervision in Practice\\n5. Wrapping Up\\n6. Part III. Machine Learning in Practice\\n1. 13. Put Machine Learning into Practice\\n1. Deciding to Use Machine Learning\\n2. Setting Up the Application\\n3. Integrating Nx with Phoenix\\n4. Seeding Your Databases\\n5. Building the Search LiveView\\n6. Wrapping Up\\n2. 14. That’s a Wrap\\n1. Learning from Experience\\n2. Diffusing Innovation\\n3. Talking to Large Language Models', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 9}),\n",
       " Document(page_content='4. Compressing Knowledge\\n5. Moving Forward\\n7. Bibliography\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 10}),\n",
       " Document(page_content='Disclaimer\\n\\xa0\\nThe views expressed in this publication are those of the author and do not\\nnecessarily reflect the official policy or position of the Department of\\nDefense or the U.S. government. The public release clearance of this\\npublication by the Department of Defense does not imply Department of\\nDefense endorsement or factual accuracy of the material.\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 11}),\n",
       " Document(page_content='Early Praise for Machine Learning in\\nElixir\\nThere is no better person to teach Machine Learning in Elixir than the\\nperson who started it all: Sean Moriarity. Regardless, if you are new to\\nElixir or new to Machine Learning, this book has you covered with plenty\\nof concepts, examples, and tools.\\n→José Valim\\nCreator of Elixir, Dashbit\\nMachine Learning in Elixir is a superbly structured dive into the world of\\nML in Elixir. Sean has put tremendous care into making the learning\\nexperience as streamlined and engaging as possible. I found myself eagerly\\nfollowing along with practical examples in Livebook that made the\\ninformation jump off the page and into my hands.\\n→Brooklin Myers\\nElixir Instructor, DockYard\\nFrom basic concepts to building real-world applications, Machine Learning\\nin Elixir guides you through the complex world of AI, with practical\\ninsights to make you a proficient developer in today’s AI market.\\n→Nicholas Franck', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 12}),\n",
       " Document(page_content='Software Developer, Independent', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 13}),\n",
       " Document(page_content='Acknowledgments\\n\\xa0\\nI would first like to acknowledge José Valim for creating such a wonderful\\nlanguage and placing faith in me at the beginning of my open-source career.\\nThis book and the projects within would not exist without him. I would also\\nlike to acknowledge Paulo Valente, Jonatan Kłosko, and Christopher\\nGrainger for the many design discussions and debugging sessions that\\nhelped shape the projects in this book. Finally, I would like to thank Noah\\nDeMoes, Leo Kosta, Scott Mueller, Nick Franck, Mike Binns, Brooklin\\nMyers, Jonatan Kłosko (again), and José Valim (again) for their valuable\\nfeedback on the contents of this book.\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 14}),\n",
       " Document(page_content='Preface\\n\\xa0\\nMachine learning, specifically deep learning, is constantly pushing the\\nboundaries of what we thought was possible—and it’s doing it in every\\nindustry.\\nSeemingly every day, a company or research group releases a new model\\nthat pushes the state of the art even further ahead. In recent years, ChatGPT\\nand Stable Diffusion, among others, have taken the world by storm,\\nbringing artificial intelligence to the forefront and redefining what types of\\napplications are possible.\\nFor most of their existence, Elixir and the BEAM weren’t viable options for\\nmachine learning tooling. But the Nx ecosystem has changed that. Within\\nthis ecosystem, you can now write machine learning and numerical routines\\ndirectly in Elixir and achieve performance that is the same as or better than\\nan equivalent program in Python or Julia. Nx offers new capabilities to\\nElixir programmers. It also provides an off-ramp for existing machine\\nlearning engineers and researchers looking to explore ecosystems and build\\napplications without Python.\\nMy goal is for this book to serve as the definitive Nx and machine learning\\nresource for programmers who want to explore machine learning in Elixir.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 15}),\n",
       " Document(page_content='While a lot of machine learning knowledge is available online, none of the\\nexisting resources are written with Elixir in mind. This book is designed to\\nteach machine learning the Elixir way—functional, pragmatic, and fun.\\nYou’ll start with the basics of machine learning and Nx, and you’ll build up\\nto deploying powerful pre-trained models in an application with Phoenix.\\nThroughout this book, you’ll use Livebook—Elixir’s interactive code\\nnotebook—to work with data and train models in an interactive and\\nreproducible way. By the end of the book, you’ll have experience working\\nwith Nx, Axon, Scholar, Bumblebee, Explorer, VegaLite, and many more\\nlibraries in the growing Elixir machine learning ecosystem.\\nWe’ve finally made it to the always-promised but never-delivered golden\\nage of artificial intelligence-enabled products. So, if you want to build\\npowerful, intelligent applications augmented with machine learning models,\\nElixir is one of the best languages to use.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 16}),\n",
       " Document(page_content='Why Elixir for Machine Learning?\\nElixir is a dynamic, functional, general-purpose programming language that\\ntargets the BEAM virtual machine. It’s proven itself as a capable language\\nfor building and maintaining scalable applications. But Elixir wasn’t\\ndesigned with machine learning in mind. Prior to the Nx project, machine\\nlearning in Elixir was, at the very least, ill-advised. The BEAM virtual\\nmachine wasn’t designed to perform the type of heavy lifting that machine\\nlearning requires. However, Nx and the ecosystem around it were built to\\nenable Elixir programmers to perform the computationally expensive work\\nrequired for machine learning applications.\\nElixir and Nx are new players in the machine learning field. While the Nx\\necosystem has no aim to overtake the established Python machine learning\\necosystem, understanding machine learning in Elixir has numerous benefits.\\nFirst, Elixir is a functional programming language. While this might seem\\nlike a drawback for the heavy computation required in machine learning\\nworkloads, it’s actually a benefit. In a functional world, everything is\\nimmutable. This means that when performing computations on large\\namounts of data, you need to return a new copy of the data after every\\nindividual operation. Fortunately, due to the rise of deep learning,\\ncomputations are staged out to hardware accelerators such as GPUs before\\nany of the actual operations are executed. These computations are staged\\nout with just-in-time (JIT) compilers, which typically implement a\\n[1]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 17}),\n",
       " Document(page_content='functional interface for building up computation graphs. As you’ll see in\\nlater chapters, you can write in Elixir’s functional style and not worry about\\nlarge intermediate copies.\\nSecond, Elixir is designed for concurrency. When training machine learning\\nalgorithms, the performance bottleneck is typically in loading and preparing\\ndata for a training step on a hardware accelerator. This type of concurrent\\ndata processing is natural in Elixir because Elixir was designed with\\nconcurrency in mind. You need only to read Concurrent Data Processing in\\nElixir [Gos21] to see how good a fit Elixir is for those types of applications.\\nThird, Elixir and its web framework, Phoenix, are exceptional at building\\nand scaling real-time, fault-tolerant applications. More and more machine\\nlearning systems are being deployed in scenarios where latency and uptime\\nmatter. For example, a credit card fraud detection system should ideally\\nwork in real time, immediately alerting customers when a fraudulent\\ntransaction occurs. Additionally, with large amounts of fraudulent\\ntransactions taking place every day, those systems can’t go down. Elixir and\\nPhoenix have proven capable of building systems that are low-latency and\\nresilient to failure. But until now, integrating machine learning in an Elixir\\napplication required leaving the safety of the BEAM to an external service.\\nFinally, Elixir is a lot of fun and an absolute joy to use. As you’ll see\\nthroughout this book, most machine learning algorithms are naturally', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 18}),\n",
       " Document(page_content='expressed with functional constructs that work particularly well with the\\nbeauty and thoughtfulness of the Elixir programming language.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 19}),\n",
       " Document(page_content='Who This Book Is For\\nThis book is for programmers interested in learning machine learning in\\nElixir and programmers with prior Elixir or machine learning experience.\\nElixir programmers can pick up and read this book with no prerequisite\\nknowledge of machine learning, mathematics, or numerical computing, as\\nthis book will teach you everything you need to know from the basics\\nonward.\\nProgrammers with machine learning experience but no Elixir experience\\nwill be able to follow along, but some syntactic details and functional\\nconcepts might seem unfamiliar. If you fit into this category, I recommend\\nkeeping the Elixir documentation handy to reference some language details\\nthat this book omits.\\nIf you have no machine learning or Elixir experience, then learning both\\nElixir and machine learning at the same time will be a difficult task. So I\\nrecommend getting familiar with Elixir first and then returning to this book\\nto learn machine learning in Elixir.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 20}),\n",
       " Document(page_content='What’s in This Book\\nIn Chapter 1,  Make Machines That Learn , you’ll answer the question,\\n“What is machine learning?” You’ll learn the fundamental concepts of\\nmachine learning and set the course for the rest of your machine learning\\njourney. You’ll create, train, and use your first machine learning model. By\\nthe end of the chapter, you’ll be able to define machine learning, discuss the\\ntypes of problems machine learning is good for, and solve a problem with\\nmachine learning in Elixir.\\nIn Chapter 2,  Get Comfortable with Nx , you’ll get familiar with Nx—the\\nnumerical computing library that serves as the backbone of the Elixir\\nmachine learning ecosystem. You’ll learn about the fundamental data\\nstructure in numerical computing, the tensor, and a bit about what makes\\nNx so important for machine learning. By the end of this chapter, you’ll\\nknow how to create and manipulate tensors, encode real-world data with\\nNx, and accelerate routines with numerical definitions.\\nIn Chapter 3,  Harness the Power of Math , you’ll channel your inner\\nmathematician and dive into machine learning math. You’ll learn the three\\nareas of mathematics that underpin most of the concepts in this book: linear\\nalgebra, probability, and vector calculus. By the end of this chapter, you’ll\\nunderstand the power of mathematics and how it relates to machine\\nlearning.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 21}),\n",
       " Document(page_content='In Chapter 4,  Optimize Everything , you’ll start navigating an ocean of loss\\ncurves with optimization routines. You’ll learn what a loss function is and\\nhow optimization and machine learning are related and different. By the end\\nof this chapter, you’ll be able to formulate machine learning problems as\\noptimization problems. You’ll also be able to implement some loss\\nfunctions and optimization algorithms in Nx.\\nIn Chapter 5,  Traditional Machine Learning , you’ll graduate from machine\\nlearning’s foundations to real machine learning with Scholar. You’ll learn\\nabout various traditional machine learning algorithms, such as linear and\\nlogistic regression. By the end of this chapter, you’ll be able to use Scholar\\nto create and train traditional machine learning algorithms.\\nIn Chapter 6,  Go Deep with Axon , you’ll start working with Axon to create\\nand train neural networks in Elixir. You’ll learn what deep learning is and\\nwhy it’s so powerful. You’ll create neural networks in both Nx and Axon,\\nand you’ll get a better understanding of what a neural network really is. By\\nthe end of this chapter, you’ll know how to create and train basic neural\\nnetworks with Axon. You’ll also be able to distinguish and discuss the\\ntrade-offs between traditional and deep learning methods.\\nIn Chapter 7,  Learn to See , you’ll implement computer vision models with\\nAxon and Elixir. You’ll learn about convolutional neural networks and how\\nthey offer an improvement over traditional neural networks when working\\nwith image data. By the end of this chapter, you’ll be able to create and', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 22}),\n",
       " Document(page_content='train a convolutional neural network in Axon. You’ll also be able to\\nimplement more complex machine learning training pipelines with Elixir’s\\nstandard libraries.\\nIn Chapter 8,  Stop Reinventing the Wheel , you’ll make use of pre-trained\\nmodels to improve performance and save time and money. You’ll learn\\nabout transfer learning and fine-tuning. By the end of this chapter, you’ll be\\nable to implement transfer learning in Axon. You’ll also know how to\\nconvert models from Python to Elixir to take advantage of Python’s vast\\nmachine learning ecosystem.\\nIn Chapter 9,  Understand Text , you’ll teach your programs to read with\\nrecurrent neural networks, and you’ll learn how to work with text in Elixir.\\nBy the end of this chapter, you’ll be able to create and train recurrent neural\\nnetworks in Axon, define and use various text preprocessing strategies with\\nElixir and Nx, and discuss why recurrent neural networks perform better\\nthan traditional models when working with sequential data-like text.\\nIn Chapter 10,  Forecast the Future , you’ll break out your crystal ball and\\nattempt to peer into the future with recurrent neural networks. You’ll learn\\nabout time-series forecasting and some of the challenges of predicting the\\nfuture with machine learning. By the end of this chapter, you’ll be able to\\ncreate and train a recurrent neural network to perform single-step time-\\nseries predictions.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 23}),\n",
       " Document(page_content='In Chapter 11,  Model Everything with Transformers, you’ll unlock the\\npower behind models like GPT-3 and Stable Diffusion with transformers.\\nYou’ll learn what transformers are and why they perform so well on almost\\nall types of data. By the end of this chapter, you’ll be able to use pre-trained\\ntransformer models with Bumblebee. You’ll also know how to fine-tune a\\npre-trained transformer with Bumblebee and Axon.\\nIn Chapter 12,  Learn Without Supervision , you’ll go off the rails and create\\nmodels that learn without supervision. You’ll learn about unsupervised\\nlearning and generative modeling. By the end of this chapter, you’ll be able\\nto create and train autoencoders, variational autoencoders, and generative\\nadversarial networks.\\nIn Chapter 13,  Put Machine Learning into Practice , you’ll use everything\\nyou learned throughout this book, and you’ll put a real machine learning\\nmodel into a production application. You’ll learn what to consider when\\noperationalizing machine learning models, and you’ll deploy a model in a\\nPhoenix application. By the end of this chapter, you’ll know how to use\\nNx.Serving to integrate your trained models into real applications.\\nIn Chapter 14,  That’s a Wrap , you’ll conclude your journey with this book.\\nYou’ll get a recipe for training models in practice, and you’ll get resources\\nfor staying up to date with the latest trends in machine learning. You’ll also\\nlearn a bit about emerging trends in the field, as well as some topics this\\nbook didn’t cover, such as adversarial attacks. By the end of this chapter,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 24}),\n",
       " Document(page_content='you’ll be ready to build intelligent applications with machine learning in\\nElixir.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 25}),\n",
       " Document(page_content='[1]\\nHow to Use This Book\\nThe code examples in each chapter of this book are isolated and designed to\\nbe run as standalone programs. For the best experience, read the chapters in\\nsuccessive order. While you can skip around, the concepts in each chapter\\nare designed to build off one another, so you might be left confused at\\ncertain points if you skip around too much.\\nIf you have some machine learning experience and want to dive right in,\\nyou can safely skip Chapter 1,  Make Machines That Learn , Chapter 3,  \\nHarness the Power of Math , and Chapter 4,  Optimize Everything . I still\\nrecommend reading Chapter 2,  Get Comfortable with Nx , to get a better\\nunderstanding of Nx and then reading the remaining chapters in order.\\nI also recommend you follow along with the examples in a Livebook as\\nmost of the chapters will assume you’re running the code in a Livebook\\ninstallation.\\nFinally, don’t worry about having access to CPUs or GPUs. A modern,\\ncommercial laptop should be powerful enough to run all the examples in\\nthis book.\\nFOOTNOTES\\nhttps://github.com/elixir-nx/nx', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 26}),\n",
       " Document(page_content='Copyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 27}),\n",
       " Document(page_content='Part 1\\nFoundations of Machine Learning', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 28}),\n",
       " Document(page_content='Chapter 1\\nMake Machines That Learn\\n\\xa0\\nIn 1958, Frank Rosenblatt, then a researcher at Cornell, revealed \"the first\\nmachine which [was] capable of having an original idea.\" [Lef19]. At the\\ntime, Rosenblatt’s primitive machine, which learned to identify markings on\\npunch cards, was dubbed a \"Frankenstein Monster Designed by Navy That\\nThinks.\" [Met21]. Despite the hype, it would take another 60 years of peaks\\nand valleys for artificial intelligence and machine learning to prevail over\\nhumans in feats of intelligence.\\nIn 2016, the artificial intelligence company, DeepMind, designed AlphaGo\\n—a model capable of achieving superhuman performance in the game of\\nGo. AlphaGo subsequently beat Go champions Lee Sedol and Ke Jie on a\\nworld stage, creating a resurgence in the promise of machine learning and\\nartificial intelligence. Only a handful of years later, machine learning\\nmodels, such as GPT-3 and Stable Diffusion, blur the lines between science\\nfiction and reality, demonstrating both impressive feats of seeming\\nintelligence and laughable shortcomings. While models such as GPT-3 and\\nStable Diffusion are impressive—and on the surface, they seem like feats of\\nscience outside the reach of nonacademics and researchers—the truth is', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 29}),\n",
       " Document(page_content='these models build on the primitive foundations laid by Rosenblatt and\\nresearchers like him.\\nPerhaps more impressive than models like GPT-3 and Stable Diffusion are\\nthe countless models that drive billions of dollars in economic production\\nevery year. At every stage and in every industry, from startups to Forbes\\n500, retail to pharmaceuticals, companies are taking advantage of machine\\nlearning to accelerate products and move their industries forward.\\nThroughout this book, you’ll learn the foundations of machine learning in\\nElixir by building some of the most fundamental machine learning models.\\nYou’ll work through challenging problems and think critically about how to\\napproach machine learning problems. Additionally, you’ll see hands-on\\nhow to use machine learning in production. But first, let’s answer the\\nquestion, “What is machine learning” by working through an example.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 30}),\n",
       " Document(page_content='Classifying Flowers\\nImagine you’ve been hired by a botanist. Your job is to automate the\\nclassification of the Iris genus into one class from a set of species. You must\\nalso prove that your system has a minimum success rate of 85%.\\nThe botanist has already automated the process of collecting and measuring\\nthe flowers but cannot invest time into classifying each one individually.\\nInstead, they’ve given you a dataset consisting of 50 examples each of the\\nseneca, versicolor, and virginica species of the Iris genus—150 examples\\ntotal. Each example contains the sepal length (cm), sepal width (cm), petal\\nlength (cm), petal width (cm), and species.\\nWith a clear mission and some data in hand, it’s time to write your first\\nmachine learning algorithm.\\nScoping the Project\\nBefore writing the first line of code, it’s always a good idea to sit down and\\nclearly define the criteria for your project. Defining criteria for a machine\\nlearning problem is a bit different from defining criteria for a software\\nproject.\\nFirst, you need to understand what it means for a machine to learn. In his\\n1997 book, Machine Learning [Mit97], Tom Mitchell writes, “A computer\\nprogram is said to learn from experience E with respect to some class of', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 31}),\n",
       " Document(page_content='tasks T and performance measure P, if its performance at tasks T, as\\nmeasured by P improves with experiences E.”\\nSo, for a machine to learn, it requires three main components:\\n1. Task\\n2. Performance measure\\n3. Experience\\nDefining the Task\\nA task is what you want your model to do. A model is a predictive function.\\nA model takes inputs and produces outputs. For example, if you want your\\nmodel to differentiate between pictures of apples and oranges, the model’s\\ntask is to differentiate between pictures of apples and oranges.\\nIn machine learning, tasks are generally too complex to solve using a small\\nset of rules. While you could probably give a model the task of sorting\\nnumbers in a list, it’s not something you would generally do. You could\\nsimply write a sorting algorithm in a few minutes using a small set of rules.\\nThe tasks you ask machine learning algorithms to solve are the ones you\\nmight consider easy but are far too complex to handle using a small number\\nof simple rules. Consider the apples and oranges differentiator. What small\\nset of rules would you implement to differentiate between pictures of apples\\nand oranges? Would checking only the color be enough? What happens', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 32}),\n",
       " Document(page_content='when you evaluate red oranges and orange apples? What rule would\\ndifferentiate between those two varieties?\\nAs you attempt to think of rules, you’ll quickly find exceptions to them—\\nand then exceptions to exceptions, and so on. A good rule of thumb to\\nfollow is that if you can’t perform reasonably well with a few simple rules,\\nmachine learning might be a good fit for your problem.\\nKnowing Your Type\\nYou can frame most machine learning tasks into one of two major\\ncategories:\\n1. Classification\\n2. Regression\\nWhile you might see some tasks thrown into their own distinct categories,\\nsuch as generative modeling—the process of learning to generate realistic\\ndata—you can mostly formulate solutions to every machine learning\\nproblem in terms of classification and regression subtasks.\\nClassification problems are concerned with assigning labels to inputs.\\nThese labels are discrete, drawn from a set of categories such as “apples”\\nand “oranges” or whole-numbers such as 0, 1, 2, 3, and so on. Your apples\\nand oranges differentiator is an example of a classification task. Similarly,\\nyou can define fraud detection as a classification task.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 33}),\n",
       " Document(page_content='Regression problems are concerned with assigning numeric values to inputs.\\nThese values are continuous and can occupy any real number value. A good\\nexample of a regression task is predicting housing prices from a set of\\nhouse features or predicting a stock’s future price from the current\\ninformation available.\\nDefining your problem in terms of one of these two types of tasks will help\\nyou decide what kind of data to collect, what performance measures to use,\\nand what models to train.\\nBeing SMART\\nMachine learning tasks need to be carefully defined for a system to be\\nsuccessful. If you’ve ever read anything about setting goals, you’ve\\nundoubtedly heard of the SMART acronym. SMART is a framework for\\ngoal setting. It stands for Specific, Measurable, Achievable, Relevant, and\\nTime-bound. Although SMART is designed for goal-setting, it’s a\\nsurprisingly good framework for defining machine learning tasks.\\nYour tasks should be specific. The need for specificity stems from the no-\\nfree lunch theorem. The no-free lunch theorem essentially states that there\\nare no universally good machine learning algorithms. You sacrifice\\nperformance on one set of problems for performance on another set of\\nproblems. Many machine learning algorithms subscribe deeply to the Zen\\nphilosophy of “Do one thing well.” If you train an algorithm to recognize', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 34}),\n",
       " Document(page_content='cats and dogs, don’t expect it to translate English to German too well. You\\ntrain your algorithms to do one thing well.\\nYour tasks should be measurable. Measurability ties into the performance\\nmeasure aspect of learning. It’s impossible to tell if an algorithm is learning\\nwithout having any measure of success. Even with a measure of success, it\\nmight not be the correct one or a good enough one. For example, if you\\ndesign a machine learning system to bet on sports and measure its success\\nin terms of profitability, it may learn to never bet at all. Is an algorithm that\\nbets $0 on every game successful?\\nYour tasks should be achievable and time-based. Machine learning relies\\nheavily on optimization. Many of the formulations in machine learning\\nalgorithms could be computed with access to perfect or infinite information.\\nOf course, you never have access to perfect information. Instead, you need\\nto define your task in a manner that’s achievable with machine learning in a\\nreasonable amount of time.\\nFinally, your tasks should be relevant, which may seem obvious but can be\\nmissed in subtle ways. Your task should be relevant to the problem you’re\\ntrying to solve. As an example of the importance of relevance, consider\\nautonomous driving. You might think you can solve the problem of\\nautonomous driving on public roads by training an autonomous vehicle on\\nclosed courses. But your task wasn’t to create an autonomous vehicle', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 35}),\n",
       " Document(page_content='capable of driving on closed courses; it was to create an autonomous\\nvehicle capable of driving on public roads.\\nDefining tasks correctly is critical to creating a good model. If you have a\\npoorly or incorrectly defined task, you’ll end up with a poorly or incorrectly\\ntrained model.\\nWith a better understanding of the importance of defining a good task and\\nsome considerations when defining tasks, you need to define a task for your\\niris problem. Fortunately, the botanist has made your task very clear:\\nclassify flowers from the Iris genus into one of a set number of species. You\\ncan assess the viability of this task in the context of the SMART\\nframework: your task is specific to classifying iris flowers—you weren’t\\nasked to classify other species of flowers or to create a model that makes\\nother kinds of predictions. Your task is easily measurable as you’ll find in  \\nFormulating a Performance Measure . This task is both achievable and\\ntime-based—tasks similar to it have been solved before. Finally, you’ve\\nbeen given data relevant to the task at hand.\\nFormulating a Performance Measure\\nWith your task clearly defined, you need to choose a performance measure.\\nTo measure a model’s progress, you need some sort of measure of success.\\nThis measure of success is typically something like accuracy in\\nclassification tasks and absolute error in regression tasks. The process of', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 36}),\n",
       " Document(page_content='learning comes from direct or indirect optimization of your performance\\nmeasure, which means choosing a good performance measure is critical to\\nthe success of your model.\\nChoosing a good performance measure can be difficult in practice. If your\\nproblem isn’t well-formulated (for example, you haven’t defined a SMART\\ntask), you’ll struggle to find a good performance measure. If you set out\\nwith the task of “creating artificial general intelligence,” you’ll probably\\nstruggle to come up with good performance measures. What is your\\nmeasure of “general” intelligence?\\nEven for simple problems, deciding on a performance measure can be\\ndifficult. For machine translation, which can typically be formulated as a\\nclassification task, is accuracy sufficient? What is the accuracy of a\\ntranslation of a sentence? Because translations aren’t exactly one-to-one—\\nand meaning changes drastically as words are changed or moved around—\\naccuracy can be a poor measure of success in translation tasks. Another\\nexample of this is in machine learning on medical data. You could train a\\nmodel that’s 99% accurate in predicting whether or not a patient has cancer\\nsimply by predicting they don’t every time. Is that model useful?\\nBecause the process of creating and training models is often defined as an\\noptimization task, you typically need to substitute actual performance\\nmeasures with ones that behave better in an optimization setting. Metrics\\nsuch as accuracy are discrete, not differentiable, and thus can’t be optimized', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 37}),\n",
       " Document(page_content='with gradient-based optimization. You’ll learn more about differentiability\\nand optimization in Chapter 4,  Optimize Everything . All you need to know\\nis that you can’t always optimize directly for your chosen performance\\nmeasure, so instead you optimize indirectly on another.\\nThis process of indirect optimization is akin to marathon runners indirectly\\ntracking their progress on a marathon with times on shorter distance runs.\\nYou can’t run a marathon every day, but you can track your performance in\\nother ways. You optimize for the task of running a marathon indirectly by\\nrunning shorter routes and tracking your progress over time.\\nThe direct or indirect measure of success you use to optimize your\\nperformance measure is known as a loss function or cost function. The\\nterminology is rooted in the mathematics of statistical learning theory, but\\nits meaning is straightforward. A loss function is the measure of the\\n“goodness” of a model. Machine learning often aims to minimize the loss or\\ncost function to implicitly maximize your performance measure.\\nAside from simply choosing a performance measure, you also need to\\ndefine minimum thresholds for your metrics before declaring victory over a\\ngiven problem. For example, 85% accuracy in flower species classification\\nis fine, but in fraud detection is catastrophic. Success thresholds are\\nproblem-dependent and should be carefully established with input from\\nevery project stakeholder. Once again, the botanist has been very clear: your', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 38}),\n",
       " Document(page_content='measure of success is accuracy, and your minimum objective is 85%\\naccuracy.\\nFinally, you need to have a clear understanding of what your model inputs\\nand outputs will look like. If you’re in a situation where you don’t have any\\ndata at all, you’ll need to go out and collect data that looks like the data\\nyour model will actually experience. In this example, the botanist has told\\nyou that your model must take various flower measurements and output a\\nlabel. They’ve also given you a dataset of features and labels already.\\nGiven this is a classification task, it makes sense to use accuracy as a\\nperformance measure. For this example, you can define success for this\\nproblem as training a classifier that achieves 85% accuracy on an unseen\\ntest set.\\nImproving from Experience\\nFinally, to actually “learn,” your model needs exposure to some sort of\\nexperience. An experience is just data. What that data looks like is\\ndependent on the data you have and the type of learning algorithm you’re\\ntrying to use. You can usually put a learning problem into one of two\\ncategories based on the type of experiences you want your model to have:\\n1. Supervised learning\\n2. Unsupervised learning', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 39}),\n",
       " Document(page_content='Supervised learning uses data or experiences with labels or targets. Each\\ninput to the model is associated with a correct answer, and your model\\nlearns from how close its prediction is to the target. Supervised learning\\nproblems are generally easier to work with in practice. However, if your\\ndata doesn’t come with a natural label, you need to go through the time-\\nconsuming, difficult, and expensive process of annotating each input\\nexample.\\nUnsupervised learning uses data without labels. Rather than learning from\\ngiven correct answers, unsupervised models learn properties of input data,\\nsuch as how the data can be clustered into some fixed number of clusters.\\nYou’ll often see the collection of experiences your model goes through\\ncalled a dataset. More accurately, the collection of experiences your model\\nlearns from is called the training set.\\n“Training” in machine learning refers to the process of optimizing your\\nmodel on your given performance measure using experiences in your\\ntraining set. Not all machine learning problems come with a fixed dataset.\\nFor example, reinforcement learning problems instead interact with\\nnonstatic environments and learn from reward signals encoded into the\\nenvironment. Rather than giving a model environment state and target\\naction, you typically let the model explore the environment and learn from\\nprevious actions taken. Reinforcement learning isn’t always considered', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 40}),\n",
       " Document(page_content='supervised or unsupervised but is often considered its own type of machine\\nlearning.\\nIn machine learning, you don’t actually care how well your model does\\nduring training. All that matters is how well your model does on unseen\\ndata. A basketball team doesn’t care about how well their players play\\nduring practice; they only care about how well they play during a live game\\n(just ask Allen Iverson). The same goes for your models.\\nTypically, you train your model on a train set and evaluate your model on\\nan unseen test set. Your concern is generalization (that is, how well your\\nmodel generalizes to experiences it hasn’t seen). In reality, evaluating your\\nmodel is much more complex than throwing a static test set at it. If your\\nmodels are going to be used in production, you’ll need to constantly\\nevaluate their performance as your static test set might not have accurately\\ncaptured the data your model will see in production.\\nIn this example, you’ve been handed experiences out of the box. The\\nbotanist has given you a large set of labeled iris flower data where each\\nexample consists of flower measurements. You can use this dataset to both\\ntrain and evaluate your model.\\nPutting It All Together\\nOverall, you can define your learning problem in the following terms:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 41}),\n",
       " Document(page_content='T: Classify flowers from the Iris genus into one of setosa, versicolor, or\\nvirginica species\\nP: Accuracy of classifications (minimum 85% for success)\\nE: Flower measurements (sepal length, sepal width, petal length, petal\\nwidth, and a species label)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 42}),\n",
       " Document(page_content='Learning with Elixir\\nWith your problem scoped, it’s time to dive into the code. First, you’ll need\\nto have a working installation of Elixir. This is a book on machine learning\\nin Elixir, so you’ll need Elixir to run the code! For the examples in this\\nbook to work, you’ll need at least Elixir version 1.14. The easiest way to\\ninstall Elixir is through a version manager like asdf. If you have asdf\\ninstalled, you’ll first need to install Erlang:\\n$ asdf install erlang 25.0.2\\nThis installs Erlang/OTP version 25.0.2. Next, you can install Elixir:\\n$ asdf install elixir 1.14.3\\nThis will install Elixir version 1.14.3. You can verify everything worked by\\nrunning this:\\n$ elixir --version\\nAnd you’ll see the following:\\nErlang/OTP 25 [erts-13.0.2] [source] [64-bit] \\n[smp:10:10]\\n  [ds:10:10:10] [async-threads:1] [jit]\\n[2]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 43}),\n",
       " Document(page_content='Elixir 1.14.3 (compiled with Erlang/OTP 23)\\nNext, you’ll want to head over to the Livebook website and follow the\\nofficial Livebook installation instructions. All of the code in this book is\\nmeant to run inside an Elixir Livebook. Livebooks are interactive code\\nnotebooks similar to Jupyter Notebooks. You can install Livebook locally,\\nor run it in the cloud with Fly.  Once you have Livebook installed, you’ll\\nwant to open it up by using the installed icon or running livebook server from\\nyour command line. After that, create a new notebook and you’re ready to\\ngo.\\nNow, you’ll need to install the libraries. For this example, you’ll need\\nAxon, Nx, Explorer, and Kino.\\nAxon is a library for creating and training neural networks. You can also use\\nit to create and train more basic models as well. You’ll see a lot of Axon in\\nPart 2,  Deep Learning .\\nNx is the foundation of every other project in the Elixir machine learning\\necosystem. It’s a library for creating and manipulating multidimensional\\narrays, or as they’re called in Nx, tensors. In Chapter 2,  Get Comfortable\\nwith Nx , you’ll dive deeper into Nx. But for now, all you need to know is\\nthat Nx is the common language of data spoken by every library in the Nx\\necosystem. You’ll represent your data as Nx tensors and your models as Nx\\noperations.\\n[3]\\n[4]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 44}),\n",
       " Document(page_content='Explorer is a library for creating and manipulating DataFrames.\\nDataFrames are two-dimensional, tabular data structures. You’ll use\\nExplorer throughout this book to conduct data analysis, preprocessing,\\nvalidation, and more.\\nWhen using Livebook, you should also install Kino. Kino is a library for\\ninteractive visualizations and smart cells in Livebook. Axon ships with\\nsome Kino integrations and visualizations that you’ll use in this chapter.\\nTo install these dependencies, run the following in a Livebook cell:\\nMix.install([\\n  { :axon ,  \"  ~> 0.5\" },\\n  { :nx ,  \"  ~> 0.5\" },\\n  { :explorer ,  \"  ~> 0.5\" },\\n  { :kino ,  \"  ~> 0.8\" }\\n])\\nWhen working with Explorer you are encouraged to use the Explorer\\nqueries API, which make use of macros for many manipulation tasks. To\\nuse these macros, you need to require them. In a new cell, run the\\nfollowing:\\nrequire  Explorer.DataFrame,  as:  DF\\n[5]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 45}),\n",
       " Document(page_content='This will include all of the query macros which you can access via the DF\\nalias. Now you’re ready to jump into the data.\\nWorking with Data\\nThe dataset you’re using is available to download within the\\nExplorer.Datasets module. Start by loading the dataset into a DataFrame:\\niris = Explorer.Datasets.iris()\\nAfter running this command and downloading the dataset, you’ll see the\\nfollowing output:\\n#Explorer.DataFrame<\\n  Polars[150 x 5]\\n  sepal_length float [5.1, 4.9, 4.7, 4.6, 5.0, \\n\"...\"]\\n  sepal_width float [3.5, 3.0, 3.2, 3.1, 3.6, \\n\"...\"]\\n  petal_length float [1.4, 1.4, 1.3, 1.5, 1.4, \\n\"...\"]\\n  petal_width float [0.2, 0.2, 0.2, 0.2, 0.2, \\n\"...\"]\\n  species string [\"Iris-setosa\", \"Iris-setosa\", \\n\"Iris-setosa\",\\n  \"Iris-setosa\", \"Iris-setosa\", \"...\"]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 46}),\n",
       " Document(page_content='>\\nNotice that the DataFrame has 150 rows (examples) and 5 columns\\n(features). Each Explorer column has an associated datatype and a list of\\nrow-values within that column. The list of values, which make up an\\nindividual column, is often referred to as a Series. Most operations apply to\\nan individual series because it only makes sense to apply them across an\\nindividual series. For example, computing the mean of the values in this\\nDataFrame doesn’t make sense. But computing the mean of sepal_length\\ndoes make sense.\\nFrom here, you can do a number of interesting things with the data, such as\\nvisualizations and filtering. For now, you’ll focus on getting the data into a\\nformat that’s readily ingestible for model training.\\nPreparing the Data for Training\\nMachine learning isn’t magic. You can’t feed an algorithm any data you\\nwant and expect it to learn the relationships you want it to learn correctly. In\\nreality, most machine learning algorithms rely on some linear algebra and\\nprobability. So, you need to get your data into a format that’s conducive to\\nlearning.\\nOne common requirement is that data should be normalized. In the context\\nof machine learning, normalizing data is the process of ensuring input', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 47}),\n",
       " Document(page_content='features operate on a common scale. Most algorithms only perform some\\nnaive mathematical operations. If you have a feature that can take on values\\nup into the thousands or millions, that feature might get treated as\\nsignificantly more important than a feature that only takes on values\\nbetween 0 and 1.\\nThere are a few ways to appropriately scale data. For instance, you can\\nsqueeze the values of a feature between 0 and 1 by subtracting every\\nindividual feature by the min value of that feature column and then dividing\\nthe result by the range between the max and min values across a feature.\\nYou can also compute a z-score for each feature. A z-score is a statistical\\nmeasure that essentially represents a data point’s deviation from the average\\ndata point in a feature space. Significantly positive or negative z-scores\\nindicate that a value is significantly higher or significantly lower than the\\nrest of the data in the distribution. You’ll often see this type of scaling\\nreferred to as standardization.\\nFor this example, either of those scaling strategies will work well. For now,\\nyou’ll use standardization to scale your data. In a new cell, add the\\nfollowing standardization code:\\ncols =  ~ w(sepal_width sepal_length petal_length \\npetal_width)\\nnormalized_iris =\\n  DF.mutate(', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 48}),\n",
       " Document(page_content='   iris,\\n    for col <- across(^cols)  do \\n      {col.name, (col - mean(col)) / \\nstandard_deviation(col)}\\n     end \\n  )\\nTo standardize a series, all you need to do is calculate the mean and\\nstandard deviation of the series using mean and standard_deviation,\\nrespectively. You then reduce each value in the series by the mean and\\ndivide the result by the standard deviation. Notice that you didn’t need to\\nbreak out Enum.map or Enum.reduce for this function. Instead, you took\\nadvantage of the functions in the Explorer API. DF.mutate/2 mutates the\\ncolumns in the DataFrame according to the second argument. You use\\nacross/1 to access the columns given in the list of column names, and for\\neach column you standardize the values in the series.\\nYou should now have a standardized DataFrame which looks like this:\\n#Explorer.DataFrame<\\n  [rows: 150, columns: 5]\\n  sepal_length float [-1.0840606189132314, \\n-1.3757361217598396,\\n   -1.6674116246064494, -1.8132493760297548, \\n-1.2298983703365356, \"...\"]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 49}),\n",
       " Document(page_content=' sepal_width float [2.372289612531505, \\n-0.28722789030650403,\\n   0.7765791108287006, 0.24467561026109824, \\n2.904193113099107, \"...\"]\\n  petal_length float [-0.7576391687443842, \\n-0.7576391687443842,\\n   -0.7897606710936372, -0.725517666395131, \\n-0.7576391687443842, \"...\"]\\n  petal_width float [-0.7576391687443842, \\n-0.7576391687443842,\\n   -0.7897606710936372,, -0.725517666395131, \\n-0.7576391687443842, \"...\"]\\n  species string [\"Iris-setosa\", \"Iris-setosa\", \\n\"Iris-setosa\",\\n  \"Iris-setosa\", \"Iris-setosa\", \"...\"]\\n>\\nNotice you don’t want to standardize the species. It’s a categorical feature,\\nso there’s no notion of scale. A categorical feature is a feature that takes on\\none of a number of fixed values. After standardizing each series\\nindividually, you apply your changes to the DataFrame by calling\\nDF.mutate/2.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 50}),\n",
       " Document(page_content='Next, you’ll want to convert your species column to a categorical feature so\\nthat it’s treated properly when you convert the DataFrame into a tensor. You\\ncan do this by running the following:\\nnormalized_iris = DF.mutate(normalized_iris, [\\n   species:  Explorer.Series.cast(species,  :category \\n)\\n])\\nThis will cast the species column to a categorical variable and thus will be\\nhandled properly when you convert your DataFrame to a tensor.\\nOne thing you may have noticed is that your data is ordered by flower\\nspecies. All of the examples of the setosa species are first, followed by\\nversicolor, and finally by virginica. In a production setting, you can’t\\nexpect your data to arrive ordered like that. To simulate a real-world\\nenvironment, shuffle your data by running the following:\\nshuffled_normalized_iris = \\nDF.shuffle(normalized_iris)\\nThis returns the following:\\n#Explorer.DataFrame<\\n  [rows: 150, columns: 5]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 51}),\n",
       " Document(page_content=' sepal_length float [0.08264139247320726, \\n1.2493434038596445,\\n  -0.2090341103734024, -1.3757361217598396, \\n0.3743168953198156, \"...\"]\\n  sepal_width float [-0.28722789030650403, \\n0.24467561026109824,\\n  3.9680001142343095, -3.4786488937121156, \\n-0.28722789030650403, \"...\"]\\n  petal_length float [0.4308564181779819, \\n0.3023704087809695,\\n  -0.6612746616966249, -0.1473306241085746, \\n0.36661341347947585, \"...\"]\\n  petal_width float [0.4308564181779819, \\n0.3023704087809695,\\n  -0.6612746616966249, -0.1473306241085746, \\n0.36661341347947585, \"...\"]\\n  species [\"Iris-versicolor\", \"Iris-setosa\", ...]\\n>\\nExplorer.DataFrame.slice/2 grabs row indices from the given DataFrame in the\\norder given. You have 150 values, so indices are 0 to 149. By shuffling the\\nrange of indices, you shuffle the order of elements in the DataFrame.\\nShuffling the DataFrame is important for both training and testing, as you’ll\\nsoon see.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 52}),\n",
       " Document(page_content='Splitting into Train and Test Sets\\nThe botanist has asked you to prove the efficacy of your model before\\nputting it into production. A common practice to validate a model’s\\nperformance is to use a test or holdout set. The test or holdout set is a small\\npercentage of an original dataset, which you don’t present to the model\\nduring training. You use performance on the test set as the final evaluation\\nof performance on your given task. It’s important that your model never\\nsees examples from the test set during training.\\nFor this example, you can split the dataset into training and test sets with\\nthe following code:\\ntrain_df = DF.slice(shuffled_normalized_iris, \\n0..119)\\ntest_df = DF.slice(shuffled_normalized_iris, \\n120..149)\\nBecause you’ve gone through the trouble of shuffling your dataset, you can\\nslice it into train and test sets using slice/2. Had you not shuffled your\\ndataset, you’d have ended up with a test set filled entirely with examples of\\nversicolor flowers. Your test set wouldn’t be a good representative sample\\nof the data, and you would have a pretty bad idea about how well your\\nmodel is actually performing.\\nPreparing Data for Training', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 53}),\n",
       " Document(page_content='Now that your data is normalized and split into train and test sets, you can\\nstart thinking about training your model. But, before you create and train\\nyour model, you need to get your data into a format the model will\\nunderstand.\\nAs mentioned before, the Nx.Tensor is the common language spoken by all of\\nthe libraries in the Nx ecosystem. That means you need to convert your data\\ninto a tensor or a tensor-compatible format.\\nAdditionally, when passing Explorer DataFrames to Nx, you need to make\\nsure your data is in one of the specific supported Nx input types. For this\\nexample, you have a column \"species\" which is currently represented as a\\nstring. \"species\" is the categorical variable you want to train your model to\\npredict. Typically, categorical variables are represented using integers or\\none-hot encoding. One-hot encoding produces a tensor with N columns\\nwith every value being a 0 (meaning “off”) except the index of the class,\\nwhich is a 1 (meaning “on”). As a simple example, if you consider we\\ndiscretize the labels in this example into the integer values 0, 1, and 2, then\\nthe one-hot encoded representation for each class would look like this:\\n# class 0\\n[1, 0, 0]\\n# class 1\\n[0, 1, 0]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 54}),\n",
       " Document(page_content='# class 2\\n[0, 0, 1]\\nFor this example, you need to convert your training and testing labels to\\none-hot encoded tensors. Implement the one-hot encoding like this:\\nfeature_columns = [\\n   \"  sepal_length\" ,\\n   \"  sepal_width\" ,\\n   \"  petal_length\" ,\\n   \"  petal_width\" \\n]\\nx_train = Nx.stack(train_df[feature_columns],  \\naxis:  -1)\\ny_train =\\n  train_df[ \"  species\" ]\\n  |> Nx.stack( axis:  -1)\\n  |> Nx.equal(Nx.iota({1, 3},  axis:  -1))\\nx_test = Nx.stack(test_df[feature_columns],  axis:  \\n-1)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 55}),\n",
       " Document(page_content='y_test =\\n  test_df[ \"  species\" ]\\n  |> Nx.stack( axis:  -1)\\n  |> Nx.equal(Nx.iota({1, 3},  axis:  -1))\\nFirst, notice that you extract feature_columns from both the training and\\ntesting DataFrame and call the variables x_train and x_test, respectively. In\\nmachine learning, it’s common to use the variable x to indicate model\\nfeatures. You use Nx.stack/2 to convert your features in the DataFrame into a\\ntensor. This will stack the rows of the DataFrame into individual entries.\\nNext, you extract a label tensor by one-hot encoding your species column.\\nYou do this by converting the species column to a tensor, which implicitly\\ncasts each category to a unique integer, and then by making use of some Nx\\nmagic to do the one-hot encoding. Don’t worry if you don’t follow the Nx\\nlogic just yet. You’ll get more comfortable with Nx in Chapter 2,  Get\\nComfortable with Nx .\\nNow that you have your features and targets split up for both training and\\ntesting, you’re ready to create a machine learning model.\\nMultinomial Logistic Regression with Axon', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 56}),\n",
       " Document(page_content='Now that you’ve wrangled your data into a format conducive to training\\nmodels, it’s time to actually create and train a model. Training a machine\\nlearning model in Axon essentially boils down to three steps:\\n1. Defining the model\\n2. Creating an input pipeline\\n3. Declaring and running the training loop\\nDefining the Model\\nWhen you hear the term “model” in the context of machine learning, think\\nof a function. A model is anything that takes data in and gives a value out.\\nAxon includes a model creation API that’s typically used for creating neural\\nnetworks. In this example, however, you can use Axon to create a basic\\nmultinomial logistic regression model. You’ll see the inner workings of\\nlogistic regression in Chapter 5,  Traditional Machine Learning . For now,\\nsimply copy and run the following code in a new Livebook cell:\\nmodel =\\n  Axon.input( \"  iris_features\" ,  shape:  {nil, 4})\\n  |> Axon.dense(3,  activation:   :softmax )\\nAfter declaring your model, you’ll see an output that looks like this:\\n#Axon<\\n  inputs: %{\"iris_features\" => {nil, 4}}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 57}),\n",
       " Document(page_content=' outputs: \"softmax_0\"\\n  nodes: 3\\n>\\nEvery Axon model starts with an input with a specific name. You may also\\nspecify an optional :shape. The shape represents the values of each\\ndimension in your data. Subsequent calls on the result of an Axon.input layer\\nrepresent transformations of the input data. This model essentially\\ntransforms your data into three probabilities, which represent the\\nprobability that a given example belongs to one of the three classes.\\nRemember your inputs are actually Explorer DataFrames, so you need a\\nway to get them to work well with Axon’s other functions.\\nAxon.stack_columns/2 will process the DataFrame into a single tensor, which\\nis a bit closer to what Axon expects.\\nYou can visualize your model with Kino by running the following code:\\nAxon.Display.as_graph(model, Nx.template({1, 4},  \\n:f32 ))\\nAnd you’ll see the following image:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 58}),\n",
       " Document(page_content='Visualizing smaller models is a useful way to debug and understand how\\ndata flows through your models.\\nCreating an Input Pipeline\\nAxon implements minibatch training with gradient descent. This means\\nAxon’s training API performs updates to the model iteratively. You’ll learn\\nabout gradient descent more in-depth in Chapter 4,  Optimize Everything .\\nThe training API expects to be able to step through a dataset in “batches” or\\nsmaller groups of examples. In this example, your dataset is small, so you\\ncan update your model on the entire dataset at once. However, Axon will\\nstill expect to be able to grab data from your input pipeline for multiple\\nsteps over a given training run. You can accomplish this using Elixir’s\\nStream module:\\ndata_stream = Stream.repeatedly( fn  ->\\n  {x_train, y_train}\\nend )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 59}),\n",
       " Document(page_content='This function will repeatedly return tuples of your train features and train\\ntargets, respectively. Axon expects input data to be in pairs of {features,\\ntargets}. Here your features are the features present in x_train and the targets\\nare y_train.\\nDeclaring and Running the Training Loop\\nThe Axon.Loop API is Axon’s primary API for training models with gradient\\ndescent. A training loop is essentially a process consisting of the following\\nsteps:\\n1. Grabbing inputs from the input pipeline\\n2. Making predictions from inputs\\n3. Determining how good the predictions were\\n4. Updating the model based on prediction goodness\\n5. Repeating the steps\\nAxon calls the process of going through these steps one time a single\\niteration. You can group multiple iterations together to form a single epoch.\\nFor small- to medium-sized datasets, a single epoch typically means a pass\\nthrough every example in the training set. For extra small and extra large\\ndatasets, a single epoch can be assigned a fixed number of steps. Because\\nyour dataset is extra small, you’ll want to set a fixed number of iterations to\\nstep through to ensure Axon can update your model enough times to reach a\\ngood final model state.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 60}),\n",
       " Document(page_content='An Axon training loop is actually a data structure that tells Axon things\\nabout the loop such as: how to initialize the loop, how to update the model\\nstate after every iteration, and what metrics to track during the loop. You\\nbuild up the loop data structure using functions in the Axon loop API and\\nthen execute the loop with Axon.Loop.run. To implement your training loop,\\ncopy and run the following code:\\ntrained_model_state =\\n  model\\n  |> Axon.Loop.trainer( :categorical_cross_entropy , \\n:sgd )\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.run(data_stream, %{},  iterations:  \\n500,  epochs:  10)\\nAfter the loop runs, you’ll see this:\\nEpoch: 0, Batch: 450, accuracy: 0.7204926 loss: \\n0.7272241\\nEpoch: 1, Batch: 450, accuracy: 0.8676836 loss: \\n0.5376064\\nEpoch: 2, Batch: 450, accuracy: 0.9028787 loss: \\n0.4587657\\nEpoch: 3, Batch: 450, accuracy: 0.9206356 loss: \\n0.4105419', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 61}),\n",
       " Document(page_content='Epoch: 4, Batch: 450, accuracy: 0.9304860 loss: \\n0.3763140\\nEpoch: 5, Batch: 450, accuracy: 0.9483036 loss: \\n0.3500875\\nEpoch: 6, Batch: 450, accuracy: 0.9500036 loss: \\n0.3290386\\nEpoch: 7, Batch: 450, accuracy: 0.9538460 loss: \\n0.3116082\\nEpoch: 8, Batch: 450, accuracy: 0.9583363 loss: \\n0.2968430\\nEpoch: 9, Batch: 450, accuracy: 0.9583363 loss: \\n0.2841162\\nIn this example, you used one of Axon.Loop’s factory functions to declare a\\nsupervised training loop with the :categorical_cross_entropy loss function\\noptimized with :sgd or stochastic-gradient descent. The model, loss function,\\nand optimizer correspond directly to steps 2, 3, and 4 of the previously\\noutlined training loop process. Next, you tell the loop to keep track of\\naccuracy during training, so you have a better idea of how training is\\nprogressing. Finally, you run your training loop for 10 epochs, with each\\nepoch consisting of 500 iterations pulled from your input pipeline.\\nThe output of Axon’s supervised training loop is the trained model state or\\ntrained model parameters, which you can use to implement evaluation loops', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 62}),\n",
       " Document(page_content='or make predictions on new data later on. You’ll notice Axon outputs\\ntraining progress at each step. You might get excited when you see the\\naccuracy of your model on the training set approaches 90%, but remember,\\nperformance on the training set doesn’t mean anything. To get a decent\\nunderstanding of the model’s performance, you need to evaluate it on the\\ntest set.\\nEvaluating the Trained Model\\nTo prove your model’s efficacy, you need to evaluate it on the test set.\\nFortunately, Axon also has conveniences for evaluating models. To evaluate\\nyour model, copy and run the following code:\\ndata = [{x_test, y_test}]\\nmodel\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )\\n|> Axon.Loop.run(data, trained_model_state)\\nThis outputs the following:\\nBatch: 0, accuracy: 0.9666666\\nIn this example, you create a supervised evaluation loop using\\nAxon.Loop.evaluator/2. Next, you tell the loop to aggregate accuracy—your', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 63}),\n",
       " Document(page_content='performance measure—as it goes through the input data. Finally, you run\\nthe loop on your test set. Notice that your final model accuracy on the test\\nset exceeds 85%. You’ve successfully met the botanist’s standards and\\ntrained a model to predict species of the iris flower just from a few\\nmeasurements.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 64}),\n",
       " Document(page_content='[2]\\nWrapping Up\\nIn this chapter, you learned what machine learning is, what kind of\\nproblems it can solve, and how to do some basic machine learning with\\nElixir and Nx. You have got a feel for some of the libraries in the Elixir Nx\\necosystem such as Axon, Explorer, Kino, and Nx itself, and you have seen\\nhow you can integrate a machine learning model into Elixir applications.\\nYou learned about some of the important things to consider when\\napproaching a problem with machine learning including defining problems\\nin terms of a task, performance measure, and experience, using a SMART\\nframework to define your tasks, differentiating between classification and\\nregression problems, and the importance of obtaining good data.\\nYour implementations made use of some of the surface-level features of\\nAxon, Nx, and Explorer. To implement more complex machine learning\\nmodels, you’ll need to go deeper. In the next chapter, you’ll dive head first\\ninto Nx, and you’ll learn more about what Nx is and the power Nx brings to\\nElixir. You’ll see how tensors are used as the fundamental language of data,\\nhow Nx’s automatic differentiation system allows you to train complex\\nmachine learning models, and how Nx enables accelerated computation on\\nthe CPU and GPU.\\nFOOTNOTES\\nhttps://asdf-vm.com/', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 65}),\n",
       " Document(page_content='[3]\\n[4]\\n[5]\\nhttps://livebook.dev\\nhttps://fly.io\\nhttps://hexdocs.pm/explorer/Explorer.Query.html\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 66}),\n",
       " Document(page_content='Chapter 2\\nGet Comfortable with Nx\\n\\xa0\\nIn the previous chapter, you built your first machine learning model using\\nNx, Axon, Explorer, and Kino. You might have noticed that most of the\\ncode helped to wrangle the data into a form your algorithm could use—the\\nNx.Tensor. As you read through this book, you’ll see that much of your time\\nwill be spent working with data rather than writing algorithms, so you’ll\\nneed to be comfortable manipulating data with Nx. In this chapter, you’ll\\ndive deeper into Nx and learn why Nx is the foundational library of the\\nElixir machine learning ecosystem.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 67}),\n",
       " Document(page_content='Thinking in Tensors\\nData is arguably the most important component in a machine learning\\nsystem. Even with the most complex, cutting-edge model, you still need\\nsignificant amounts of high-quality data to train a quality model. Bad data\\ninevitably leads to bad models.\\nResearchers are generally divided into two schools of thought on the\\nimportance of data versus the importance of models in machine learning.\\nSome researchers believe in a data-centric approach. The argument for a\\ndata-centric approach is that large amounts of high-quality data are the key\\nto engineering quality machine learning systems. While other researchers\\nbelieve in a model-centric approach. The model-centric approach focuses\\non spending more time researching new models to build quality machine\\nlearning systems.\\nIn practice, a successful machine learning system requires balancing the\\ntwo approaches, which means you need large amounts of high-quality data\\nand a model with enough capacity to fit that data. You’ll dive deeper into\\nwhat model capacity means in Chapter 4,  Optimize Everything . For now, all\\nyou need to know is that capacity is a model’s ability to effectively find\\npatterns in the data.\\nThe relationship between model and data is akin to that of an engine and\\nfuel. In this relationship, models are the engine and data is the fuel. To build', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 68}),\n",
       " Document(page_content='a successful machine learning system you need a balance of the right model\\nand data.\\nThe core abstraction for representing data in Nx is the Nx.Tensor. In Nx, a\\ntensor is a multidimensional array—a potentially nested array of arrays of\\nnumbers, or for Elixir programmers, a list of lists of numbers. If you’re\\ncoming from a mathematical background, this description of a tensor might\\nconfuse you. The use of the word “tensor” to describe a multidimensional\\narray in Nx comes from the ubiquity of the word in numerical computing,\\nnot from a stance of mathematical correctness.\\nThe Nx.Tensor is designed with two goals in mind: flexibility and\\nperformance. The abstraction needs to be flexible enough to represent real-\\nworld data and performant enough for the compute-intensive calculations\\ninvolved in machine learning. Real-world data can often be easily mapped\\nto multidimensional arrays of numbers. Additionally, multidimensional\\narrays are well-suited to acceleration with modern hardware.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 69}),\n",
       " Document(page_content='Winning the Lottery\\nThe multidimensional array or tensor is the most common\\ndata structure in machine learning today, which is largely\\ndriven by the popularity of deep learning. Arguably, the\\nwidespread usage of both multidimensional arrays in\\nnumerical computing and deep learning instead of other\\nmethods of machine learning is mainly because they were\\nsuited to the hardware available during the early 2010s,\\nnamely GPUs. Multidimensional arrays and deep learning\\nwon the hardware lottery [Hoo21] because they were well-\\nsuited for acceleration on GPUs, not because they’re superior\\nin flexibility or performance. For example, alternative\\nabstractions, such as using hash tables [CMFg20], have been\\nproposed to train neural networks. Additionally, other\\nalgorithms might prove superior to deep learning on less data,\\nbut they’re prohibitively slow on available hardware.\\nIf you’ve never worked with array programming libraries or languages,\\nsuch as NumPy or Matlab, programming in Nx for the first time can feel a\\nbit strange. While Elixir emphasizes a programming paradigm based on\\ndata transformations that map well to numerical computing applications, Nx\\ntensors are fundamentally different from other data types available in the\\nElixir standard library.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 70}),\n",
       " Document(page_content='Getting comfortable with Nx requires some practice, so open up a Livebook\\nand install the required dependencies:\\nMix.install([\\n  { :nx ,  \"  ~> 0.5\" },\\n  { :exla ,  \"  ~> 0.5\" },\\n  { :benchee ,  github:   \"  bencheeorg/benchee\" ,  \\noverride:  true}\\n])\\nYou’ll be working with Nx and EXLA. EXLA is used to accelerate Nx\\nprogramming; you’ll see what that means in  Going from def to defn . You\\nalso need Benchee to benchmark some Nx functions.\\nWith the required dependencies installed, you’re ready to dive into some\\nexamples.\\nUnderstanding Nx Tensors\\nStart by running the following code in a new Livebook cell:\\nNx.tensor([1, 2, 3])\\nThis generates the following output:\\n#Nx.Tensor<\\n  s64[3]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 71}),\n",
       " Document(page_content=' [1, 2, 3]\\n>\\nYou’ve just created a tensor using one of Nx’s creation methods. Nx.tensor/2\\nis the easiest way to create a tensor from a number, a list of numbers, or a\\nnested list of numbers. Try creating a few more tensors using the following\\ncode:\\na = Nx.tensor([[1, 2, 3], [4, 5, 6]])\\nb = Nx.tensor(1.0)\\nc = Nx.tensor([[[[[[1.0, 2]]]]]])\\ndbg(a)\\ndbg(b)\\ndbg(c)\\nThis returns the following:\\n[#cell:d4okkentiv5fwvyryl47ypcqm4vkedbm:4: (file)]\\na #=> #Nx.Tensor<\\n  s64[2][3]\\n  [\\n    [1, 2, 3],\\n    [4, 5, 6]\\n  ]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 72}),\n",
       " Document(page_content='[#cell:d4okkentiv5fwvyryl47ypcqm4vkedbm:5: (file)]\\nb #=> #Nx.Tensor<\\n  f32\\n  1.0\\n>\\n[#cell:d4okkentiv5fwvyryl47ypcqm4vkedbm:6: (file)]\\nc #=> #Nx.Tensor<\\n  f32[1][1][1][1][1][2]\\n  [\\n    [\\n      [\\n        [\\n          [\\n            [1.0, 2.0]\\n          ]\\n        ]\\n      ]\\n    ]\\n  ]\\n>\\nYou’ll see three properties of a tensor every time you inspect its contents:\\ntype, shape, and data. All of these properties make a tensor distinctly', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 73}),\n",
       " Document(page_content='different from a generic Elixir list.\\nTensors Have a Type\\nCreate and inspect two tensors by running the following code in a Livebook\\ncell:\\na = Nx.tensor([1, 2, 3])\\nb = Nx.tensor([1.0, 2.0, 3.0])\\ndbg(a)\\ndbg(b)\\nYou’ll see the following output:\\n[#cell:d4okkentiv5fwvyryl47ypcqm4vkedbm:3: (file)]\\na #=> #Nx.Tensor<\\n  s64[3]\\n  [1, 2, 3]\\n>\\n[#cell:d4okkentiv5fwvyryl47ypcqm4vkedbm:4: (file)]\\nb #=> #Nx.Tensor<\\n  f32[3]\\n  [1.0, 2.0, 3.0]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 74}),\n",
       " Document(page_content='Do you notice a difference between the two tensors, aside from the\\ndifference in data?\\nNotice that tensor a displays s64, whereas tensor b displays f32. Both s64 and\\nf32 are the numeric type of the tensor’s data. If you’ve worked with types in\\nprogramming languages before, you’re likely familiar with some of the\\nnumeric types Nx offers.\\nNx types dictate how the underlying tensor data is interpreted during\\nexecution and inspection. You’ll see in  Tensors Have Data , that tensor data\\nisn’t represented in an Elixir list, but instead as raw bytes. The tensor’s type\\ntells Nx how to interpret those raw bytes.\\nTensor types are defined by a type class and a bit width. The type class can\\nbe a signed integer, an unsigned integer, a float, or a brain float. Signed and\\nunsigned integers can have a bit width of 8, 16, 32, or 64. Floats can have\\nbit widths of 16, 32, or 64. Brain floats can only have a bit width of 16.\\nBrain floats are a special type of floating point number optimized for deep\\nlearning. You can specify types when creating tensors using a tuple of {class,\\nbit-width}. The following table illustrates each type, their Elixir\\nrepresentation, and their inspected string representation:\\n\\xa0', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 75}),\n",
       " Document(page_content='Class Widths Elixir\\nRepresentation\\nString\\nRepresentation\\nsigned\\ninteger\\n8, 16,\\n32, 64\\n{:s, 8}, {:s, 16}, {:s, 32},\\n{:s, 64}\\ns8, s16, s32, s64\\nunsigned\\ninteger\\n8, 16,\\n32, 64\\n{:u, 8}, {:u, 16}, {:u,\\n32}, {:u, 64}\\nu8, u16, u32, u64\\nfloat 16, 32,\\n64\\n{:f, 16}, {:f, 32}, {:f, 64} f16, f32, f64\\nbrain float 16 {:bf, 16} bf16\\ncomplex 64, 128 {:c, 64}, {:c, 128} c64, c128\\n\\xa0\\nNotice that you specify a type’s class with an atom. If you’re familiar with\\nElixir, you know an atom is a constant whose values are their own name.\\nYou then specify the type’s bit width. This dictates the size each element of\\nthe tensor occupies in memory. For example, each element in a signed 64-\\nbit tensor occupies 64 bits or 8 bytes of memory. Larger bit-width types are\\nmore numerically precise, which means you can represent a larger range of', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 76}),\n",
       " Document(page_content='values and not worry as much about underflow or overflow. Underflow\\noccurs when you try to represent a value that’s too small for a computer to\\nrepresent in storage. For example, create a tensor with the following code:\\nNx.tensor(0.000000000000000000000000000000000000000\\nYou’ll see the following output:\\n#Nx.Tensor<\\n  f32\\n  0.0\\n>\\nBut you didn’t want to create a tensor with a value of 0.0; you wanted a\\ntensor with a value of 1.0e-45. The value you’re trying to represent is\\nunderflowed to 0.0. A 32-bit float isn’t precise enough to store your value. If\\nyou increase the bit width to 64, you’ll be able to properly represent the\\nnumber you want. You can tell Nx to use a specific type by passing the :type\\noption to Nx.tensor/2:\\nNx.tensor(1.0e-45,  type:  { :f , 64})\\nRunning this code will return this:\\n#Nx.Tensor<\\n  f64', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 77}),\n",
       " Document(page_content=' 1.0e-45\\n>\\nA 64-bit float occupies more memory and is thus able to store a larger range\\nof numbers. Overflow occurs when the number you are trying to store is too\\nlarge for the given type. This happens often with low-precision integer\\ntypes. For example, create a tensor using the following code:\\nNx.tensor(128,  type:  { :s , 8})\\nHere, you’re trying to create a tensor with a value of 128 and a type of {:s, 8}\\nor a signed 8-bit integer tensor. After running the code, you’ll see the\\nfollowing:\\n#Nx.Tensor<\\n  s8\\n  -128\\n>\\nThat’s surprising! A signed 8-bit integer tensor occupies 1 byte of memory\\nand can only represent values between -128 and 127. Anything outside of\\nthat range will be squeezed to some value within the supported range,\\nwhich results in the behavior you see here.\\nPrecision issues are common in machine learning because you’re often\\nworking with floating-point types. Floating-point types attempt to capture a', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 78}),\n",
       " Document(page_content='large range of real values. But it’s not possible to fit an infinite range of\\nnumbers into a finite amount of storage. You’ll sometimes see surprising\\nissues due to precision issues. Throughout this book, you’ll see code\\nexamples that attempt to work around the limitations of floating-point\\nnumbers.\\nAs you may have noticed, tensors have a homogenous type. For every\\ntensor you’ve created, there’s always been a single type. You cannot have\\ntensors with mixed types. Nx will choose a default type capable of\\nrepresenting the values you are trying to use when you create a tensor\\nunless you explicitly state otherwise by passing a :type parameter. You can\\nsee this default typing in action by running the following code:\\nNx.tensor([1.0, 2, 3])\\nThis returns the following:\\n#Nx.Tensor<\\n  f32[3]\\n  [1.0, 2.0, 3.0]\\n>\\nEven though the last two values are integers, Nx cast them to floats because\\nthe highest type was a floating-point value and Nx didn’t want you to\\nunnecessarily lose precision.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 79}),\n",
       " Document(page_content='Having homogenous types in an array programming library like Nx is\\nnecessary for a couple of reasons. First, it eliminates the need to store\\nadditional information about every value in the tensor. Second, it enables\\nunique optimizations for certain algorithms. For example, imagine you want\\nto compute the index of the maximum value in a tensor of type {:s, 8}.\\nBecause you know that every value in the tensor is a signed 8-bit integer,\\nyou also know that the maximum possible value is 127. If you ever observe\\n127 in the tensor, you can halt the algorithm without traversing the rest of\\nthe tensor because 127 is guaranteed to be maximal. Type-specific\\noptimizations, such as this one, are common in numerical computing.\\nTensors Have Shape\\nYou’ve probably noticed the nested list representation of data when\\ninspecting the contents of a tensor. However, tensor data isn’t stored as a list\\nat all. The nesting you see during inspection is actually a manifestation of\\nthe tensor’s shape. A tensor’s shape is the size of each dimension in the\\ntensor. Consider the following tensors:\\na = Nx.tensor([1, 2])\\nb = Nx.tensor([[1, 2], [3, 4]])\\nc = Nx.tensor([[[1, 2], [3, 4]], [[5, 6], [7, \\n8]]])\\nYou can inspect each tensor with the following code:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 80}),\n",
       " Document(page_content='dbg(a)\\ndbg(b)\\ndbg(c)\\nYou’ll see the following output:\\n[#cell:d4okkentiv5fwvyryl47ypcqm4vkedbm:1: (file)]\\na #=> #Nx.Tensor<\\n  s64[2]\\n  [1, 2]\\n>\\n[#cell:d4okkentiv5fwvyryl47ypcqm4vkedbm:2: (file)]\\nb #=> #Nx.Tensor<\\n  s64[2][2]\\n  [\\n    [1, 2],\\n    [3, 4]\\n  ]\\n>\\n[#cell:d4okkentiv5fwvyryl47ypcqm4vkedbm:3: (file)]\\nc #=> #Nx.Tensor<\\n  s64[2][2][2]\\n  [', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 81}),\n",
       " Document(page_content='   [\\n      [1, 2],\\n      [3, 4]\\n    ],\\n    [\\n      [5, 6],\\n      [7, 8]\\n    ]\\n  ]\\n>\\nNotice the value next to each tensor’s type. That value is its shape. Shapes\\nin Nx are expressed using tuples of positive integer values. The\\nrepresentation you see in the previous code example is a pretty-printed\\nversion of each tensor’s shape. Tensor a has a shape of {2} because it has one\\ndimension of size 2. Tensor b has shape {2, 2} because it has two dimensions,\\neach of size 2. Finally, tensor c has a shape of {2, 2, 2} because it has three\\ndimensions, each of size 2. Notice that as the number of dimensions\\nincreases, so does the level of nesting in the inspected data.\\nThe number of dimensions is typically referred to as the tensor’s rank.\\nAgain, if you’re coming from a mathematical background, this use of the\\nword rank might confuse you. In the world of numerical computing, the\\nrank corresponds to the number of dimensions or the level of nesting in the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 82}),\n",
       " Document(page_content='tensor. Scalars don’t have any level of nesting at all because they don’t\\nhave any shape. You can think of a scalar as a zero-dimensional tensor. A\\nscalar is a single value. Run the following in a new cell:\\nNx.tensor(10)\\nAnd you’ll see the following output:\\n#Nx.Tensor<\\n  s64\\n  10\\n>\\nNotice there’s no output where the shape typically is shown. That’s because\\nthis is a scalar tensor, and so it has no shape.\\nThen why do tensors need to have a shape? Remember, the point of tensors\\nis to have a flexible numeric representation of the outside world. If you\\nwere to try to represent an image with no semblance of shape, it would be\\nvery difficult.\\nImagine you have a 28x28 RGB image. Images are typically represented\\nwith a shape {num_images, height, width, channels} where channels corresponds\\nto the number of color channels in the image—three in this case for red,\\ngreen, and blue color values. If you were asked to access the green value of\\nthe tenth pixel down and the 3rd pixel towards the center of the image, how', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 83}),\n",
       " Document(page_content='would you do that, given only a flat representation of the image? It\\nwouldn’t be possible. You would have no idea how the image is laid out in\\nmemory. Without any information as to the shape of the image, you can’t\\neven be sure how many color channels the image has or what the height and\\nwidth of the image are.\\nA tensor’s shape helps you naturally map tensors to and from the real world.\\nAlso, a tensor’s shape tells you how to perform certain operations on the\\ntensor. For example, if tensors didn’t have any shape, there would be no\\nway to perform matrix multiplications between two tensors because you\\nwould have no understanding of the size of each dimension in your\\nmatrices.\\nTo more naturally map a tensor’s shape to the real world, Nx implements\\nthe concept of named tensors. Named tensors introduce dimension or axis\\nnames for more idiomatic tensor manipulation. For example, if you have an\\nimage, you might have dimension names of :height, width, and :channels.\\nEach dimension name is an atom. You can use dimension names to perform\\noperations on specific dimensions. You can specify the names of a tensor on\\ncreation. For example, run the following code:\\nNx.tensor([[1, 2, 3], [4, 5, 6]],  names:  [ :x ,  :y ])\\nAnd it will return the following output:\\n#Nx.Tensor<', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 84}),\n",
       " Document(page_content=' s64[x: 2][y: 3]\\n  [\\n    [1, 2, 3],\\n    [4, 5, 6]\\n  ]\\n>\\nNotice the shape representation now tells you the size and name of each\\ndimension. Rather than saying dimension 1, you can say dimension :y.\\nNamed dimensions give semantic meaning to otherwise meaningless\\ndimension indices.\\nTensors Have Data\\nAs previously mentioned, tensor data is stored as a byte array or an Elixir\\nbinary. A binary is an array of character bytes. These bytes are interpreted\\nas a nested list of values depending on the tensor’s shape and type.\\nRepresenting tensor data in this way helps simplify many Nx\\nimplementations. When you create a new tensor using Nx.tensor/2, Nx\\ntraverses the values in each list and rewrites the value in a binary\\nrepresentation. To view this binary representation, create a tensor with the\\nfollowing code:\\na = Nx.tensor([[1, 2, 3], [4, 5, 6]])', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 85}),\n",
       " Document(page_content='Now, get the underlying binary representation using Nx.to_binary/1:\\nNx.to_binary(a)\\nThe results will be the following:\\n<<1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, \\n3,\\n  0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5,\\n  0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0>>\\nNotice the binary representation has no semblance of shape or type. It’s\\nliterally a flat collection of byte values. Because Nx has to turn your data\\ninto a binary representation when you use Nx.tensor/2, it’s more performant\\nto instead create tensors using Nx.from_binary/2:\\n<<1::64-signed-native, 2::64-signed-native, 3::64-\\nsigned-native>>\\n|> Nx.from_binary({ :s , 64})\\nThe <<>> syntax creates an Elixir binary. Note you can construct binaries in\\nthe style shown using binary modifiers. The previous code creates the\\nfollowing tensor:\\n#Nx.Tensor<\\n  s64[3]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 86}),\n",
       " Document(page_content=' [1, 2, 3]\\n>\\nNx.from_binary/2 takes a binary and a type and creates a one-dimensional\\ntensor from the binary data. You can change the shape of the tensor using\\nNx.reshape/2:\\n<<1::64-signed-native, 2::64-signed-native, 3::64-\\nsigned-native>>\\n|> Nx.from_binary({ :s , 64})\\n|> Nx.reshape({1, 3})\\nThis returns the following output:\\n#Nx.Tensor<\\n  s64[1][3]\\n  [\\n    [1, 2, 3]\\n  ]\\n>\\nNotice the usage of the native binary modifier. Because Nx operates at the\\nbyte level, endianness matters. Endianness is the order in which bytes are\\ninterpreted or read in the computer. The native modifier tells the virtual\\nmachine to use your system’s native endianness. If you’re attempting to', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 87}),\n",
       " Document(page_content='read binary data from a computer with a different endianness than your\\nmachine’s, you might run into some problems. For the most part, you\\nshouldn’t have to worry, but it’s something to be conscious of if you need to\\nwork with a tensor’s raw data.\\nTensors Are Immutable\\nOne notable distinction between Nx and other numerical computing\\nlibraries is that Nx tensors are immutable, which means that none of Nx’s\\noperations change the tensor’s underlying properties. Every operation\\nreturns a new tensor with new data every time. In some situations, this can\\nbe expensive. Nx overcomes the limitation of immutability by introducing a\\nprogramming model that enables Nx operator fusion. You’ll use this\\nprogramming model in  Going from def to defn .', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 88}),\n",
       " Document(page_content='Using Nx Operations\\nNow that you have a better understanding of what a tensor is, it’s time to\\nget a better understanding of how to manipulate tensors.\\nNx comes with a number of operations you can use and compose into\\ncomplex mathematical operations and algorithms. There are four common\\ntypes of operations you should get comfortable with first: shape and type\\noperations, element-wise unary operations, element-wise binary operations,\\nand reductions.\\nShape and Type Operations\\nShape and type operations work on the shape and type properties of a\\ntensor. Run the following code in a Livebook cell to create a new tensor\\nwith shape {3} and type {:s, 64}:\\na = Nx.tensor([1, 2, 3])\\nThen, run the following code to cast the tensor to type {:f, 32} and change its\\nshape to {1, 3, 1}:\\na\\n|> Nx.as_type({ :f , 32})\\n|> Nx.reshape({1, 3, 1})', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 89}),\n",
       " Document(page_content='The code returns this tensor:\\n#Nx.Tensor<\\n  f32[1][3][1]\\n  [\\n    [\\n      [1.0],\\n      [2.0],\\n      [3.0]\\n    ]\\n  ]\\n>\\nYou’ll often need to use Nx.reshape/2 and Nx.as_type/2 to get your data into\\nthe proper shape and type for your algorithms. Nx.reshape/2 is a constant-\\ntime operation. All it does is return a new tensor with an updated shape; it\\ndoesn’t manipulate tensor data in any way. Changing a tensor’s shape only\\nchanges how you interpret a tensor’s underlying data. It doesn’t change\\nanything about the data.\\nNx.as_type/2 does manipulate the tensor’s underlying data. It’s possible to\\nchange the underlying type in constant-time using Nx.bitcast/2. But note that\\na bitcast will drastically change the values you’re working with and is\\nusually not the desired behavior. You can see this by running the following\\ncode:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 90}),\n",
       " Document(page_content='Nx.bitcast(a, { :f , 64})\\nAfter running the code, you’ll see this outcome:\\n#Nx.Tensor<\\n  f64[3]\\n  [5.0e-324, 1.0e-323, 1.5e-323]\\n>\\nNotice the values are drastically different from the original ones. That’s\\nbecause you’re trying to interpret 64-bit integer bytes as 64-bit float bytes.\\nElement-wise Unary Operations\\nElement-wise unary operations are similar to calling Enum.map/2 on a list of\\ndata with a mathematical operation. For example, imagine you wanted to\\ncalculate the absolute value of every element in a list. Most Elixir\\nprogrammers would probably implement a solution that looks like this:\\na = [-1, -2, -3, 0, 1, 2, 3]\\nEnum.map(a, &abs/1)\\nThis would return a list:\\n[1, 2, 3, 0, 1, 2, 3]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 91}),\n",
       " Document(page_content='The code applies &abs/1 to every element in the list. Now, imagine you were\\ngiven a nested list of lists. Calculating the absolute value of every value in\\nthe list would require a nested Enum.map/2. As the degree of nesting\\nincreases, so does the difficulty of the problem.\\nIf you were asked to do the same thing with a tensor, you might be tempted\\nto use a solution that resembled your usage of Enum.map/2. But Nx\\nimplements a number of operations that work element-wise on the tensor’s\\ndata, regardless of the degree of nesting present. These operations work on\\nthe flattened representation of tensor data while still preserving the tensor’s\\nshape. To see this in action, create the following tensor:\\na = Nx.tensor([[[-1, -2, -3], [-4, -5, -6]], [[1, \\n2, 3], [4, 5, 6]]])\\nTo calculate the absolute value of every value in the tensor, use this:\\nNx.abs(a)\\nYou’ll get the following output:\\n#Nx.Tensor<\\n  s64[2][2][3]\\n  [\\n    [\\n      [1, 2, 3],', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 92}),\n",
       " Document(page_content='     [4, 5, 6]\\n    ],\\n    [\\n      [1, 2, 3],\\n      [4, 5, 6]\\n    ]\\n  ]\\n>\\nYou didn’t need any nested maps, loops, or recursion. The solution is a\\nsingle call to an Nx library function. Nx has a number of functions that\\nwork in exactly the same way for computing element-wise square roots,\\nexponentials, logarithms, and so on. If you need to apply a function to every\\nelement in a tensor, you should be composing already existing unary\\noperations and not attempting to manually iterate over every value in the\\ntensor. That’s because Nx’s backends and compilers likely already have\\nefficient routines for Nx operations, which are significantly more efficient\\nthan any manual loops or maps you can write by hand.\\nElement-wise Binary Operations\\nInevitably you’ll want to add, subtract, multiply, and divide your data. In\\nElixir, you would probably try to accomplish this task with Enum.zip_with/3.\\nFor example, if you wanted to add corresponding elements in each list of\\ndata, you would probably do something like this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 93}),\n",
       " Document(page_content='a = [1, 2, 3]\\nb = [4, 5, 6]\\nEnum.zip_with(a, b,  fn  x, y -> x + y  end )\\nIt would return this list:\\n[5, 7, 9]\\nOnce again, as the level of nesting increases, so does the complexity of the\\nproblem. Fortunately, Nx has library functions that take care of this zip-map\\noperation for you. Start by creating the following two tensors:\\na = Nx.tensor([[1, 2, 3], [4, 5, 6]])\\nb = Nx.tensor([[6, 7, 8], [9, 10, 11]])\\nNow, add the tensors together with the following code:\\nNx.add(a, b)\\nThis returns the following result:\\n#Nx.Tensor<\\n  s64[2][3]\\n  [\\n    [7, 9, 11],\\n    [13, 15, 17]\\n  ]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 94}),\n",
       " Document(page_content='>\\nYou can try a number of other binary operations, such as multiplication:\\nNx.multiply(a, b)\\nThis returns the following output:\\n#Nx.Tensor<\\n  s64[2][3]\\n  [\\n    [6, 14, 24],\\n    [36, 50, 66]\\n  ]\\n>\\nBinary operations work on corresponding elements in two tensors. But what\\nif the tensors aren’t the same shape?\\nIf you have tensors with different shapes and you attempt to perform a\\nbinary operation on them, Nx will attempt to broadcast your tensors\\ntogether. Broadcasting is the process of repeating an operation over the\\ndimensions of two tensors to make their shapes compatible. For example,\\nwhat if you wanted to add a scalar tensor and a tensor with shape {3}? You\\ncan add the tensors together by broadcasting the scalar over the entire\\ntensor of shape {3}. You would add the scalar to every value in the shape {3}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 95}),\n",
       " Document(page_content='tensor. Two shapes can be broadcast together only when either of the\\nfollowing conditions are met:\\n1. One of the shapes is a scalar.\\n2. Corresponding dimensions have the same size OR one of the dimensions\\nis size 1.\\nFor example, the {1, 3, 3, 2} and {4, 1, 3, 2} can be broadcast together because\\nevery dimension either matches or nonmatching dimensions are size 1. On\\nthe other hand, the shapes {1, 3, 3, 2} and {4, 2, 3, 2} cannot be broadcast\\ntogether because the second dimension has a mismatch. You can’t broadcast\\n3 to 2, and thus these shapes cannot be broadcast together.\\nIf the ranks of shapes don’t match, Nx will try to prepend dimensions of\\nsize 1 to the lower rank shape to perform broadcasting. For example, if you\\ntry to broadcast the shapes {3} and {2, 3}, it will work because Nx can\\nprepend 1 into the first shape such that you get shape {1, 3}. The shapes {1, 3}\\nand {2, 3} can be broadcast together according to our broadcasting rules.\\nTry running the following to get a feel for how broadcasting works:\\nNx.add(5, Nx.tensor([1, 2, 3]))\\nYou’ll see the following output:\\n#Nx.Tensor<', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 96}),\n",
       " Document(page_content=' s64[3]\\n  [6, 7, 8]\\n>\\nNotice the 5 was added to each element in the tensor. You can further see\\nthe impact of broadcasting by running this:\\nNx.add(Nx.tensor([1, 2, 3]), Nx.tensor([[4, 5, 6], \\n[7, 8, 9]]))\\nYou’ll see the following output:\\n#Nx.Tensor<\\n  s64[2][3]\\n  [\\n    [5, 7, 9],\\n    [8, 10, 12]\\n  ]\\n>\\nNotice how the values in the first tensor are applied to each row of the\\nsecond tensor.\\nReductions', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 97}),\n",
       " Document(page_content='The final type of operation you should get comfortable with in Nx is a\\nreduction operation. Imagine you had a tensor that represented revenues\\nover the last 12 months:\\nrevs = Nx.tensor([85, 76, 42, 34, 46, 23, 52, 99, \\n22, 32, 85, 51])\\nHow would you calculate the total revenue across all 12 months? If your\\ndata was in a list, you would probably want to use Enum.reduce/3. Nx offers\\na number of out-of-the-box reductions that allow you to compute\\naggregates over entire tensors or specific axes. To compute the total\\nrevenue, you can use Nx.sum/2:\\nNx.sum(revs)\\nIt returns this:\\n#Nx.Tensor<\\n  s64\\n  647\\n>\\nNx reductions can work on single axes as well. Imagine you have revenues\\nfrom the last four years:\\nrevs = Nx.tensor(', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 98}),\n",
       " Document(page_content=' [\\n    [21, 64, 86, 26, 74, 81, 38, 79, 70, 48, 85, \\n33],\\n    [64, 82, 48, 39, 70, 71, 81, 53, 50, 67, 36, \\n50],\\n    [68, 74, 39, 78, 95, 62, 53, 21, 43, 59, 51, \\n88],\\n    [47, 74, 97, 51, 98, 47, 61, 36, 83, 55, 74, \\n43]\\n  ],  names:  [ :year ,  :month ])\\nYou can compute the total revenue per year or per month by specifying an\\naxis to sum over. If you wanted to know the total revenue for each month\\nover the last four years, you could sum over the :year dimension:\\nNx.sum(revs,  axes:  [ :year ])\\nThis returns the following result:\\n#Nx.Tensor<\\n  s64[month: 12]\\n  [200, 294, 270, 194, 337, 261, 233, 189, 246, \\n229, 246, 214]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 99}),\n",
       " Document(page_content='Alternatively, you could get the total revenue for each year by summing\\nover :month:\\nNx.sum(revs,  axes:  [ :month ])\\nAnd it returns this result:\\n#Nx.Tensor<\\n  s64[year: 4]\\n  [705, 711, 731, 766]\\n>\\nThe Nx library has a number of reduction functions for computing\\naggregates over axes or entire tensors. You can compute averages, sums,\\nmins, maxes, and so on.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 100}),\n",
       " Document(page_content='Representing the World\\nThe purpose of the Nx tensor abstraction is to give you a flexible data\\nstructure for representing the real world. Machine learning is applicable to\\nnearly every field. To use machine learning for your applications, you need\\nto choose an appropriate numerical representation for your data. Depending\\non the type of data you have, there’s likely already a common or natural\\nway to represent the data as a tensor.\\nTabular Data\\nTabular data or structured data is data with a structure similar to what you\\nwould find in a relational database or CSV file. Each column represents a\\nfeature of the data, such as petal length in the iris example from Chapter 1,  \\nMake Machines That Learn . You’ll often see tabular data represented in a\\ntwo-dimensional tensor with the shape {num_examples, num_features}.\\nnum_examples corresponds to the total number of examples in your dataset.\\nnum_features corresponds to the total number of features in the tabular data.\\nEncoding information in this way loses some semantic information about\\nwhat the values in the tensor represent, which needs to be considered when\\nusing structured data in a machine learning algorithm. For example, if you\\nhave a field that represents home price and another field that represents\\ninterest rate, you’d end up with a tensor that contains values with drastically\\ndifferent orders of magnitudes.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 101}),\n",
       " Document(page_content='Images\\nIf you consider an image as just a collection of pixels, it maps naturally to a\\ntensor. In reality, images are represented as a four-dimensional tensor with\\nthe shape {num_examples, height, width, channels}, where num_examples is the\\nnumber of examples in your dataset, height is the pixel height of the image,\\nwidth is the pixel width of the image, and channels is the number of color\\nchannels in the image. For an RGB image, channels is equal to 3 because for\\neach pixel, there are three color values: red, green, and blue.\\nYou might also see images with the following shape: {num_examples, channels,\\nheight, width}. This is a channels first representation of an image, whereas\\nthe {num_examples, height, width, channels} is a channels last representation.\\nThe position of the channels dimension differs from framework to\\nframework. Historically, PyTorch—a Python machine learning framework\\n—used channels first configurations for images while TensorFlow used\\nchannels last. From a performance perspective, representing an image with\\nthe channels last format is a bit faster when performing operations such as\\nconvolutions. You’ll learn more about working with images in Chapter 7,  \\nLearn to See .\\nVideo\\nIf you consider a video as a collection of images, then you can easily extend\\nyour image representation to represent a video as well. A video is simply a', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 102}),\n",
       " Document(page_content='sequence of images where each frame of the video is a tensor representation\\nof an image.\\nIt’s common to represent videos with a tensor shape of {num_examples,\\nframes, height, width, channels} where num_examples is the number of examples\\nin your dataset, frames is the number of frames per video example, height is\\nthe pixel height of each frame, width is the pixel width of each frame, and\\nchannels is the number of color channels in the video. For example, one\\nvideo filmed at 15 frames per second for four seconds at a resolution of\\n224x224 with three color channels would have a shape of {1, 60, 224, 224, 3}.\\nAudio\\nAudio is represented as a sequence of samples where each sample\\nrepresents an amplitude or strength of the audio signal at a specific time.\\nThe number of samples depends on the audio sampling rate and the total\\nlength of a recording. You can also have multiple audio channels. For\\nexample, in a stereo audio file, you have samples for left and right speakers.\\nYou can represent audio with a shape of {num_examples, samples, channels}\\nwhere num_examples is the number of examples in the dataset, samples is the\\nnumber of audio samples in each example, and channels is the number of\\naudio channels in each example.\\nText', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 103}),\n",
       " Document(page_content='There are many ways to extract numerical representations of text. You’ll use\\na few common ways to model text throughout this book. In models that\\nmake use of deep learning, text is usually tokenized in some way.\\nTokenization is the process of splitting text into smaller units of\\nrepresentation. For example, you can tokenize text at the character level and\\nthen assign a numeric value to each character. You can represent each token\\nwith a one-hot encoding or with a numeric value. Remember from  \\nPreparing Data for Training , one-hot encoding is essentially a binary\\nrepresentation where each index represents the presence of a value. If you\\nhave a one-hot encoded representation of the value 0 for a variable with\\nthree possible values, the one-hot representation will look like this:\\n#Nx.Tensor<\\n  u8[3]\\n  [1, 0, 0]\\n>\\nAfter tokenization, you’re left with a sequence of numeric values, which\\nmeans the shape of a numeric representation of text would be {num_examples,\\nsequence_length, token_features} where num_examples is the number of\\nexamples in your dataset, sequence_length is the length of each text\\nsequence, and token_features is the size of the feature space for each token.\\nFor example, imagine you had 32 text examples, each 255 characters long', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 104}),\n",
       " Document(page_content='and tokenized at the character level, and one-hot encoded with 100 possible\\ncharacters. The shape of your dataset would be {32, 255, 100}.\\nYou can also tokenize text into a sequence of integers such that the shape\\nwill end up being {num_examples, sequence_length}. For example, imagine you\\nhave these sentences: [\"My name is Sean\", \"I love Elixir\"]. Assuming each word in\\nthose sentences represents a unique token, your tensor might look\\nsomething like this:\\n#Nx.Tensor<\\n  s64[2][4]\\n  [\\n    [32, 49, 27, 5],\\n    [68, 34, 55, 0]\\n  ]\\n>\\nNote that the sequence \"I love Elixir\" has an additional token at the end called\\na padding token such that its sequence length matches the length of all the\\nsequences in the dataset. You’ll learn about padding more in Chapter 9,  \\nUnderstand Text .', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 105}),\n",
       " Document(page_content='Going from def to defn\\nSo far in this chapter, you’ve learned about what a tensor is, how to create\\nand manipulate tensors in Nx, and how to model the real world with\\ntensors. This is all great, but you still haven’t learned how to unlock the true\\npower of Nx just yet.\\nIf you’ve been working with Elixir for a while, you’ve probably heard the\\nargument Elixir and the Erlang Virtual Machine aren’t designed for the type\\nof computations required in machine learning and scientific computing. All\\nthe examples you’ve seen in this chapter run in pure Elixir without any\\nacceleration. The lack of acceleration is fine for the examples in this\\nchapter, but for the computations required in machine learning, you’ll need\\nacceleration. Enter the magic of defn.\\nTypical function definitions in Elixir look something like this:\\ndefmodule  MyModule  do \\n   def  adds_one(x)  do \\n    Nx.add(x, 1)\\n   end \\nend ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 106}),\n",
       " Document(page_content='The code you just saw creates a module named MyModule with a function\\nadds_one, which uses Nx.add/2 to add 1 to the parameter x. When you execute\\nMyModule.adds_one/1, your code will run in pure Elixir without any\\nacceleration. Now, tweak your module to look like this:\\ndefmodule  MyModule  do \\n   import  Nx.Defn\\n  defn adds_one(x)  do \\n    Nx.add(x, 1)\\n   end \\nend \\nYou’ve made two small but significant changes here. First, you added import\\nNx.Defn, which imports the definition for Nx numerical definitions. A\\nnumerical definition is an Elixir function that will be just-in-time (JIT)\\ncompiled using a valid Nx compiler. Just-in-time compilation means that a\\nspecial version of your function will be compiled when the function is\\ninvoked. The function compiled is dependent on the type and shape of the\\ninputs to your function. Second, you should notice the change from def\\nadds_one to defn adds_one. This change actually creates a numerical\\ndefinition that can be JIT-compiled to the CPU or GPU.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 107}),\n",
       " Document(page_content='Numerical definitions make use of a multistage programming model. On\\nfunction invocation, rather than executing the function, Nx computes an\\nexpression representation of your program and then gives that expression to\\nan Nx compiler such as EXLA. EXLA traverses the expression and\\ncompiles an optimized program from the given expression, which can then\\nbe executed on the CPU or GPU. To see this in action, tweak the adds_one/1\\nfunction:\\ndefn adds_one(x)  do \\n  Nx.add(x, 1) |> print_expr()\\nend \\nThe additional print_expr/1 will output the Nx expression representation of\\nyour function. Now, run your function:\\nMyModule.adds_one(Nx.tensor([1, 2, 3]))\\nAnd you’ll see the following:\\n#Nx.Tensor<\\n  s64[3]\\n  Nx.Defn.Expr\\n  parameter a:0   s64[3]\\n  b = add 1, a    s64[3]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 108}),\n",
       " Document(page_content='>\\nYou can think of an Nx expression as a sort of numerical assembly for your\\nNx functions. Compilers such as EXLA know how to interpret and work\\nwith this assembly to create optimized numerical programs.\\nNx compilers and numerical definitions are how Nx gets around the\\nlimitation of immutability. Rather than eagerly execute a program with a lot\\nof intermediate data copies, Nx stages computation out to an external\\ncompiler such as EXLA, which generates an optimized program that\\nappears to run as a single function call. To see the performance boost\\noffered by Nx compilers first hand, create the following module:\\ndefmodule  Softmax  do \\n   import  Nx.Defn\\n  defn softmax(n),  do : Nx.exp(n) / \\nNx.sum(Nx.exp(n))\\nend \\nThis code creates a module with a single numerical definition of the\\nsoftmax function. The softmax function is common in machine learning\\nalgorithms. You’ll see it throughout this book.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 109}),\n",
       " Document(page_content='Softmax computes a normalized probability distribution based on input\\nweights. That simply means it assigns each input weight a probability\\nbetween 0 and 1, and the value of all weights sums to 1. One thing to note is\\nthat this is actually an unstable implementation of the softmax function—it\\noverflows for large values of n. For simplicity’s sake, you don’t need to be\\nconcerned with that for now. Next, execute the following code:\\nkey = Nx.Random.key(42)\\n{tensor, _key} = Nx.Random.uniform(key,  shape:  \\n{1_000_000})\\nBenchee.run(\\n  %{\\n     \"  JIT with EXLA\"  =>  fn  ->\\n      apply(EXLA.jit(&Softmax.softmax/1), \\n[tensor])\\n     end ,\\n     \"  Regular Elixir\"  =>  fn  ->\\n      Softmax.softmax(tensor)\\n     end \\n  },\\n   time:  10\\n)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 110}),\n",
       " Document(page_content='This code creates a random tensor with one million elements and then runs\\ntwo benchmarks. The first benchmark uses JIT compilation to execute the\\nsoftmax function. The second uses regular Elixir to execute the softmax\\nfunction. You’ll see the following output:\\nOperating  System:  Linux\\nCPU  Information:  AMD Ryzen 5 3600 6-Core Processor\\nNumber of Available  Cores:  12\\nAvailable  memory:  31.35 GB\\nElixir 1.13.0\\nErlang 24.2\\nBenchmark suite executing  with  the following  \\nconfiguration: \\nwarmup:  2 s\\ntime:  10 s\\nmemory  time:  0 ns\\nparallel:  1\\ninputs:  none specified\\nEstimated total run  time:  24 s\\nBenchmarking JIT  with  EXLA...\\nBenchmarking Regular Elixir...', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 111}),\n",
       " Document(page_content='Name                     ips        average  \\ndeviation         median\\nJIT  with  EXLA         180.60        5.54 ms      ± \\n9.26%        5.51 ms\\nRegular Elixir          2.33      428.55 ms      ± \\n2.95%      431.50 ms\\nComparison: \\nJIT  with  EXLA         180.60\\nRegular Elixir          2.33 - 77. 40 x slower \\n+423.01 ms\\nThis is a 78x speedup just from using JIT compilation. You would probably\\nsee even better speedups using a GPU. Rather than explicitly calling\\nEXLA.jit/2, you can tell Nx to always JIT-compile numerical definitions with\\na given compiler by setting defn options like this:\\nNx.Defn.global_default_options( compiler:  EXLA)\\nNow, run the benchmark again, and you’ll see this:\\nOperating System: Linux\\nCPU Information: AMD Ryzen 5 3600 6-Core Processor\\nNumber of Available Cores: 12\\nAvailable memory: 31.35 GB', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 112}),\n",
       " Document(page_content='Elixir 1.13.0\\nErlang 24.2\\nBenchmark suite executing with the following \\nconfiguration:\\nwarmup: 2 s\\ntime: 10 s\\nmemory time: 0 ns\\nparallel: 1\\ninputs: none specified\\nEstimated total run time: 24 s\\nBenchmarking JIT with EXLA...\\nBenchmarking Regular Elixir...\\nName                     ips        average  \\ndeviation         median\\nRegular Elixir        192.45        5.20 ms    \\n±10.15%        5.20 ms\\nJIT with EXLA         176.71        5.66 ms    \\n±10.74%        5.62 ms\\nComparison:\\nRegular Elixir        192.45', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 113}),\n",
       " Document(page_content='JIT with EXLA         176.71 - 1.09x slower +0.46 \\nms\\nNotice the benchmark results are essentially the same. Setting the default\\ncompiler tells Nx to always JIT-compile all defn invocations with the given\\ncompiler. With a single additional character and some configurations, Nx\\ncan give you 75x speedups for numerical applications.\\nBackend or Compiler?\\nWhen working with Nx, you’ll encounter two fundamental ways of\\nspeeding up your code: backends and compilers. The relationship between\\nbackends and compilers is kind of like the relationship between interpreted\\nprogramming languages and compiled programming languages.\\nNx backends are implementations of the Nx library that eagerly evaluate Nx\\nfunctions. The default Nx backend is Nx.BinaryBackend, which uses pure\\nElixir to manipulate tensors. Backends evaluate Nx functions and yield the\\nresult every time. There’s no possibility for fusion or other compiler\\noptimizations. Backends are slower, but you can more rapidly prototype as\\nyou don’t have to structure your code into modules and numerical\\ndefinitions. Working with a backend is also more flexible as you don’t need\\nto adhere to a numerical definition’s strict programming model. You can set\\na default backend using Nx.default_backend/1:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 114}),\n",
       " Document(page_content='Nx.default_backend(EXLA.Backend)\\nOr, you can use it in your application’s configuration:\\nconfig  :nx ,  default_backend:  EXLA.Backend\\nThis will tell Nx to use the EXLA backend for every call to Nx in your\\napplication.\\nCompilers implement the multistage programming model mentioned in  \\nGoing from def to defn . Compilers are often more performant, but they\\nrequire a stricter programming model. You’ve already seen how you can set\\nthe default compiler using Nx.Defn.global_default_options/1:\\nNx.Defn.global_default_options( compiler:  EXLA)\\nYou can also set the default compiler in your application’s configuration:\\nconfig  :nx ,  :default_options , [ compiler:  EXLA]\\nThere are some pitfalls when setting a default compiler for your application.\\nTo avoid these pitfalls, it’s often recommended to only set a default\\nbackend, and then explicitly JIT-compile functions when you deem it\\nnecessary. You’ll see this pattern followed throughout the examples in this\\nbook.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 115}),\n",
       " Document(page_content='Wrapping Up\\nIn this chapter, you learned what a tensor is and how to do some common\\noperations on tensors. You also learned how to use tensors to represent the\\nreal world and how to accelerate your Nx code using numerical definitions\\nand JIT compilation. With a better understanding of the principles of Nx,\\nyou’re ready to start unlocking the power of machine learning. In the next\\nchapter, you’ll use your newfound Nx abilities to better understand the math\\nthat underlies the machine learning algorithms in this book.\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 116}),\n",
       " Document(page_content='Chapter 3\\nHarness the Power of Math\\n\\xa0\\nIn the previous chapter, you learned about the core Nx data structure, the\\ntensor, and the array programming paradigm that Nx encourages. You also\\nused JIT compilation and saw how Nx defn enables acceleration via\\npluggable backends and compilers, which make machine learning\\nworkloads practical in Elixir. Before you start writing some machine\\nlearning algorithms, you need to dig a little deeper into the mathematics,\\nwhich backs modern machine learning.\\nIn this chapter, you’ll get a baseline understanding of machine learning\\nmath and what it looks like in practice with Nx. This chapter isn’t meant to\\nbe a comprehensive study of mathematics. Rather, it’s meant to give you a\\nbetter idea about the inner workings of your machine learning algorithms. If\\nyou’re interested in diving deeper into this topic, check out Mathematics for\\nMachine Learning [DFO20].\\nNow, fire up a new Livebook and get ready to learn some math.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 117}),\n",
       " Document(page_content='Understanding Machine Learning Math\\nMathematics is the foundation of every scientific field. Machine learning is\\nno different. The earliest machine learning research was born from\\ntheoretical mathematical analysis. Researchers working in machine learning\\nlacked sufficient computing power and data to test their ideas. As a result,\\nmachine learning was often overlooked in favor of expert systems and more\\nlogical approaches to artificial intelligence.\\nExpert systems and logic-based approaches generally centered around the\\nidea that it should be possible to “create” intelligence from a finite set of\\nrules and logical primitives. Programming languages like Prolog stemmed\\nfrom this belief. Today, with an abundance of computing power and data,\\nmachine learning reigns supreme, though it’s not necessarily well-grounded\\nin theory—it’s much easier to test machine learning ideas than it is to\\ndevelop the theory to support those ideas.\\nMany researchers argue that machine learning—especially deep learning—\\nlacks a rigorous mathematical foundation, ultimately inhibiting its progress.\\nOther researchers take a more relaxed stance, feeling that some approaches\\nempirically just work, and putting them on a rigorous mathematical\\nfoundation is unnecessary. It’s probably best to be somewhere in the middle\\nof the “mathematical deference” spectrum. You should understand enough\\nmath to know why your algorithms work (or don’t work), but it’s okay if\\nyou don’t write proofs for every model you put into practice.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 118}),\n",
       " Document(page_content='While many areas of mathematics lay at the foundation of machine\\nlearning, the biggest are linear algebra, probability, and vector calculus. If\\nyou have a STEM background, you likely have experience with some or all\\nof these. If you don’t—or you cringe at the thought of computing another\\nsingular-value decomposition or derivative by hand—don’t worry. You\\ndon’t need to go that deep into any of these fields. Having a high-level\\nunderstanding is attainable with a bit of work. To get you started, the rest of\\nthis chapter will provide a surface-level introduction to each of these\\nmathematical fields in the context of machine learning and Nx.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 119}),\n",
       " Document(page_content='Speaking the Language of Data\\nYou’ll often hear linear algebra referred to as the language of data. Recall\\nfrom Chapter 2,  Get Comfortable with Nx , that Nx tensors give you a\\npowerful and flexible abstraction for modeling and manipulating data. The\\npower of Nx tensors and Nx operations comes from the fact that they build\\non existing theory and operations in linear algebra. Many of the algorithms\\nyou’ll see in this book rely heavily on established principles in linear\\nalgebra. Deep learning especially leans heavily on the power of linear\\nalgebra. In this section, you’ll get a quick feel for what linear algebra is all\\nabout, and you’ll see some linear algebra in action with Nx. This\\nconversation is meant to serve only as an introduction to an in-depth topic.\\nIf you’d like to explore deeper, I strongly recommend Linear Algebra by\\nGilbert Strang [Str16] and the 3Blue1Brown YouTube series on Linear\\nAlgebra.\\nBefore diving in, open up a new Livebook and install the following\\ndependencies:\\nMix.install([\\n  { :nx ,  \"  ~> 0.5\" },\\n  { :exla ,  \"  ~> 0.5\" },\\n  { :kino ,  \"  ~> 0.8\" },\\n  { :stb_image ,  \"  ~> 0.6\" },\\n  { :vega_lite ,  \"  ~> 0.1\" },', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 120}),\n",
       " Document(page_content=' { :kino_vega_lite ,  \"  ~> 0.1\" }\\n])\\nYou will be using Nx and EXLA for manipulating tensors to illustrate some\\nmathematical concepts. You should already be familiar with Kino. You will\\nuse some of the conveniences Kino provides for working with Livebook.\\nStbImage is a library for loading and manipulating images, as well as for\\nconverting images to and from Nx tensors. Finally, you need to install\\nVegaLite and its Kino counterpart. VegaLite is an Elixir library for\\nbuilding graphics. You’ll be working with VegaLite quite a bit throughout\\nthis book.\\nFinally, you’ll also want to set EXLA to be the default backend by running:\\nNx.default_backend(EXLA.Backend)\\nNow you’re ready to dive in.\\n[6]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 121}),\n",
       " Document(page_content='Linear Algebra Routines\\nLibraries that implement linear algebra routines and\\nmanipulations are the foundations of numerical computing.\\nGrowing access to computing resources in the early 1970s and\\nthe development of standardized implementations of high-\\nlevel programming languages like ALGOL and FORTRAN\\nmade it possible to implement standardized routines for\\nnumerical analysis. Handbook for Automatic Computation\\n[WR74] by Wilkinson and Reinsch was a first attempt at\\nimplementing common numerical procedures. Published in\\n1971, it outlines algorithms for solving matrix linear equation\\nand eigenvalue problems in ALGOL60 and served as\\ninspiration for the development of EISPACK and LINPACK\\n—two early attempts at creating and disseminating high-\\nquality mathematical software. EISPACK and LINPACK\\ninspired the development of MATLAB and were later\\ncombined into a single package, LAPACK, which is still\\nmaintained and in use today.\\nThe Building Blocks of Linear Algebra\\nThe fundamental object in linear algebra is the vector. You can think of a\\nvector as a collection of numbers that map to some real-world properties.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 122}),\n",
       " Document(page_content='There are different, but somewhat complementary meanings for vectors in\\ncomputer science, physics, and math. Nx doesn’t explicitly differentiate\\nbetween scalars, vectors, and matrices. Everything in Nx is a tensor. Vectors\\nare represented as Nx tensors with a rank of 1, or a single-dimensional\\ntensor. In this book, when something is referred to as a vector, it means it’s\\na rank-1 tensor. To help visualize this concept, run the following cell to\\ncreate a vector with Nx:\\na = Nx.tensor([1, 2, 3])\\nb = Nx.tensor([4.0, 5.0, 6.0])\\nc = Nx.tensor([1, 0, 1],  type:  { :u , 8})\\nIO.inspect a,  label:   :a \\nIO.inspect b,  label:   :b \\nIO.inspect c,  label:   :c \\nThis returns the following tensors:\\na: #Nx.Tensor<\\n  s64[3]\\n  [1, 2, 3]\\n>\\nb: #Nx.Tensor<\\n  f32[3]\\n  [4.0, 5.0, 6.0]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 123}),\n",
       " Document(page_content='c: #Nx.Tensor<\\n  u8[3]\\n  [1, 0, 1]\\n>\\nNotice how each tensor has a single dimension with a size of 3 and is\\nrepresented as a list of values. Often in machine learning, these values map\\nto real-world properties. For example, if you were trying to predict whether\\na stock will move up or down, you might choose to represent stocks as a\\nvector of stock properties such as current price, price-to-earnings, market\\ncap, and so on. You could then represent individual stocks as vectors:\\ngoog_current_price = 2677.32\\ngoog_pe = 23.86\\ngoog_mkt_cap = 1760\\ngoog = Nx.tensor([goog_current_price, goog_pe, \\ngoog_mkt_cap])\\nAnother object in linear algebra is the scalar. Scalars are just numbers.\\nIn the previous chapter, you learned that scalars are represented as zero-\\ndimensional tensors, for example, tensors with no shape. Most of the time\\nin machine learning and linear algebra, you’re concerned with a collection\\nof numbers, rather than individual numbers. But understanding scalars and\\nhow to create them is important.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 124}),\n",
       " Document(page_content='With Nx, you can either represent scalars as tensors or leave them as regular\\nnumbers:\\ni_am_a_scalar = Nx.tensor(5)\\ni_am_also_a_scalar = 5\\nNx will interpret both variables as scalars when performing operations. For\\nreadability, it’s typically best not to wrap scalars in the tensor constructor.\\nScalars are useful for encoding things such as discount rates or factors.\\nWhen modeling multiple real-world objects as vectors, you’ll probably find\\na need to represent the objects as a collection in two dimensions. A two-\\ndimensional array of numbers, or a package of vectors, is known as a\\nmatrix. For example, if you have information regarding multiple stocks, you\\ncan encode the entire collection of data as a matrix. In Nx, you can think of\\na matrix as a tensor with rank-2, for example, a tensor with two dimensions.\\nRun the following code to see what a matrix looks like in Nx:\\ngoog_current_price = 2677.32\\ngoog_pe = 23.86\\ngoog_mkt_cap = 1760\\nmeta_current_price = 133.93\\nmeta_pe = 11.10\\nmeta_mkt_cap = 360', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 125}),\n",
       " Document(page_content='stocks_matrix = Nx.tensor([\\n  [goog_current_price, goog_pe, goog_mkt_cap],\\n  [meta_current_price, meta_pe, meta_mkt_cap]\\n])\\nIO.inspect stocks_matrix\\nThe snippet returns the following output:\\n#Nx.Tensor<\\n  f32[2][3]\\n  [\\n    [2677.320068359375, 23.860000610351562, \\n1760.0],\\n    [133.92999267578125, 11.100000381469727, \\n360.0]\\n  ]\\n>\\nNotice that a matrix is represented as a two-dimensional grid of numbers.\\nThe concept of matrices arises often in linear algebra as they can be\\nconveniently used to represent linear transformations. You’ll learn more\\nabout linear transformations in  Linear Transformations .', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 126}),\n",
       " Document(page_content='Important Operations in Linear Algebra\\nNow that you understand the basic building blocks of linear algebra and\\nhow they map to Nx data structures, your next task is to learn about some of\\nthe most common operations in linear algebra and how they map to Nx.\\nThis discussion isn’t meant to be a comprehensive introduction to linear\\nalgebra and the mechanics of linear algebra operations, but it’ll get you\\nstarted. If you want to know more, read The Matrix Cookbook [PP08].\\nVector Addition\\nMaintaining the property of vector addition is a fundamental characteristic\\nof a vector. For Nx vectors, vector addition computes an element-wise sum\\nthat adds individual components of vectors to obtain a new vector. An\\nelement-wise sum adds individual elements along a tensor. Vector addition\\nis an element-wise sum of rank-1 tensors. For example, imagine you have\\ntwo vectors encoded as Nx tensors which represent sales of multiple\\ndifferent products in a catalog across two different days:\\nsales_day_1 = Nx.tensor([32, 10, 14])\\nsales_day_2 = Nx.tensor([10, 24, 21])\\nIf you wanted to draw conclusions about sales in the aggregate, you could\\nsimply sum the two vectors together using vector addition:\\ntotal_sales = Nx.add(sales_day_1, sales_day_2)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 127}),\n",
       " Document(page_content='This returns the following:\\n#Nx.Tensor<\\n  s64[3]\\n  [42, 34, 35]\\n>\\nVector addition adds the individual components of a vector to obtain a new\\nvector. Intuitively, total sales represent the sales per-product across both\\ndays. This definition of addition also extends to matrices and other higher-\\ndimensional tensors in Nx. Recall from the previous chapter that Nx tensors\\nalso obey specific broadcasting rules. With broadcasting, you can\\nconveniently add vectors to scalars, vectors to matrices, scalars to matrices,\\nand so on.\\nScalar Multiplication\\nThe other fundamental property of a vector is scalar multiplication. In Nx,\\nscalar multiplication is a broadcasted multiplication of a scalar and a vector.\\nImagine you’re trying to draw more conclusions about the sales data\\ndiscussed in  Vector Addition . For example, you want to determine how your\\ngiven sales volume will translate to revenue. You assume about 10% of\\nsales will result in an item being returned, which means that only about\\n90% of each product’s sales will count toward revenue. You can represent\\nthis discounting with scalar multiplication:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 128}),\n",
       " Document(page_content='sales_day_1 = Nx.tensor([32, 10, 14])\\nsales_day_2 = Nx.tensor([10, 24, 21])\\ntotal_sales = Nx.add(sales_day_1, sales_day_2)\\nkeep_rate = 0.9\\nunreturned_sales = Nx.multiply(keep_rate, \\ntotal_sales)\\nThis returns the following:\\n#Nx.Tensor<\\n  f32[3]\\n  [37.79999923706055, 30.599998474121094, 31.5]\\n>\\nOf course, you can’t sell a percentage of an item. But scalar multiplication\\nis still useful for making a projection into the future. Element-wise\\nmultiplication also generalizes out to higher-dimensional tensors in Nx. But\\nit wouldn’t be correct to call the element-wise multiplication of two N-\\ndimensional tensors a scalar multiplication. Instead, you’ll commonly see\\nelement-wise multiplication referred to as a Hadamard product—a binary\\noperation that multiplies matrices of the same size, element-wise.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 129}),\n",
       " Document(page_content='The Hadamard Product\\nThe Hadamard product is named after mathematician Jacques\\nHadamard, who is recognized as one of the mathematicians\\nwho originally described the properties of the operation.\\nYou can use element-wise multiplication to convert sales totals to revenue.\\nImagine you have another vector that represents the price of each product.\\nYou can multiply price by unreturned sales to get revenue per product:\\nprice_per_product = Nx.tensor([9.95, 10.95, 5.99])\\nrevenue_per_product = \\nNx.multiply(unreturned_sales, price_per_product)\\nThis returns the following:\\n#Nx.Tensor<\\n  f32[3]\\n  [376.1099853515625, 335.0699768066406, \\n188.68499755859375]\\n>\\nTranspose', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 130}),\n",
       " Document(page_content='The transpose of a matrix is accomplished by flipping it along its diagonal,\\nwhere the rows and columns of the matrix are swapped. In Nx, you can take\\nthe transpose of a matrix using Nx.transpose/2.\\nImagine you’ve decided to encode your sales data into a matrix. Each row\\nof the matrix represents a day of sales while each column represents the\\nsales data for a given product. You might want to flip this relationship, such\\nthat each column represents a day of sales and each row represents a\\nproduct:\\nsales_matrix = Nx.tensor([\\n  [32, 10, 14],\\n  [10, 24, 21]\\n])\\nNx.transpose(sales_matrix)\\nThis returns the following:\\n#Nx.Tensor<\\n  s64[3][2]\\n  [\\n    [32, 10],\\n    [10, 24],\\n    [14, 21]\\n  ]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 131}),\n",
       " Document(page_content='>\\nNotice the rows and columns switch places. The transpose operation also\\ngeneralizes to higher dimensions. Rather than simply fixing rows and\\ncolumns, you can permute the dimensions of a tensor. For vectors or one-\\ndimensional tensors in Nx, the transpose operation is an identity operation,\\nreturning the tensor identical to the input tensor because Nx doesn’t\\ndifferentiate between row vectors and column vectors. You can see the\\ndifference by running the following code:\\nvector = Nx.tensor([1, 2, 3])\\nNx.transpose(vector)\\nThis code returns the following output:\\n#Nx.Tensor<\\n  s64[3]\\n  [1, 2, 3]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 132}),\n",
       " Document(page_content='Row Vectors and Column Vectors\\nIn linear algebra, it’s common to represent vectors as a\\ncolumn of scalar entries. You can distinguish between vectors\\nwith a single column and vectors with a single row by\\nreferring to them as row vectors and column vectors\\nrespectively. Nx doesn’t truly distinguish between row vectors\\nand column vectors. However, languages such as MatLab do.\\nIf the distinction is important in your calculations, you can\\ncreate a rank-2 tensor where one of the dimensions is 1.\\nLinear Transformations\\nIn machine learning, you’ll often see mentions of linear transformations. A\\nlinear transformation, also known as a linear map, is a function that maps\\ninputs to outputs. Linear transformations are special because they preserve\\nlinearity. What this means is that they produce a different representation of\\nan input, while still preserving its fundamental properties. This type of\\nfunction is useful, for example, when you need to get a different or more\\nconvenient representation of the input.\\nImagine you have the following image of a cat, and you want to invert the\\ncolors.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 133}),\n",
       " Document(page_content='You can achieve this task using a linear transformation:\\ninvert_color_channels = Nx.tensor([\\n  [-1, 0, 0],\\n  [0, -1, 0],\\n  [0, 0, -1]\\n])\\n\"  Cat.jpg\" \\n|> StbImage.read_file!()\\n|> StbImage.resize(256, 256)\\n|> StbImage.to_nx()\\n|> Nx.dot(invert_color_channels)\\n|> Nx.as_type({ :u , 8})', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 134}),\n",
       " Document(page_content='|> Kino.Image.new()\\nAfter doing so, you’ll have a newly color-inverted image:\\nMost visual transformations map directly to linear transformations. You can\\nuse linear transformations to reverse images, rotate images, shear images,\\nand more. Every linear transformation is associated with a matrix and can\\nbe thought of as applying matrix multiplication between the input matrix\\nand the transformation matrix.\\nIn Nx, matrix multiplications are done via the Nx.dot/2 operator. Dot\\nproducts between vectors are treated as normal dot products would be—\\ntaking an element-wise product between each vector and then computing\\nthe sum. Vector-matrix and matrix-matrix dot products are computed\\naccording to the rules of matrix multiplication, meaning that the last', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 135}),\n",
       " Document(page_content='dimension of the left-hand side of the operation contracts along the first\\ndimension of the right-hand side of the operation. You can see the semantics\\nof the Nx dot product in action in the following code snippet:\\nvector = Nx.dot(Nx.tensor([1, 2, 3]), \\nNx.tensor([1, 2, 3]))\\nvector_matrix = Nx.dot(Nx.tensor([1, 2]), \\nNx.tensor([[1], [2]]))\\nmatrix_matrix = Nx.dot(Nx.tensor([[1, 2]]), \\nNx.tensor([[3], [4]]))\\nvector |> IO.inspect( label:   :vector )\\nvector_matrix |> IO.inspect( label:   :vector_matrix )\\nmatrix_matrix |> IO.inspect( label:   :matrix_matrix )\\nThis code returns the following output:\\nvector: #Nx.Tensor<\\n  s64\\n  14\\n>\\nvector_matrix: #Nx.Tensor<\\n  s64[1]\\n  [5]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 136}),\n",
       " Document(page_content='matrix_matrix: #Nx.Tensor<\\n  s64[1][1]\\n  [\\n    [11]\\n  ]\\n>\\nDon’t worry too much if the semantics of the dot product doesn’t make\\nmuch sense. You’ll get plenty of practice working with them throughout this\\nbook.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 137}),\n",
       " Document(page_content='Thinking Probabilistically\\nNow that you understand how linear algebra helps you model data, you’re\\nready to learn how probability helps you think about data and make\\npredictions from it.\\nA crucial aspect of modern approaches to artificial intelligence is the\\nacceptance of probabilistic methods over logic-based approaches. Earlier\\ngenerations of researchers believed the secret to creating artificial\\nintelligence was finding enough good formal rules to capture patterns in\\nlanguage, speech, and other areas. Unfortunately, logic-based systems grow\\nexponentially with the difficulty of the problem, and they don’t adapt well\\nto chaotic systems. Modern approaches succeed because they embrace\\nuncertainty as a rule.\\nThere are three primary tools used in machine learning to reason about data\\nand make predictions: probability theory, decision theory, and information\\ntheory. In this section, you’ll learn about all three and how they lay the\\nfoundation for modern machine learning algorithms. Once again, this isn’t a\\ncomprehensive discussion. If you want to dive deeper into the topic, read\\nPattern Recognition and Machine Learning [Bis06] or Probabilistic\\nMachine Learning [Mur22].\\nReasoning About Uncertainty', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 138}),\n",
       " Document(page_content='Probability theory is a framework for understanding uncertainty. Recall that\\nearlier approaches to artificial intelligence relied on enumerating formal\\nlogic-based rules to come to conclusions about data. With perfect\\ninformation and infinite computing power, logic-based approaches would\\nyield perfect results. In this context, “perfect information” means you have\\naccess to every scenario your algorithm will be exposed to in practice.\\nImagine you were given the task of designing a machine that sorts coins.\\nYou were guaranteed your machine will only ever be given US\\ndenominations—specifically quarters, dimes, nickels, and pennies. You\\nhave access to “perfect information,” and you can design a system that\\nhandles all four cases correctly. For example, because you know the\\nspecifications of each case ahead of time, you can design a machine that\\nsorts coins based on their size. Unfortunately, this “perfect information”\\nscenario requires strong assumptions that break down in real-world\\ncontexts:\\n1. You assume your machine will only ever encounter four coins in US\\ndenominations. What if somebody accidentally gives your machine a\\nhalf-dollar or a gold-dollar? What happens if somebody gives your\\nmachine a non-US coin?\\n2. You assume all of the coins fed to your machine will perfectly match the\\nspecs of one of the four scenarios you expect. What if some of the coins\\nare damaged? What if some of the coins are stuck together? What if the\\nUS mint incorrectly mints a quarter the size of a dime?', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 139}),\n",
       " Document(page_content='3. Is sorting by size a perfect method? Is it possible you’ve built the\\nmachine imperfectly and occasionally a dime might slip in with the\\npennies?\\nYou’ll see that uncertainty is a fact of life, and while in some applications it\\nmakes sense to ignore uncertainty, in others it’s best to fully embrace it. For\\nexample, if you wanted to design an algorithm that detects images that\\ncontain birds, you couldn’t possibly enumerate every species of bird, their\\npossible orientations in the images, the environments the images were taken\\nin, and so on. Machine learning shines in uncertain situations because\\nuncertainty is a built-in assumption.\\nVery few applications are absolutely certain. In the Deep Learning Book\\n[GBC16], the authors identify three sources of uncertainty that will pop up\\nin every problem:\\n1. Inherent stochasticity. Some problems are inherently stochastic. That\\nmeans a source of uncertainty or randomness is built in. No matter what,\\nthe outcome isn’t deterministic.\\n2. Incomplete observability. If you don’t know all of the variables dictating\\nthe behavior of a system, the system will always have an element of\\nuncertainty. In machine learning, you’ll never have access to every\\nvariable that dictates an outcome.\\n3. Incomplete modeling. Some models discard information intentionally.\\nFor example, it’s common to downsample images for faster processing.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 140}),\n",
       " Document(page_content='Downsampling intentionally discards some information, and thus you\\ncannot fully model the problem at hand.\\nGiven you’ll inevitably encounter sources of uncertainty when creating\\nmodels, you need to understand how to quantify uncertainty, interpret\\nuncertain quantities, and use uncertainty to your advantage.\\nIn mathematics and machine learning, probability is a measure or\\nquantification of uncertainty. You’re probably familiar with probability in\\nthe traditional sense. No doubt, you were introduced to probability as the\\nlikelihood of an event occurring in the long run. Your understanding of\\nprobability equips you to answer questions such as, if you have two red\\nballs and three blue balls in a hat, what is the probability you’ll draw a red\\nball? Probability from the frequentist perspective as it’s presented in the\\nprevious question represents the long-run frequencies of events. The\\nfrequentist interpretation of probability places an emphasis on repeated\\ntrials. As an example, in a Livebook cell, run the following code:\\nsimulation =  fn  key ->\\n  {value, key} = Nx.Random.uniform(key)\\n   if  Nx.to_number(value) < 0.5,  do : {0, key},  else \\n: {1, key}\\nend ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 141}),\n",
       " Document(page_content='This function creates a scalar random variable using the Nx.Random module\\nand converts it to a regular Elixir number. Note that Nx makes use of\\nstateless pseudo-random number generators (PRNGs), which means you\\nneed to explicitly pass a key every time you want a random number.\\nNx.Random.uniform/1 and similar functions return an updated key that you can\\npass to a subsequent random function to get a different result. Passing the\\nsame key to one of Nx’s random functions with the same parameters will\\nresult in deterministic behavior.\\nAfter creating a random number, the simulation checks the variable. If the\\nvariable is less than 0.5, it returns 0, and if the variable is greater than or\\nequal to 0.5, it returns 1. The simulation also returns the updated key for use\\nin subsequent runs of the simulation.\\nAssuming the random number generator is perfectly random, how many\\ntimes do you think this function will return 1 if you run it 10 times? 100\\ntimes? 1,000 times? 10,000 times? Run the following code to see the\\nresults:\\nkey = Nx.Random.key(42)\\nfor n <- [10, 100, 1000, 10000]  do \\n  Enum.map_reduce(1..n, key,  fn  _, key -> \\nsimulation.(key)  end )\\n  |> elem(0)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 142}),\n",
       " Document(page_content=' |> Enum.sum()\\n  |> IO.inspect()\\nend \\nAnd, after about two minutes, you’ll see this result:\\n6\\n49\\n501\\n5025\\nNotice how the frequency of a 1 occurring starts to converge to about half\\nthe time as you increase the number of trials. This means that if you were to\\nrun this simulation forever, one out of every two events would be a 1.\\nIn this example, you can say the probability of the simulation returning a 1\\nis 0.5. From the frequentist perspective, a probability of 0.5 means that for\\nan infinite number of trials, an event would occur exactly one time out of\\nevery two trials. The probability of the event occurring in an infinite time\\nframe gives you a framework for discussing the expected results of a single\\nsimulation. The result of a single simulation has uncertainty built in. You\\ncan’t come up with a logic-based rule to assess the result of individual runs.\\nInstead, you can quantify the results of a single simulation with a\\nprobability that helps you express the uncertainty of the event.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 143}),\n",
       " Document(page_content='\\xa0\\nNot So Random\\nIn reality, random number generators on the computer don’t\\nproduce truly random numbers, so the true probability of\\ndrawing a 1 is likely slightly skewed in one direction or\\nanother—albeit likely within a small level of precision.\\nRandom number generators on computers are often pseudo-\\nrandom number generators with a very large period. That\\nmeans they start repeating the same sequence after a finite\\nnumber of draws. On an infinite scale, the probability of a\\nPRNG drawing a 1 is equal to the observed probability of it\\ndrawing a 1 during one full period.\\n\\xa0\\nAll About Bayes\\nThe frequentist interpretation of probability is useful and easy to understand\\nin scenarios that are easily repeatable, but some applications aren’t\\nconcerned with easily repeatable trials. For example, if a doctor predicts\\nthere’s a 60% chance a patient has some disease, what do they really mean?\\nThe doctor isn’t really making a statement about the patient having a\\ndisease over an infinite number of trials (unless they believe in an infinite', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 144}),\n",
       " Document(page_content='number of universes). Instead, the doctor is presenting a quantification of\\nuncertainty that the patient has the disease given the information available\\nto them. The bayesian perspective is fundamentally tied to available\\ninformation, and probabilities are updated as more information becomes\\navailable.\\nOne famous example of the bayesian interpretation in action is the Monty\\nHall problem. The Monty Hall problem is named after the host of “Let’s\\nMake a Deal.” In the show, contestants are presented with a choice between\\nthree doors. One door has a prize behind it, such as a sports car, while the\\nother doors have “Zonks,” or prizes that you didn’t want. After a contestant\\npicks a door, the host, Monty Hall, opens one of the unpicked doors that\\ncontains a Zonk. Then Monty gives the contestant an opportunity to switch\\nbetween the original door and the other remaining unopened door.\\nThe problem is concerned with the optimal strategy when you’re presented\\nwith the option to switch or keep the original door after being shown a\\nZonk behind one of the doors. Mathematically speaking, you should always\\nswitch because the probability the prize is behind the unopened door is 2/3\\n—much higher than the probability that the prize is behind your originally\\npicked door. At first, this might not make much sense. The probability of\\neach door containing the prize started at 1/3. But remember, you need to\\nupdate your assessment based on the available information. The probability\\nthe prize is behind the door the contestant picks is 1/3, and the probability\\nthe prize isn’t behind the door the contestant picks is 2/3. After Monty Hall', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 145}),\n",
       " Document(page_content='opens one of the doors and reveals a Zonk, the probability the prize isn’t\\nbehind your door “concentrates” on the remaining unpicked door.\\nThe Monty Hall problem is a classic illustration of Bayes’ Theorem in\\naction. Bayes’ Theorem describes the conditional probability of an event\\nbased on available information or the occurrence of another event. Bayes’\\nTheorem is a rule describing how probabilities should be updated in the\\nface of new evidence.\\nYou can see this outcome visually in the table.\\nDoor with car I chose If stay If switch\\nA A Win car Win goat\\nA B Win goat Win car\\nA C Win goat Win car\\nB A Win goat Win car\\nB B Win car Win goat\\nB C Win goat Win car', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 146}),\n",
       " Document(page_content='Door with car I chose If stay If switch\\nC A Win goat Win car\\nC B Win goat Win car\\nC C Win car Win goat\\nNotice how in six of the nine scenarios where you choose to switch, you\\nwin a car.\\n\\xa0\\nAs you read this book, you’ll get a stronger feel for the mechanics of\\nmanipulating probabilities. Additionally, you’ll see why the bayesian\\nperspective on probability fits in nicely with machine learning. Humans are\\nnatural practitioners of bayesian probability. As you go through life gaining\\nnew information, you update your estimates. You are making inferences or\\npredictions about your surroundings. Machine learning models do the same\\nthing. But what do you do after making predictions? You make decisions.\\nMaking Decisions', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 147}),\n",
       " Document(page_content='Imagine that you’ve been told that your favorite sports team has a 60%\\nchance of winning their next game. You’re then asked to make a bet on that\\ngame. How do you decide? You use decision theory. Decision theory\\nprovides a framework for acting optimally in the presence of uncertainty.\\nWhen making decisions in the presence of uncertainty, you first need to\\nconsider your goals. In the betting example, your goal is to make money, so\\nit might not make sense to bet on the team with a 60% chance of winning\\ntheir next game because the potential reward may be too low. This same\\nlogic applies to machine learning models as well. You need to consider the\\nobjective of your model to make decisions after inference.\\nTypically, making decisions after inference is easier. For example, if you\\nhave a model that predicts the probability that an image is an apple versus\\nan orange, and your goal is to maximize accuracy, you’d want to classify\\nthe object as an apple if the probability that it’s an apple is greater than 0.5.\\nIf it’s not, classify the image as an orange. This rule, based on inference\\nprobabilities, represents a decision boundary. Imagine a line in space: if\\nyour prediction falls on one side, you take one action, and if it falls on the\\nother, you take the other action.\\nOf course, there are also more complex scenarios to consider in machine\\nlearning. For example, rather than only maximizing accuracy, you might\\nwant to also maximize reward or minimize cost. Again, if you imagine the\\nbetting example, choosing to bet on the given sports team might not be the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 148}),\n",
       " Document(page_content='best choice even though you’re more certain your team will win than lose.\\nThis is because the action has an associated cost or risk.\\nA final scenario to consider when making decisions from probabilities\\ninvolves making no decision at all. Depending on the application, it might\\nmake sense to take no action based on a given probability. For example,\\nimagine your apples and oranges classifier is given a picture of a banana,\\nand it spits out a probability that makes you think it’s unsure about which\\nclass the image belongs to. In this instance, it would be correct to not\\nclassify the image as an apple or an orange.\\nLearning from Observations\\nThe final tool you should be aware of in probabilistic machine learning is\\ninformation theory. Information theory provides a framework for reasoning\\nabout information in systems. Information theory was pioneered by\\nmathematician Claude Shannon in his Mathematical Theory of\\nCommunication [Sha48].\\nInformation theory is the mathematical study of coding information in\\nsequences and symbols and the study of how much information can be\\nstored and transmitted in these mediums. Typically, information theory is\\nthought of in the context of communications and signals; however,\\ninformation theory provides a strong foundation for the development of\\nmachine learning algorithms.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 149}),\n",
       " Document(page_content='To understand how information theory fits in, let’s consider an example.\\nSay you always see your neighbor walking by your house every day at the\\nsame time. Without fail, day after day, your neighbor walks by at the same\\ntime. What new information do you glean from seeing your neighbor walk\\nby every day? Outside of what you immediately observe, you can’t gather\\nmuch else. Your neighbor’s walks are essentially a certainty. But what if\\none day you don’t see your neighbor walk by? What do you learn? Has\\nsomething happened? How surprised are you? In information theory, we are\\nconcerned with the degree of surprise of a particular observation. Observing\\nyour neighbor on their daily walk isn’t surprising. Not observing your\\nneighbor on their daily walk is surprising.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 150}),\n",
       " Document(page_content='Tracking Change\\nIf you’ve dabbled a little in machine learning, you more than likely have\\nheard of words like derivative, gradient, and automatic differentiation. All\\nof these terms come from vector calculus.\\nVector calculus is concerned with the differentiation and integration of\\nvector fields or functions. If you’re not a math person, all of this probably\\nsounds a bit overwhelming. Fortunately, you don’t need to have an intimate\\nunderstanding of these concepts; you only need to be familiar with them. If\\nyou don’t get confused when somebody says “differentiable function,” you\\nprobably know enough calculus for machine learning. In this section, you’ll\\ngo through a bit of vocabulary and see how vector calculus relates to\\nmachine learning. Then you’ll see how Nx does most of the heavy lifting\\nfor you.\\nUnderstanding Differentiation\\nThe most important concept from calculus to understand in machine\\nlearning (and to even understand in calculus) is the derivative. A derivative\\nis a measure of the instantaneous rate of change of a function. A classic\\nexample of a derivative is position and velocity. Let’s say you have a\\nfunction that plots your position on a fixed axis over time. The derivative of\\nyour position function gives you your velocity, or how fast your position is\\nchanging at a given point in time. For example, if you were plotting the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 151}),\n",
       " Document(page_content='height of a rocket over time, its position would correspond to its height, and\\nthe velocity would correspond to how fast the rocket is moving (for\\nexample, how fast the position is changing).\\nYou can visualize the derivative of a function at a given point as the line\\nthat runs tangent to that point or just touches it:\\nBut why should you care so much about derivatives? If you have taken a\\ncalculus class, you might remember using derivatives to find local (and\\nglobal) maxima and minima by hand. Visually, you can see that the\\nderivative at the maximum and minimum of this function shown on the\\nfollowing interval is 0:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 152}),\n",
       " Document(page_content='Given a function, you can compute its derivative by hand to determine\\nwhich input maximizes or minimizes the function. For example, imagine\\nyou ran a berry farm and had access to an analytic which projected profits\\nbased on the number of berry trees planted for a given year. To maximize\\nprofits for the year, you could compute the derivative of your profit function\\nto determine the quantity of berry trees that maximizes profit. Nx makes the\\nprocess of finding derivatives easy. For example, consider you have this\\nfunction which models the profit of your berry farm given an input variable\\nx for the number of berry trees planted in a year:\\ndefmodule  BerryFarm  do \\n   import  Nx.Defn\\n  defn profits(trees)  do \\n    trees\\n    |> Nx.subtract(1)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 153}),\n",
       " Document(page_content='   |> Nx.pow(4)\\n    |> Nx.negate()\\n    |> Nx.add(Nx.pow(trees, 3))\\n    |> Nx.add(Nx.pow(trees, 2))\\n   end \\nend \\nYou can visualize this function with VegaLite, Elixir’s bindings to the Vega\\nplotting library, by running this code:\\ntrees = Nx.linspace(0, 4,  n:  100)\\nprofits = BerryFarm.profits(trees)\\nalias VegaLite,  as:  Vl\\nVl.new( title:   \"  Berry Profits\" ,  width:  1440,  \\nheight:  1080)\\n|> Vl.data_from_values(%{\\n   trees:  Nx.to_flat_list(trees),\\n   profits:  Nx.to_flat_list(profits)\\n})\\n|> Vl.mark( :line ,  interpolate:   :basis )\\n|> Vl.encode_field( :x ,  \"  trees\" ,  type:   \\n:quantitative )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 154}),\n",
       " Document(page_content='|> Vl.encode_field( :y ,  \"  profits\" ,  type:   \\n:quantitative )\\nAfter running it, you’ll see the following plot:\\nUsing Nx, you can compute and visualize the derivative of your profits, or\\nhow your profits change with respect to the number of trees by adding the\\nfollowing function to your BerryFarm module:\\ndefmodule  BerryFarm  do \\n   import  Nx.Defn', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 155}),\n",
       " Document(page_content=' defn profits(trees)  do \\n    -((trees - 1) ** 4) + (trees ** 3) + trees ** \\n2\\n   end \\n  defn profits_derivative(trees)  do \\n    grad(trees, &profits/1)\\n   end \\nend \\nNow you can plot the profits function and its derivative overlayed by\\nrunning this:\\ntrees = Nx.linspace(0, 3,  n:  100)\\nprofits = BerryFarm.profits(trees)\\nprofits_derivative = \\nBerryFarm.profits_derivative(trees)\\nalias VegaLite,  as:  Vl\\ntitle =  \"  Berry Profits and Profits Rate of Change\" \\nVl.new( title:  title,  width:  1440,  height:  1080)\\n|> Vl.data_from_values(%{\\n   trees:  Nx.to_flat_list(trees),', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 156}),\n",
       " Document(page_content='  profits:  Nx.to_flat_list(profits),\\n   profits_derivative:  \\nNx.to_flat_list(profits_derivative)\\n})\\n|> Vl.layers([\\n  Vl.new()\\n  |> Vl.mark( :line ,  interpolate:   :basis )\\n  |> Vl.encode_field( :x ,  \"  trees\" ,  type:   \\n:quantitative )\\n  |> Vl.encode_field( :y ,  \"  profits\" ,  type:   \\n:quantitative ),\\n  Vl.new()\\n  |> Vl.mark( :line ,  interpolate:   :basis )\\n  |> Vl.encode_field( :x ,  \"  trees\" ,  type:   \\n:quantitative )\\n  |> Vl.encode_field( :y ,  \"  profits_derivative\" ,  \\ntype:   :quantitative )\\n  |> Vl.encode( :color ,  value:   \"  #  ff0000\" )\\n])\\nThen, you’ll see the following plot:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 157}),\n",
       " Document(page_content='One thing you should notice is that the derivative of your profit function is\\n0 at exactly the same time your profit function maxes out. By tracing the\\nderivative until it reaches 0, you can find the maximums and minimums of\\nthe original function.\\nIn this berry farm example, you used the grad/2 function inside a numerical\\ndefinition. But why is this function called grad/2 and not derivative or\\nsomething else? grad/2 is an Nx function that’s capable of taking the\\ngradient of a function.\\nThe gradient is the direction of greatest change of a scalar function. You can\\nthink of a scalar function as an Nx function which takes a rank-N tensor as\\ninput and returns a scalar tensor as output. The gradient is like a heat-', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 158}),\n",
       " Document(page_content='seeking missile attracted to change. For high-dimensional inputs, you can\\ncompute the gradient to find the maximums and minimums. Consider an\\nexample. Say you are on a boat in a lake. You’ve been asked to find the\\nlocation of the deepest point in the lake as quickly as possible, using just a\\ndepth finder. You’re dropped off in the middle of the lake and don’t have\\ntime to map out the depth at each point in the lake. How do you solve this\\nproblem? One method is to compute the depth change in every direction\\nand always travel in the direction of steepest descent. That way you can\\nguarantee you’re always following a path toward a deeper point in the lake.\\nIf the bottom of the lake resembles a valley, you’re guaranteed to find the\\nsteepest point.\\nFinding the deepest point in the lake using the direction of steepest descent\\nis the same process used in gradient descent to optimize objective functions\\nin machine learning. Gradient descent is useful because you don’t often\\nhave access to the complete gradient of your objective function—just like\\nyou don’t have access to depth changes in the lake. Instead, you need to\\nlook at a few samples at a time and move accordingly.\\nThroughout this book, you’ll use gradients to optimize the parameters of\\nmodels in order to maximize performance on some machine learning tasks.\\nIn many of the examples you’ll implement in this book, the usage of grad/2\\nwill be abstracted away; but you’ll still need to break it out a few times. So,\\nwhat is grad/2 and how does it work?', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 159}),\n",
       " Document(page_content='Automatic Differentiation with defn\\nOne of the key features of Nx is its ability to perform automatic\\ndifferentiation. Automatic differentiation is the process of computing\\nderivatives from programs. It’s far more accurate than numerically\\napproximating gradients, and it’s far more efficient than symbolic\\ndifferentiation. Automatic differentiation is a rich field of research.\\nIn this section, you’ll see how you can use Nx to calculate the gradient of a\\nsimple function so you can use the power of Nx to optimize more complex\\nmodels later on. For a more in-depth treatment of automatic differentiation,\\ncheck out Evaluating Derivatives [GW08].\\nNx defn allows you to calculate the derivative of a scalar function using the\\ngrad function. Start by creating the following scalar function in a new\\nmodule:\\ndefmodule  GradFun  do \\n   import  Nx.Defn\\n  defn my_function(x)  do \\n    x\\n    |> Nx.cos()\\n    |> Nx.exp()\\n    |> Nx.sum()', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 160}),\n",
       " Document(page_content='   |> print_expr()\\n   end \\nend \\nThis function takes the element-wise cosine, followed by the element-wise\\nexponential of an input tensor, before taking the sum of the entire tensor.\\nBoth the element-wise cosine and element-wise exponential apply\\nmathematical functions to each element in the input. The element-wise\\ncosine applies the cosine function, while the element-wise exponential\\napplies the exponential function. Remember, gradients are the direction of\\nsteepest change for scalar functions, so your function must return a scalar\\ntensor. Additionally, gradients are only valid for continuous value functions,\\nso your function must return a floating-point type.\\nNext, add the following function to your module:\\ndefn grad_my_function(x)  do \\n  grad(x, &my_function/1) |> print_expr()\\nend \\nHere, you take the gradient of my_function and return its value. You also\\ninspect the gradient expression for my_function. The complete module looks\\nlike this:\\ndefmodule  GradFun  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 161}),\n",
       " Document(page_content='  import  Nx.Defn\\n  defn my_function(x)  do \\n    x\\n    |> Nx.cos()\\n    |> Nx.exp()\\n    |> Nx.sum()\\n    |> print_expr()\\n   end \\n  defn grad_my_function(x)  do \\n    grad(x, &my_function/1) |> print_expr()\\n   end \\nend \\nNow, run the following cell to obtain the gradient of a given input tensor:\\nGradFun.grad_my_function(Nx.tensor([1.0, 2.0, \\n3.0]))\\nYou’ll see these outputs:\\n#Nx.Tensor<\\n  f32', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 162}),\n",
       " Document(page_content=' Nx.Defn.Expr\\n  parameter a:0                            f32[3]\\n  b = cos a                                f32[3]\\n  c = exp b                                f32[3]\\n  d = sum c, axes: nil, keep_axes: false   f32\\n>\\n#Nx.Tensor<\\n  f32[3]\\n  Nx.Defn.Expr\\n  parameter a:0       f32[3]\\n  b = cos a           f32[3]\\n  c = exp b           f32[3]\\n  d = sin a           f32[3]\\n  e = negate d        f32[3]\\n  f = multiply c, e   f32[3]\\n>\\n#Nx.Tensor<\\n  f32[3]\\n  [-1.444406509399414, -0.5997574925422668, \\n-0.05243729427456856]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 163}),\n",
       " Document(page_content='First, take note of the two expression outputs. The first expression is the\\noriginal function, but you never invoked the original function. So how is\\nthis possible? Under the hood, Nx is actually executing a forward pass or\\nevaluation trace of the original function given your inputs. Nx computes\\nthe forward expression, and then recursively applies the chain-rule\\nbackwards through the forward trace to obtain the gradient of the original\\nfunction. This backwards evaluation of the chain-rule is called\\nbackpropagation or reverse-mode automatic differentiation. If you’re\\nfamiliar with the differentiation rules for the functions used in my_function/1,\\nthe connection between the first and second expressions should be obvious.\\nThe derivative of cos(x) is negate(sin(x)), and the derivative of exp(x) is exp(x).\\nYou can see Nx computed both of those derivatives for you, and correctly\\napplied the chain-rule by composing the derivatives of the decomposed\\nfunction. You can more easily visualize it here, where d indicates the\\nderivative of an expression:\\nparamater a:0   ->  da = a\\nb = cos a       ->  db = negate(sin(a))\\nc = exp b       ->  dc = multiply(exp(b), db)\\nNotice that computing the derivative of the original function is essentially\\nas easy as recursively computing derivatives of intermediate functions. You\\ntransform your original function into a new one by transforming the\\nintermediate expressions with established gradient rules. If you’re interested', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 164}),\n",
       " Document(page_content='in a deeper dive, the Nx source code is a great place to look and understand\\nwhat’s going on under the hood.\\nForward or Backward?\\nNx strictly uses reverse-mode automatic differentiation, which\\nis common in most automatic differentiation libraries. An\\nalternative mode of differentiation is forward-mode, in which\\nyou evaluate the derivative at the same time as you evaluate\\nthe actual expression itself. There are compute and memory\\ntrade-offs to both modes of differentiation. Some libraries,\\nsuch as JAX, can compute both forward- and reverse-mode\\ngradients and even combine modes for more efficient gradient\\ncomputations.\\nThe output tensor represents the vector with the maximum rate of change\\nwith respect to your original input. So why does this matter? Gradient-\\nbased optimization is a powerful form of optimization and is widely used in\\nmachine learning, especially deep learning. With automatic differentiation,\\nall you need is an objective function and you can optimize it with respect to\\nsome model parameters.\\nNx’s automatic differentiation can backpropagate through complex\\nexpressions and with wide operator coverage. In the spirit of the original', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 165}),\n",
       " Document(page_content='Autograd paper [Mac16], Nx allows you to focus on writing objective\\nfunctions for your models and to let Nx take care of the differentiation for\\nyou. There are a number of other unique applications of automatic\\ndifferentiation in simulations, graphics rendering, and more.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 166}),\n",
       " Document(page_content='[6]\\nWrapping Up\\nIn this chapter, you learned about the high-level mathematical concepts\\nused in machine learning algorithms. You now have a better understanding\\nof the foundations your future models will build on and the resources and\\ntools you’ll need to study the math further. In the next chapter, you’ll see\\nsome of this math tie together, and you’ll see how to frame machine\\nlearning problems as optimization problems.\\nFOOTNOTES\\nhttps://hexdocs.pm/vega_lite/VegaLite.html\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 167}),\n",
       " Document(page_content='Chapter 4\\nOptimize Everything\\n\\xa0\\nIn the previous chapter, you read about the three pillars of machine learning\\nmath: linear algebra, probability, and vector calculus. Even so, you may not\\nunderstand how they fit together to form the foundations of machine\\nlearning because you haven’t seen anything concrete. In this chapter, you’ll\\ndiscover how these topics help to frame machine learning as an\\noptimization problem, and more importantly, you’ll implement real\\ntechniques for solving that optimization problem.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 168}),\n",
       " Document(page_content='Learning with Optimization\\nOptimization is the search for the best. But how does this have anything to\\ndo with learning? Recall from Chapter 1,  Make Machines That Learn , that\\nyou can define a machine learning system as any computing system that’s\\ncapable of improving from experience with a specific task according to\\nsome arbitrary performance measure. The end goal is to have a system\\ncapable of providing predictions about unseen examples. Fundamentally,\\nyou want an interface that looks something like this:\\ndef  predict(input)  do \\n  label = do_something(input)\\n  label\\nend \\nAt its core, a trained machine learning system transforms inputs into labels.\\nBut what do these transformations look like? Is a machine learning\\nalgorithm synthesizing some sort of decision-making program under the\\nhood that assigns labels based on some ever-changing rules to each input?\\nFor some algorithms, you could justifiably argue that’s what’s happening.\\nHowever, it makes more sense to think about these transformations visually\\nas shown in the scatterplot.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 169}),\n",
       " Document(page_content='In this scatterplot, notice the relationship between inputs X and outputs Y.\\nHow would you draw a line to minimize the total distance between the line\\nand every example in that scatterplot? Then, using the line you just drew,\\nhow would you make predictions about the Y value of the new X inputs?', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 170}),\n",
       " Document(page_content='Take a step back, and you’ll start to see that the line you drew is a visual\\nrepresentation of the do_something function in predict.\\nWhen given an unseen input X, you can predict its expected output Y by\\nfollowing the line. If you can recall your primary school math lessons,\\nyou’ll remember that lines are nothing more than visualizations of two\\nparameters: m, or the slope of the line, and b, or the intercept of the line.\\nWhen you consider the line you drew is a visual representation of the\\ntransformation occurring in the predict function, you can understand the\\nfunction looks more like this:\\ndef  predict(input, m, b)  do \\n  label = m * input + b\\n  label\\nend \\nHere, input is equivalent to X; label is equivalent to Y; and m and b are the\\nparameters of the line you drew.\\nAll of this is to say that machine learning, in practice, revolves around\\nfinding input parameters that best transform inputs into labels. But the form\\nthese parameters assume won’t always look like m and b. This\\ndemonstration is a simplified example of linear regression in two\\ndimensions—more about linear regression in Chapter 5,  Traditional\\nMachine Learning .', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 171}),\n",
       " Document(page_content='In practice, the forms of the transformations and parameters in your\\nmachine learning algorithm will vary from problem to problem and will\\ntypically have much higher dimensionality. So, rather than fixating on m\\nand b, it’s more practical and generalized to say that predict looks something\\nmore like this:\\ndef  predict(input)  do \\n  label = f(input, params)\\n  label\\nend \\nHere, f represents the transformation performed by your machine learning\\nalgorithm of choice, and params represent the learned parameters for your\\nalgorithm. The goal is to find the parameters that best map inputs to labels\\nwith the function. Ideally, you want params for f that yield the best\\nperformance on the task at hand. You want to optimize params to achieve the\\nbest performance on your task.\\nMinimizing Risk to Maximize Performance\\nMachine learning boils down to finding the best parameters for a model.\\nRecall from Chapter 1,  Make Machines That Learn , that a machine learning\\nsystem’s performance is measured on an unseen test set. You also saw this\\nin the previous section of this chapter.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 172}),\n",
       " Document(page_content='If the goal is to end up with a predict function whose parameters are\\noptimized to correctly assign labels to unseen inputs, you might be\\nwondering: how do you optimize something you can’t observe?\\nRather than optimize for performance on an unobserved test set, you\\noptimize for performance on an available training set. In doing this, you\\nhope that the training set will capture enough of the features of the unseen\\ntest set that your model’s parameters are able to optimally assign labels to\\nunseen inputs.\\nDuring training, you define a loss function, or cost function, that you then\\nattempt to optimize according to your model’s parameters. The\\nparameterized loss function is your overall objective function. The\\nrelationship between the model, parameters, loss function, and objective\\nfunction in a machine learning problem looks something like the following\\nElixir code:\\ndef  model(params, inputs)  do \\n  labels = f(params, inputs)\\n  labels\\nend \\ndef  loss(actual_labels, predicted_labels)  do \\n  loss_value = measure_difference(actual_labels, \\npredicted_labels)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 173}),\n",
       " Document(page_content=' loss_value\\nend \\ndef  objective(params, actual_inputs, \\nactual_labels)  do \\n  predicted_labels = model(params, actual_inputs)\\n  loss(actual_labels, predicted_labels)\\nend \\nmodel is a function that uses params to transform inputs into labels. loss is a\\nfunction that measures the difference between an actual label and a\\npredicted label. Loss functions are supposed to give some indication or\\nmeasure of correctness.\\nSuppose you wanted to train a model to predict the number of total points\\nthat both teams will score in an NBA game. You could measure loss as the\\nsquared difference, or squared error, between the actual score of a given\\ngame and the model’s predicted score. If the model predicts a combined\\nscore will be 242, and the actual combined score is 245, the loss for this\\ngame is 9. You’ll see many different types of loss functions in  Defining\\nObjectives .\\nAlso, notice that the objective function is the loss function parameterized\\nwith your model and parameters. The objective function measures the loss\\nof your model with respect to true training examples. The process of', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 174}),\n",
       " Document(page_content='learning in machine learning boils down to optimizing an objective\\nfunction.\\nYou can optimize objective functions with any of a number of known\\noptimization techniques or solvers. Some objective functions can be solved\\nanalytically, while others require numerical optimization to find\\napproximate solutions to the optimization problem.\\nAs you now know, machine learning relies heavily on optimization—so\\nmuch that it might be difficult to see how machine learning differs from\\npure optimization or curve fitting. Consider this: machine learning is\\nconcerned with performance on unseen inputs. More specifically, it’s\\nconcerned with reducing generalization errors on unseen inputs.\\nIn statistical learning theory, generalization error is also known as risk, and\\nyour goal is to minimize risk. However, you can’t optimize for risk when\\nyou don’t have access to the entire distribution of possible inputs to your\\nmodel. So instead, you optimize empirical risk, which is the performance of\\nyour model on the empirical distribution or training set.\\nTheoretically, the empirical distribution has some equivalence to the true\\ndistribution of inputs, so minimizing empirical risk is equivalent to\\nminimizing true risk. In practice, there are always sources of errors when\\ntrying to capture infinite distributions with finite data and computing power.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 175}),\n",
       " Document(page_content='The process of minimizing empirical risk is known as empirical risk\\nminimization (ERM) and is a key distinction between pure optimization and\\nmachine learning. When you have access to an entire distribution, you can\\nminimize risk directly with optimization. When you don’t, you can\\nminimize risk indirectly by minimizing empirical risk. The first case is an\\noptimization problem; the second case is a machine learning problem.\\nDefining Objectives\\nThe definition of best parameters for a model is subject to the constraints\\nand objectives put forth by you when defining your machine learning task.\\nRemember from Chapter 1,  Make Machines That Learn , that you need to\\ndefine SMART tasks when designing machine learning systems. This need\\nbecomes evident during the training process of machine learning.\\nAdditionally, machine learning algorithms almost never directly optimize\\nfor the performance measures you actually care about. For example, you’ll\\nnot find a machine learning algorithm that directly optimizes for the\\naccuracy or classification error of a model. Instead, machine learning\\nalgorithms use surrogate loss functions.\\nSurrogate loss functions serve as proxies for true objectives. These loss\\nfunctions are typically much easier to optimize, and optimizing them also\\nindirectly optimizes the performance objectives you care about. For\\nexample, it’s much more common to use squared error as a surrogate for', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 176}),\n",
       " Document(page_content='absolute error because the squared error function has properties that make\\noptimization easier. The choice of surrogate loss function often depends on\\nthe machine learning task and the type of model.\\nFor the rest of this section, you’ll do the following:\\nStudy two of the most common loss functions used in machine learning.\\nLearn about why and when they’re used.\\nSee how they relate to some of the probability lessons you read about in\\nthe previous chapter.\\nLikelihood Estimation\\nAt this point, you might be wondering what loss functions actually measure.\\nTo answer that question, you first need to understand likelihood.\\nIn statistics, the likelihood function (or simply “likelihood”) describes the\\nprobability of observed data as a function of its parameters. Think of\\nlikelihood as a sort of truth meter. Statistical learning theory assumes there\\nexists some true function that generates data, maybe something like this:\\ndef  datagen(true_params)  do \\n  true_f(true_params)\\nend \\nIn this example, datagen is a data-generating function with a true model\\ntrue_f and true parameters true_params. The datagen/1 function can represent', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 177}),\n",
       " Document(page_content='any distribution of data.\\nLet’s assume datagen/1 is the data-generating function for a series of\\nwinning lottery numbers. You have a series of lottery numbers and want to\\nknow the likelihood that those are winning numbers. If you have access to\\ntrue_params and true_f, you can measure the likelihood by measuring the\\ndifference between your lottery numbers and multiple samples of winning\\nnumbers generated by datagen/1.\\nThis scenario is a bit unrealistic because you rarely have access to perfect\\ninformation about data-generating functions and their parameters. A more\\nlikely scenario is that you have a number of winning lottery tickets and\\nwant to recover the true function that generates winning lottery numbers.\\nYou start by creating an estimate of the original true_f and true_params:\\ndef  estimate_datagen(estimate_params)  do \\n  estimate_f(estimate_params)\\nend \\nNow, you can measure the likelihood of estimate_params generating your\\nwinning lottery tickets, which essentially amounts to measuring the\\nsimilarity or distance between your estimate of the data-generating function\\nand the true data-generating function.\\nAfter measuring the similarity between your estimated data-generator and\\nthe true data generator, you update estimate_params with some optimization', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 178}),\n",
       " Document(page_content='routine and try again. You optimize estimate_params to maximize the\\nlikelihood that estimate_params are the same parameters that generated your\\nwinning lottery tickets. In statistical learning theory, this optimization\\nprocess is known as maximum likelihood estimation (MLE). When using the\\nmaximum likelihood estimator, your loss function is a measure of similarity\\nbetween functions.\\nCross-Entropy\\nIn machine learning, you’ll commonly see the term cross-entropy used in\\nthe context of classification tasks. The meaning of cross-entropy in these\\ncontexts is given by the following function (assuming a binary\\nclassification task):\\ndefn binary_cross_entropy(y_true, y_pred)  do \\n  y_true * Nx.log(y_pred) - (1 - y_true) * \\nNx.log(1 - y_pred)\\nend \\nThis function assumes y_true is a label of 0 or 1, representing a binary class;\\nand y_pred is a probability between 0 and 1, predicting the true label y_true.\\nCross-entropy as a loss function can also be used for categorical\\nclassification tasks, where the number of possible classes extends out\\nindefinitely. The definition of cross-entropy as a loss function only for\\nclassification tasks isn’t necessarily true in an academic sense. However, its', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 179}),\n",
       " Document(page_content='usage in the context of classification problems is a convention in machine\\nlearning frameworks.\\nNumerical Stability\\nThe given implementation of binary cross-entropy may not be\\nnumerically stable in all cases. You should use the\\nimplementations from around the Nx ecosystem in Axon and\\nScholar.\\nMean Squared Error\\nMean squared error measures per-example loss as the average squared\\ndifference between true labels and predicted labels. You can implement\\nmean squared error in Nx using the following code:\\ndefn mean_squared_error(y_true, y_pred)  do \\n  y_true\\n  |> Nx.subtract(y_pred)\\n  |> Nx.pow(2)\\n  |> Nx.mean( axes:  [-1])\\nend \\nMean squared error is often used in place of absolute error when\\noptimizing objectives for regression tasks. Absolute error is the absolute', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 180}),\n",
       " Document(page_content='difference between true labels and predicted labels. It’s computed as\\nNx.abs(Nx.subtract(y_true, y_pred)). Mean squared error is always a\\nnonnegative value (assuming you’re only dealing with real values) because\\nthe difference between y_true and y_pred is squared.\\nConverging to a Solution\\nOne thing to be aware of as you continue your machine learning journey is\\nthe concept of convergence in the context of optimization and machine\\nlearning. Most of the optimization algorithms used in machine learning\\ndon’t have any guarantees of optimality. This means they don’t guarantee\\nthey’ll find the best set of model parameters.\\nThe best set of model parameters is the global optima of the entire\\nparameter space. With infinite parameter spaces and functions that cannot\\nbe optimized analytically, finding the global optima is impossible. Instead,\\noptimization routines are mechanical and designed to loop until some\\ndesired performance threshold.\\nWhile it’s nearly impossible to find the global optima in high-parameter\\nspaces, it’s possible for your models to converge on the local optima. Local\\noptima are localized regions of a parameter space that are better than\\nneighboring points.\\nConsider your model’s parameter space as the ocean: the global optima is\\nthe Mariana trench, and the local optima are the small trenches and valleys', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 181}),\n",
       " Document(page_content='in the ocean. Fortunately, you do not need to be concerned about not having\\nyour model converge on a global optima. Remember, when training a\\nmachine learning system, the goal is performance on a real-world task. Your\\nmodels will still perform well on the task without needing to converge on\\nglobal optima—or possibly any optima at all.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 182}),\n",
       " Document(page_content='Regularizing to Generalize\\nAs you learned in the previous section, the difference between machine\\nlearning and optimization is that machine learning is concerned with\\nperformance on unseen data. In Chapter 1,  Make Machines That Learn , you\\nlearned that this concept is known as generalization. Your objective is to\\ntrain models that generalize—performance on training data isn’t necessarily\\nimportant.\\nImagine you have two models. Model A performs noticeably worse on the\\ntraining set than model B but noticeably better on the test set than model B.\\nIn this scenario, which model would you prefer?\\nGiven your primary objective is performance on unseen data, for example,\\nthe test set, you’d likely prefer model B from a model performance\\nperspective. But how can you have a model that performs noticeably worse\\non the training set and noticeably better on the test set?\\nThe answer is regularization. Before diving into regularization, it’s\\nimportant to understand a few things about model capacity and the concepts\\nof overfitting and underfitting.\\nOverfitting, Underfitting, and Capacity\\nIn this chapter, you discovered two learning frameworks for solving\\nlearning problems as optimization problems: empirical risk minimization', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 183}),\n",
       " Document(page_content='(ERM) and maximum likelihood estimation (MLE). Both ERM and MLE\\noptimize model parameters with respect to errors on a training set, which\\nmeans they are explicitly fitting functions to match training data. Both\\ntechniques are prone to overfitting.\\nOverfitting is a scenario in which a trained model has low training error but\\nhigh generalization error. If you were to visually represent overfitting, it\\nwould look something like the following graph:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 184}),\n",
       " Document(page_content='Notice how the line traces perfectly through every point in the original\\ndataset. The traced function perfectly matches what’s available in the\\ncurrent dataset but likely won’t generalize to new data too well.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 185}),\n",
       " Document(page_content='It’s also possible for models to be prone to underfitting. Underfitting is a\\nscenario in which a model doesn’t even have low training error. You can see\\nhow overfitting and underfitting compare in the following image:\\nBoth overfitting and underfitting are typically functions of a model’s\\ncapacity. A model’s capacity is its ability to fit many different functions.\\nWhen designing machine learning models, you have to choose a hypothesis\\nspace of functions. For example, linear regression assumes the true model\\nyou want to fit is a linear model with respect to observation data. That\\nmeans that linear regression has a hypothesis space that’s restricted to linear\\nfunctions. Consider you assume a model that has the form of the equation\\nof a line:\\ndef  model(x, params)  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 186}),\n",
       " Document(page_content=' {m, b} = params\\n  m * x + b\\nend \\nThis model only has enough capacity to fit lines. You can increase its\\ncapacity to include quadratic functions by adding another term:\\ndef  model(x, params)  do \\n  {w0, w1, w2} = params\\n  w0 + w1 * x + w2 * x ** 2\\nend \\nNotice this process can go on forever—you can keep adding additional\\npolynomial terms and parameters to increase the capacity of the model to fit\\nnth-degree polynomials.\\nModels with higher capacity are more prone to overfitting than models with\\nlower capacity because models with high capacity can fit a wider range of\\ncomplex functions. For example, you can model all quadratic functions\\nwith a cubic function, but you cannot model all cubic functions with a\\nquadratic function. Designing good models means finding the balance\\nbetween scenarios of overfitting and underfitting.\\nDefining Regularization', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 187}),\n",
       " Document(page_content='In machine learning, regularization is any technique used to combat\\noverfitting, or more generally, any technique used to reduce generalization\\nerror, for example, error on the test set without necessarily reducing training\\nerror.\\nERM and MLE are both prone to overfitting. So regularization is a\\nnecessary step for many machine learning algorithms. There are many\\nforms of regularization you’ll use throughout this book. This section will\\ncover two that are more or less universal to all machine learning algorithms.\\nComplexity Penalties\\nComplexity penalties are a commonly used regularizer for training machine\\nlearning models that generalize. Complexity penalties impose a cost at\\nmodel evaluation time by adding a penalty term with some penalty weight.\\nThe penalty term is typically a function of the model’s parameters. For\\nexample, weight decay is a common regularization penalty that introduces a\\npenalty term equal to the L2-Norm of the model’s parameters.\\nThe L2-Norm is a term you can interpret as meaning “distance from the\\norigin.” For example, in two-dimensional space, the L2-Norm of the point\\n{3, 4} is 5, given by the distance formula:\\ndefn distance(x, y)  do \\n  x\\n  |> Nx.pow(2)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 188}),\n",
       " Document(page_content=' |> Nx.add(Nx.pow(y, 2))\\n  |> Nx.sqrt()\\nend \\nAssuming you’re using the mean squared error loss and weight decay, your\\ncomplete objective function might look something like the following:\\ndef  objective(params, actual_inputs, \\nactual_labels)  do \\n  lambda = 0.5\\n  predicted_labels = model(params, actual_inputs)\\n  loss = mean_squared_error(actual_labels, \\nactual_inputs)\\n  penalty = l2_norm(params)\\n  loss + lambda * penalty\\nend \\nlambda controls the strength of the weight-decay penalty, with higher lambda\\nhaving a higher impact on model training. Weight decay expresses the\\npreference for smaller model weights. Mathematically, it constrains the\\nfeasible parameter space of a given model to those that lie closer to the\\norigin. Intuitively, you can think of weight decay as penalizing a model that\\ngets too confident in particular weights.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 189}),\n",
       " Document(page_content='Early-Stopping\\nEarly-stopping is another commonly used regularizer for training machine\\nlearning models in which model training is stopped if overfitting is\\ndetected. It’s not possible to perfectly detect overfitting, so the typical\\napproach for monitoring overfitting is with a validation set.\\nValidation sets or holdout sets are portions of the original training data that\\naren’t used to train but to periodically monitor model performance.\\nTypically, if training errors continue to decrease and validation errors start\\nto increase, the model is starting to overfit. Early stopping halts the training\\nprocess before the model has a chance to overfit to the training set. You’ll\\nsee several examples of early stopping in practice in the deep learning\\nsection of this book.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 190}),\n",
       " Document(page_content='Descending Gradients\\nNow that you know a little more about optimization and how it applies to\\nmachine learning, it’s time to see how optimization for machine learning is\\nimplemented in the context of a real problem. In this section, you’ll\\nimplement stochastic gradient descent to estimate the true parameters,\\nwhich were used to generate some training data.\\nGradient descent is an iterative optimization routine using the gradients of a\\nfunction evaluated at a particular point to minimize that particular function.\\nAs you learned in the previous chapter, the gradients of a scalar function are\\nindicative of the direction of steepest descent, so they can be useful in\\ndetermining how to navigate a function in order to find the minimum.\\nThanks to Nx’s automatic differentiation capabilities, implementing\\nstochastic gradient descent is a breeze. Start by opening up a Livebook and\\ninstalling Nx:\\nMix.install([\\n  { :nx ,  \"  ~> 0.5\" }\\n])\\nNow, you need to initialize some train data and test data. (For this example,\\nyou have access to the true data-generating function and parameters, so you\\ncan check your work afterward to make sure the gradient descent went\\nsmoothly.) The true_params of the data-generating function is a random', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 191}),\n",
       " Document(page_content='vector with length 32. This means each individual component of the vector\\nis a random value between 0 and 1—you should notice that simply guessing\\nor brute-forcing through all possible values for each parameter component\\nis impossible. The data-generating function is a linear combination of the\\ninputs and model parameters. If you remember from the previous chapter,\\nyou can compute this using Nx.dot to compute the dot-product of the true\\nparameters and input values. Copy the following code to generate 10,000\\nexamples of train and test data:\\nkey = Nx.Random.key(42)\\n{true_params, new_key} = Nx.Random.uniform(key,  \\nshape:  {32, 1})\\ntrue_function =  fn  params, x ->\\n  Nx.dot(x, params)\\nend \\n{train_x, new_key} = Nx.Random.uniform(new_key,  \\nshape:  {10000, 32})\\ntrain_y = true_function.(true_params, train_x)\\ntrain_data = Enum.zip(Nx.to_batched(train_x, 1), \\nNx.to_batched(train_y, 1))\\n{test_x, _new_key} = Nx.Random.uniform(new_key,  \\nshape:  {10000, 32})', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 192}),\n",
       " Document(page_content='test_y = true_function.(true_params, test_x)\\ntest_data = Enum.zip(Nx.to_batched(test_x, 1), \\nNx.to_batched(test_y, 1))\\nEach example is a tuple of {input, label}, where input is a uniform random\\nvector and label is obtained by evaluating the true data-generating function,\\ntrue_function.\\nNext, create a new module named SGD in a new cell. As you saw in the\\nprevious chapter, you’ll need to use defn to get access to Nx’s automatic\\ndifferentiation capabilities, so you’ll need to import Nx.Defn as well:\\ndefmodule  SGD  do \\n   import  Nx.Defn\\nend \\nGetting a Good Initialization\\nStochastic gradient descent typically starts from a random set of parameters.\\nRemember, the goal of optimization in machine learning is to find the best\\nparameterization for a particular function.\\nUnfortunately, when you don’t know anything about what the best\\nparameters should be, then you can’t intelligently choose a starting point.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 193}),\n",
       " Document(page_content='For certain learning algorithms, such as in deep learning, there is some\\nresearch to support various initializers converging better than others. But\\nfor now, you can initialize your parameters with a random uniform vector of\\nsize 32. To do so, add the following function to your SGD module:\\ndefn init_random_params(key)  do \\n  Nx.Random.uniform(key,  shape:  {32, 1})\\nend \\nNext, you need to implement your model, which is an attempt to model\\ntrue_function. Add the following function to your SGD module to represent\\nyour model:\\ndefn model(params, inputs)  do \\n  labels = Nx.dot(inputs, params)\\n  labels\\nend \\nIn this example, you know the original form of true_function, so you can use\\nthe same form to implement model. In a real machine learning problem, you\\nwouldn’t have access to the same information as you do here, so the form of\\nmodel would depend on your own intuition about the data. Here, you know\\nthe original function transforms the input with a dot product between it and\\nthe true parameters—you only need to find out what the true parameters\\nare.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 194}),\n",
       " Document(page_content='Defining an Objective\\nYou now need to implement your loss function. Remember, the stated\\nobjective is to minimize mean squared error on an evaluation set, so you\\ncan optimize for that directly by using mean squared error as your loss. You\\nalready saw how to implement mean squared error earlier in this chapter, so\\nyou can use the same implementation here:\\ndefn mean_squared_error(y_true, y_pred)  do \\n  y_true\\n  |> Nx.subtract(y_pred)\\n  |> Nx.pow(2)\\n  |> Nx.mean( axes:  [-1])\\nend \\ndefn loss(actual_label, predicted_label)  do \\n  loss_value = mean_squared_error(actual_label, \\npredicted_label)\\n  loss_value\\nend \\nYour loss function gives you a measure of how close your predicted\\ndistribution is to the actual distribution you’re trying to replicate.\\nIntuitively, you can think of gradient descent as if you’re on a mission to\\nfind the deepest point in a body of water.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 195}),\n",
       " Document(page_content='Your random initialization, init_random_params, drops you off at a random\\nposition in the body of water. You measure depth, loss, at your given\\nparameterization and figure out how rapidly the depth is changing in all\\ndirections. You then update your position in the direction of steepest\\ndescent. To figure out loss at your current parameterization, you need to\\ndefine an objective function. To do so, add the following code to your SGD\\nmodule:\\ndefn objective(params, actual_inputs, \\nactual_labels)  do \\n  predicted_labels = model(params, actual_inputs)\\n  loss(actual_labels, predicted_labels)\\nend \\nobjective/3 makes predictions on actual_inputs and returns loss/2 with respect\\nto predicted_labels and actual_labels. It’s the indication of depth with respect\\nto your current position in the boat example described previously.\\nAt this point, your module should look familiar. Notice that the code is\\nnearly identical to the original description of a machine learning problem\\ndescribed in  Minimizing Risk to Maximize Performance . It’s slightly more\\nspecific to this problem, but the key points are all the same. Almost every\\nmachine learning algorithm follows a pattern similar to this one.\\nStepping in the Right Direction', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 196}),\n",
       " Document(page_content='The final piece to your stochastic gradient descent algorithm is a step\\nfunction that moves your parameters in the right direction—you find the\\ndirection of steepest descent and move your boat in that direction.\\nRecall from Chapter 3,  Harness the Power of Math , that you can find the\\ndirection of steepest descent of a function with respect to some parameter\\nby computing its gradient. You can do that with ease using Nx’s automatic\\ndifferentiation capabilities. Add the following code to your SGD module to\\nimplement the step function:\\ndefn step(params, actual_inputs, actual_labels)  do \\n  {loss, params_grad} = value_and_grad(params,  fn  \\nparams ->\\n    objective(params, actual_inputs, \\nactual_labels)\\n   end )\\n  new_params = params - 1.0e-2 * params_grad\\n  {loss, new_params}\\nend \\nThis function makes use of a special Nx kernel function value_and_grad,\\nwhich returns both the value and gradient of a function with respect to some\\nparameter. Using value_and_grad is more desirable than just grad alone when\\nyou want access to the value of the function and its gradient at the same\\ntime. You take the gradient of objective/3 with respect to params, which', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 197}),\n",
       " Document(page_content='measures the gradient of the loss function with respect to your model’s\\nparameters. Next, you assign new_params to be the difference between\\nparams and a scaled version of params_grad.\\nIn a machine learning context, you’ll see the scaling factor shown here, 1.0e-\\n2, referred to as the learning rate. It essentially controls the size of jumps or\\nchanges between parameters for each example. The learning rate is\\ngenerally a value smaller than 1 because you only measure the gradient of\\nthe loss function with respect to a small number of points. You can’t be\\noverly confident in the gradient of a small sample. The fact that you are\\nonly using a sample of the whole dataset is what makes stochastic gradient\\ndescent actually stochastic. If you had access to the “true gradient” or the\\ngradient over the entire dataset, you wouldn’t necessarily need to scale\\nparams_grad, however, it’s often intractable to compute the true gradient.\\nPutting It All Together\\nFinally, you need something that glues all of the pieces of your SGD\\nimplementation together. You also need to add an evaluation function for\\nevaluating the final performance of your optimization routine. Add the\\nfollowing code to your module to implement both train and evaluate:\\ndef  evaluate(trained_params, test_data)  do \\n  test_data\\n  |> Enum.map( fn  {x, y} ->', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 198}),\n",
       " Document(page_content='   prediction = model(trained_params, x)\\n    loss(y, prediction)\\n   end )\\n  |> Enum.reduce(0, &Nx.add/2)\\nend \\ndef  train(data, iterations, key)  do \\n  {params, _key} = init_random_params(key)\\n  loss = Nx.tensor(0.0)\\n  {_, trained_params} =\\n    for i <- 1..iterations,  reduce:  {loss, params} \\ndo \\n      {loss, params} ->\\n        for {{x, y}, j} <- Enum.with_index(data),  \\nreduce:  {loss, params}  do \\n          {loss, params} ->\\n            {batch_loss, new_params} = \\nstep(params, x, y)\\n            avg_loss = Nx.add(Nx.mean(batch_loss), \\nloss) |> Nx.divide(j + 1)\\n            IO.write( \"  \\\\rEpoch:   #{ i }  , Loss:   #{ \\nNx.to_number(avg_loss) }  \" )\\n            {avg_loss, new_params}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 199}),\n",
       " Document(page_content='        end \\n     end \\n  trained_params\\nend \\nevaluate/2 takes your trained model parameters and test_data, computes the\\nloss for every example in test_data, and then sums the losses for every\\nexample. train/2 takes training data and an iterations parameter which\\ncontrols the number of iterations to step through data.\\n\\xa0\\nWhat Is an Epoch?\\nYou’ll often see one iteration through a dataset referred to as\\nan epoch. There’s not a good academic definition of an epoch,\\nand the abstraction is meaningless in the context of gradient\\ndescent. However, it’s so common in machine learning\\nliterature that you ought to know it. A single epoch in this\\ncontext means one iteration of the outer for-loop.\\n\\xa0', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 200}),\n",
       " Document(page_content='train/2 makes use of Elixir’s for-reduce syntax, which iterates over some\\nEnumerable, reducing some initial value with a reduction function. In this\\ncase, you’re reducing your initial parameters. The inner for-loop performs\\nthe real work of gradient-descent by calling step on each example in data\\nand returning the updated parameters. The IO.write/1 will log training\\nprogress after each iteration—so you can monitor what’s going on with\\nyour algorithm while you wait for it to finish.\\nEvaluating the Algorithm\\nYour SGD model is complete and ready to test. First, run the following code\\nto get a baseline of performance from a random set of parameters:\\nkey = Nx.Random.key(100)\\n{random_params, _} = SGD.init_random_params(key)\\nSGD.evaluate(random_params, test_data)\\nAfter running the code, you’ll see this:\\n#Nx.Tensor<\\n  f32[1]\\n  [4059.184814453125]\\n>\\nNext, run a single iteration of SGD and assign the output to a variable\\ntrained_params:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 201}),\n",
       " Document(page_content='key = Nx.Random.key(0)\\ntrained_params = SGD.train(train_data, 1, key)\\nAfter some time, you’ll see the returned parameters:\\n#Nx.Tensor<\\n  f32[32][1]\\n  [\\n    [0.32836729288101196],\\n    [0.09456641972064972],\\n    [0.3363123834133148],\\n    [0.4877675473690033],\\n    [0.6099048852920532],\\n    [0.7853176593780518],\\n    [0.40266865491867065],\\n    [0.22026798129081726],\\n    [0.1402876228094101],\\n    [0.13440364599227905],\\n    [0.7953506708145142],\\n    [0.3691302239894867],\\n    [0.47869911789894104],\\n    [0.14079436659812927],\\n    [0.6659020185470581],\\n    ...\\n  ]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 202}),\n",
       " Document(page_content='>\\nNow, run evaluate/2 to see how well your model matches the true data-\\ngenerating function:\\nSGD.evaluate(trained_params, test_data)\\nYou’ll see something like this:\\n#Nx.Tensor<\\n  f32[1]\\n  [168.16705322265625]\\n>\\nYour loss over the entire test set is only around 168.\\nNotice how a trained version does significantly better than an untrained\\nversion. Gradient descent was able to optimize your parameters such that\\nthey’re significantly closer to the true parameters.\\nMaking It Fail\\nThis example is a bit impractical because, in the real world, you won’t have\\naccess to information about the true data-generating function. You can see\\nhow challenging it might be without this information by slightly changing', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 203}),\n",
       " Document(page_content='the form of true_function without changing the form of your model. Adjust\\ntrue_function and generate new train and test data with the following code:\\nkey = Nx.Random.key(42)\\n{true_params, new_key} = Nx.Random.uniform(key,  \\nshape:  {32, 1})\\ntrue_function =  fn  params, x ->\\n  Nx.dot(x, params) |> Nx.cos()\\nend \\n{train_x, new_key} = Nx.Random.uniform(new_key,  \\nshape:  {10000, 32})\\ntrain_y = true_function.(true_params, train_x)\\ntrain_data = Enum.zip(Nx.to_batched(train_x, 1), \\nNx.to_batched(train_y, 1))\\n{test_x, _new_key} = Nx.Random.uniform(new_key,  \\nshape:  {10000, 32})\\ntest_y = true_function.(true_params, test_x)\\ntest_data = Enum.zip(Nx.to_batched(test_x, 1), \\nNx.to_batched(test_y, 1))\\ntrue_function now combines parameters with x using a dot-product, and then\\nit applies the cosine function to the output. Rerun your training and', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 204}),\n",
       " Document(page_content='evaluation to see what happens:\\nkey = Nx.Random.key(0)\\ntrained_params = SGD.train(train_data, 10, key)\\nSGD.evaluate(trained_params, test_data)\\nYou’ll see something like this:\\n#Nx.Tensor<\\n  f32[1]\\n  [2591.621337890625]\\n>\\nOverall you’ll notice that your performance on this problem is much worse\\nthan the previous run. By adding a nonlinearity to the true function, your\\nlinear model couldn’t completely recover the original parameters. Adding\\nNx.cos is a relatively minor change—real-world data-generating processes\\nare likely much more complex. That’s not to say your model doesn’t\\nperform decently well. A total error of around 2,000 for 10,000 examples is\\npretty good and might even be good enough to use in production if this\\nwere a real-world example. As you’ll see in the  Learning Linearly , linear\\nmodels work well, even though most processes aren’t linear.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 205}),\n",
       " Document(page_content='Peering into the Black Box\\nBefore moving on in your machine learning journey, it’s important to know\\nabout one more type of optimization problem you’ll encounter in machine\\nlearning: hyperparameter search.\\nHyperparameters are the details about an algorithm that aren’t directly\\nlearnable but affect the structure and outcome of the model training process.\\nFor example, the learning rate 1.0e-2 used in the previous section is a\\nhyperparameter.\\nHyperparameter search is concerned with finding hyperparameters that lead\\nto the optimal performance of a model on an evaluation set.\\nHyperparameters can have a big impact on the evaluation of a model, so\\nfinding better hyperparameters can drastically improve model performance.\\nUnfortunately, it’s difficult to describe hyperparameter search in terms of a\\ndifferentiable objective function.\\nThe goal in hyperparameter search is typically to maximize validation\\nmetrics. But this cannot be readily quantified in a simple function similar to\\nthe objective function you wrote in the previous section. Because of the\\nlack of a well-defined, differentiable objective function, hyperparameter\\nsearch often relies on black-box optimization and exhaustive search. Black-\\nbox optimization optimizes solutions based on objective observations', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 206}),\n",
       " Document(page_content='without needing access to an explicit objective function. An exhaustive\\nsearch is a brute-force search.\\nThere are many approaches to black-box optimization. Additionally, you’ll\\noften see different forms of optimization used in contexts outside of\\nhyperparameter search. In fact, some of these approaches, such as\\nevolutionary algorithms, can pose as viable alternatives to machine\\nlearning. This section will briefly introduce you to two approaches used in\\nthe context of hyperparameter search: evolutionary algorithms and grid\\nsearch.\\nEvolutionary Algorithms\\nEvolutionary algorithms are a class of optimization algorithms based on the\\nprinciples of artificial selection. In an evolutionary algorithm, you generate\\na population of solutions to a problem (for example, a set of\\nhyperparameters), evaluate each solution in the population, and then\\ncombine the best solutions to form better solutions. Evolutionary algorithms\\ncan even be used in tandem with neural networks in a process called\\nneuroevolution, in which the parameters and structure of a neural network\\nare evolved through several generations to yield better networks.\\nEvolutionary algorithms are also popular outside of machine learning and\\nhave been used in the intelligent design of radio antennas and scenario\\ngeneration for game design [SFF19].\\n[7]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 207}),\n",
       " Document(page_content='Evolutionary algorithms are easily parallelizable and benefit from the\\nparallelization and acceleration Nx offers with its JIT compilation to\\nCPU/GPU. Unfortunately, a detailed description of genetic algorithms is\\noutside of the scope of this book. If you’d like to learn more, check out\\nGenetic Algorithms in Elixir [Mor21], as well as the MEOW library.\\nGrid Search\\nGrid search differs from black-box optimization in that it’s actually an\\nexhaustive search over a discrete grid of parameters. Grid search is the most\\ncommon approach to hyperparameter optimization used in machine\\nlearning. In a grid search, you define which parameters you want to\\noptimize for and what values you want to test for each parameter. For\\nexample, you might specify 1.0e-4, 1.0e-3, and 1.0e-2 as values to try for the\\nlearning rate. Grid search will try combinations of your hyperparameters\\nand return the combination that yields the best model.\\n[8]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 208}),\n",
       " Document(page_content='[7]\\n[8]\\nWrapping Up\\nIn this chapter, you framed machine learning as an optimization problem to\\nunderstand how machine learning and optimization are related.\\nAdditionally, you learned about regularization and how the key difference\\nbetween optimization and machine learning is machine learning’s emphasis\\non generalization. You applied optimization to a machine learning problem\\nby implementing your own version of stochastic gradient descent. Finally,\\nyou learned a bit about black-box optimization and hyperparameter\\noptimization in the context of machine learning.\\nWith the foundations of machine learning and Nx covered, you’re ready to\\nstart solving real-world machine learning problems with Elixir and the Nx\\necosystem. In the next chapter, you’ll jump straight into traditional machine\\nlearning algorithms, such as linear and logistic regression, nearest neighbors\\nlearning, and decision trees.\\nFOOTNOTES\\nhttps://www.jpl.nasa.gov/nmp/st5/TECHNOLOGY/antenna.html\\nhttps://github.com/jonathanklosko/meow\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 209}),\n",
       " Document(page_content='Chapter 5\\nTraditional Machine Learning\\n\\xa0\\nIn the previous chapters, you’ve become more grounded in the foundations\\nof machine learning. At this point, you should feel more comfortable\\nworking with Nx, and it’s okay if you don’t grasp everything. Part of the\\nbeauty of the tooling in the Nx ecosystem is that it greatly reduces the\\ncomplexity of common machine learning tasks. You don’t need to be a math\\nexpert to train a machine learning model in Elixir. You need only to\\nunderstand a bit of Elixir, and a bit of Nx.\\nIn Chapter 4,  Optimize Everything , you wrote a linear regression model\\nfrom scratch using Nx. While following along, you might have noticed the\\ntask was a bit verbose to write from scratch. Nx offers a lot of flexibility,\\nbut you don’t want to have to write every algorithm from scratch.\\nThankfully, Elixir has a library for performing common machine learning\\ntasks in a few lines of code. Scholar is a set of machine learning tools\\nbuilt on top of Elixir and Nx. In this chapter, you’ll implement some\\ntraditional machine learning algorithms using Scholar, and you’ll start to\\nunlock the potential of the Elixir Nx ecosystem.\\n[9]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 210}),\n",
       " Document(page_content='Learning Linearly\\nRecent advances in machine learning have largely been attributed to deep\\nlearning. The popularity of deep learning—thanks to impressive feats in\\nnatural language processing, computer vision, and generative modeling—\\nhas made it almost synonymous with machine learning. However, the field\\nof machine learning is vast. Deep learning refers to a subset of machine\\nlearning that uses neural networks. You’ll learn more about deep learning in\\nChapter 6,  Go Deep with Axon . But first, you’ll focus on shallow machine\\nlearning algorithms.\\nShallow machine learning refers to non-deep-learning-based algorithms.\\nBut the term is a bit of a misnomer. In a deep-learning-centric world, it’s\\neasy to classify algorithms into one of two classes: deep learning and\\neverything else. In reality, the “everything else” covers a vast amount of\\nalgorithms. Many of these algorithms find significant use in the real world\\ntoday.\\nWhen tackling a problem using machine learning, it’s easy to immediately\\nreach for a complex solution such as deep learning. However, many times\\nyou can do just as well with a much simpler and more interpretable machine\\nlearning algorithm. You can get a lot of value out of a simple linear\\nregression model.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 211}),\n",
       " Document(page_content='Perhaps the simplest class of traditional machine learning algorithms is\\nlinear models. They assume linearity in the underlying relationship between\\ninputs and outputs. To think about it simply, linear models assume that input\\ndata can be modeled with a line. While this might seem like an overly\\nsimplistic assumption, it turns out that you can create pretty accurate\\nmodels in this way. Remember, there’s no such thing as a perfect model. A\\nlot of times all we care about is having a model that’s good enough for the\\ntask at hand. Reality is almost never linear, but linear models are still\\npowerful at modeling the real world.\\nLinear Regression with Scholar\\nYou’re already familiar with one type of linear model—linear regression—\\nfrom  Descending Gradients . Linear regression is an approach for modeling\\nthe relationship between some scalar target variable and one or more input\\nvariables. If you dust off your primary school mathematics lessons, you\\nmight remember the infamous equation for a line:\\ny = mx + b\\nThis equation represents a linear relationship between the target variable y\\nand the input variable x. m and b represent the slope and intercept of the\\nline, respectively. Given an m and a b, you can predict any y from any x. For\\nexample, if you know m is 3.0 and b is 2.0, you can create a function such\\nas the following:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 212}),\n",
       " Document(page_content='def  model(x)  do \\n  3.0 * x + 2.0\\nend \\nYou can then use your model to get the correct value of y at different values\\nof x:\\nIO.inspect model(2)  # prints 8 \\nIO.inspect model(3)  # prints 11 \\nIO.inspect model(4)  # prints 14 \\nIn the real world, you often don’t have access to the parameters m and b,\\nwhich drive the relationship between your target and input variables.\\nInstead, you need to infer or learn these parameters using machine learning.\\nFor example, consider this scatterplot from  Learning with Optimization :', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 213}),\n",
       " Document(page_content='You’ll notice the points on the plot loosely follow a line. Your goal with\\nlinear regression is to draw a line on that graph, such that it minimizes the\\ntotal distance between the line and every point in your dataset. The line you\\nend up drawing is often called the best-fit line and can be used as a\\npredictive model for the relationship between inputs and outputs. In  ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 214}),\n",
       " Document(page_content='Descending Gradients , you took the long way to find the best-fit line by\\nimplementing stochastic gradient descent by hand. Now, you’ll use Scholar\\nto do the same thing.\\nStart by installing the following dependencies in a new Livebook:\\nMix.install([\\n  { :scholar ,  \"  ~> 0.2\" },\\n  { :nx ,  \"  ~> 0.5\" },\\n  { :exla ,  \"  ~> 0.5\" },\\n  { :vega_lite ,  \"  ~> 0.1.6\" },\\n  { :kino_vega_lite ,  \"  ~> 0.1.6\" },\\n  { :scidata ,  \"  ~> 0.1\" }\\n])\\nYou’ll use Scholar to access a number of traditional machine learning\\nalgorithms and tools: Nx for manipulating tensors, EXLA for JIT-\\ncompilation and acceleration, and VegaLite/KinoVegaLite for creating\\nvisualizations of your data. You’ll also need SciData for downloading\\ndatasets later on.\\nNow, set the default backend to EXLA and the default defn compiler to\\nEXLA by running the following code in a new cell:\\nNx.default_backend(EXLA.Backend)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 215}),\n",
       " Document(page_content='Nx.Defn.default_options( compiler:  EXLA)\\nNext, you’ll need to create some data. Run the following code to generate\\ntraining data:\\nm =  :rand .uniform() * 10\\nb =  :random .uniform() * 10\\nkey = Nx.Random.key(42)\\nsize = 100\\n{x, new_key} = Nx.Random.normal(key, 0.0, 1.0,  \\nshape:  {size, 1})\\n{noise_x, new_key} = Nx.Random.normal(new_key, \\n0.0, 1.0,  shape:  {size, 1})\\ny =\\n  m\\n  |> Nx.multiply(Nx.add(x, noise_x))\\n  |> Nx.add(b)\\nIn this snippet, you start by declaring the target parameters m and b as\\nrandom scalars. You then create a new random PRNG key using Nx’s\\nrandom module. Remember, Nx’s random number generation modules are\\nstateless, so you always need to pass a key for generating random numbers.\\nNext, you create two random normal tensors: x and noise_x. x is a random', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 216}),\n",
       " Document(page_content='distribution of input variables. noise_x is gaussian noise added to x. This is\\nnecessary to simulate what data looks like in the real world— imperfect.\\nNow, you can run the following code to create a scatterplot of your data:\\nalias VegaLite,  as:  Vl\\nVl.new( title:   \"  Scatterplot\" ,  width:  720,  height:  \\n480)\\n|> Vl.data_from_values(%{\\n   x:  Nx.to_flat_list(x),\\n   y:  Nx.to_flat_list(y)\\n})\\n|> Vl.mark( :point )\\n|> Vl.encode_field( :x ,  \"  x\" ,  type:   :quantitative )\\n|> Vl.encode_field( :y ,  \"  y\" ,  type:   :quantitative )\\nYou’ll see the plot as shown.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 217}),\n",
       " Document(page_content='\\xa0\\nNotice the data generally follows a line. But the line isn’t perfect thanks to\\nyour gaussian noise.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 218}),\n",
       " Document(page_content='Next, you want to fit a linear regression model on input variables x and\\noutput variables y. Using Scholar, you can do this with a single line:\\nmodel = Scholar.Linear.LinearRegression.fit(x, y)\\nAfter running this code, you’ll notice you get the following output:\\n%Scholar.Linear.LinearRegression{\\n  coefficients: #Nx.Tensor<\\n    f32[1][1]\\n    [\\n      [3.4556338787078857]\\n    ]\\n  >,\\n  intercept: #Nx.Tensor<\\n    f32[1]\\n    [9.695785522460938]\\n  >\\n}\\nScholar.Linear.LinearRegression.ﬁt/3 returns a %LinearRegression{} struct with\\nattributes coeﬃcients and intercept. These variables represent the values m\\nand b.\\nIf you inspect m and b, you’ll notice they’re close to the predicted\\ncoeﬃcients and intercept. Note that your m and b will be different than what’s', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 219}),\n",
       " Document(page_content='presented here due to randomization.\\nUnder the hood, Scholar’s Scholar.Linear.LinearRegression.ﬁt/3 finds a best-fit\\nusing ordinary least-squares regression. Fortunately, you don’t need to\\nknow what that means because Scholar abstracts away the complexities into\\nan easy-to-use API. You can now use the returned model to make\\npredictions on new data. For example, after running the following:\\nScholar.Linear.LinearRegression.predict(model, \\nNx.iota({3, 1}))\\nYou’ll see this:\\n#Nx.Tensor<\\n  f32[3][1]\\n  [\\n    [9.695785522460938],\\n    [13.151419639587402],\\n    [16.607053756713867]\\n  ]\\n>\\nYou can further validate your model by visualizing the relationship it\\npredicts overlayed with your original scatterplot. To do this, start by\\nrunning the following code to generate predictions over the same interval as\\nyour training data:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 220}),\n",
       " Document(page_content='pred_xs = Nx.linspace(-3.0, 3.0,  n:  100) |> \\nNx.new_axis(-1)\\npred_ys = \\nScholar.Linear.LinearRegression.predict(model, \\npred_xs)\\nThe first line of code creates a vector with 100 entries, where values\\nlinearly increase from -3.0 up to 3.0. In the next line of code, you make\\npredictions over your entire interval using\\nScholar.Linear.LinearRegression.predict/2.\\nYou can now plot the predictions overlayed with your original data using\\nthe following code:\\ntitle =  \"  Scatterplot Distribution and Fit Curve\" \\nVl.new( title:  title,  width:  720,  height:  480)\\n|> Vl.data_from_values(%{\\n   x:  Nx.to_flat_list(x),\\n   y:  Nx.to_flat_list(y),\\n   pred_x:  Nx.to_flat_list(pred_xs),\\n   pred_y:  Nx.to_flat_list(pred_ys)\\n})\\n|> Vl.layers([\\n  Vl.new()\\n  |> Vl.mark( :point )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 221}),\n",
       " Document(page_content=' |> Vl.encode_field( :x ,  \"  x\" ,  type:   :quantitative )\\n  |> Vl.encode_field( :y ,  \"  y\" ,  type:   :quantitative \\n),\\n  Vl.new()\\n  |> Vl.mark( :line )\\n  |> Vl.encode_field( :x ,  \"  pred_x\" ,  type:   \\n:quantitative )\\n  |> Vl.encode_field( :y ,  \"  pred_y\" ,  type:   \\n:quantitative )\\n])\\nAfter running this cell, you’ll see the following output:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 222}),\n",
       " Document(page_content='You’ll probably notice the line doesn’t even intersect any of the points in\\nthe original training data. Remember, this line is meant to be a best-fit line\\nthat represents a generalization of the patterns seen in the training data. It’s\\nmore or less an average of the patterns in the training data.\\nIn this example, you performed a linear regression of a single variable. But\\nyou can extend linear regression to as many features as you’d like. For\\nexample, if you were predicting how many points a team is going to score\\nin an NBA game, you can use a number of different features to predict this:\\naverage three-point shots made per game, average points per game, whether\\nor not Tim Donaghy is refereeing, and so on. As long as the relationship', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 223}),\n",
       " Document(page_content='between input variables and target variables can be reasonably represented\\nwith a line, you can use linear regression to solve it.\\nLogistic Regression with Scholar\\nA close relative of linear regression that’s often used for classification\\nproblems is logistic regression. Logistic regression is almost identical to\\nlinear regression. However, after applying the linear transformation on the\\ninput variables, you also apply a logistic function, which squeezes the\\noutput between 0 and 1. Often, the output represents a probability for a\\nbinary classification problem. But it can also be extended to work for multi-\\nclass classification problems. For example, imagine you’re a sommelier\\ntrying to develop a model to predict the type of wine from a chemical\\nanalysis of each wine. You have a dataset of measurements, along with a\\nlabel representing one of three types of wine. This problem is a perfect\\ncandidate for applying logistic regression.\\nStart by downloading the wine dataset using SciData:\\n{inputs, targets} = Scidata.Wine.download()\\nNext, split the dataset into training and test sets, so you can validate the\\nperformance of your model:\\n{train, test} =\\n  inputs', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 224}),\n",
       " Document(page_content=' |> Enum.zip(targets)\\n  |> Enum.shuffle()\\n  |> Enum.split(floor(length(inputs) * 0.8))\\n{train_inputs, train_targets} = Enum.unzip(train)\\ntrain_inputs = Nx.tensor(train_inputs)\\ntrain_targets = Nx.tensor(train_targets)\\n{test_inputs, test_targets} = Enum.unzip(test)\\ntest_inputs = Nx.tensor(test_inputs)\\ntest_targets = Nx.tensor(test_targets)\\nThe first part of this code shuffles the original input dataset, such that the\\nclasses are uniformly distributed between the training and test set. It then\\nsplits the dataset into an 80/20 training and testing split. Finally, it\\ntransforms the training and test sets into tensors.\\nNow, you’ll want to do a little bit of preprocessing before continuing. Most\\nof the time, you want your input features normalized on some scale. This\\noften makes it easier for models to learn patterns from input data. For\\nexample, it’s common to squeeze input features between 0 and 1. You can\\nuse Scholar’s preprocessing methods to do this:\\ntrain_inputs = \\nScholar.Preprocessing.min_max_scale(train_inputs)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 225}),\n",
       " Document(page_content='test_inputs = \\nScholar.Preprocessing.min_max_scale(test_inputs)\\nThis code scales both train and test inputs between 0 and 1 by normalizing\\naccording to the min and max of the input data.\\nNext, you can train a logistic regression model using\\nScholar.Linear.LogisticRegression.ﬁt/3:\\nmodel = Scholar.Linear.LogisticRegression.fit(\\n  train_inputs,\\n  train_targets,\\n   num_classes:  3\\n)\\nNotice that you must specify num_classes: 3 because the original dataset has\\nthree classes. This code will treat the original problem as a multi-class\\nclassification problem. After some time running, you’ll see the following\\noutput:\\n%Scholar.Linear.LogisticRegression{\\n   coefficients:   #Nx.Tensor< \\n    f32[13][3]\\n    [', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 226}),\n",
       " Document(page_content='     [57.30550765991211, 40.04463577270508, \\n21.67230796813965],\\n      [9.130376815795898, 6.3609209060668945, \\n5.055447101593018],\\n      [9.905561447143555, 7.361076831817627, \\n3.8778562545776367],\\n      [70.83293914794922, 67.13773345947266, \\n35.777008056640625],\\n      [457.4987487792969, 298.3395080566406, \\n170.00132751464844],\\n      [11.34200668334961, 6.753108978271484, \\n2.0085954666137695],\\n      [11.833603858947754, 6.091976165771484, \\n0.4948000907897949],\\n      [1.158339500427246, 1.3611177206039429, \\n0.7777756452560425],\\n      [7.858388900756836, 4.755614280700684, \\n1.6751126050949097],\\n      [21.37252426147461, 9.511534690856934, \\n12.442154884338379],\\n      [4.333345890045166, 3.347238063812256, \\n1.008358359336853],\\n      [13.530704498291016, 8.850102424621582, \\n2.6361520290374756],', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 227}),\n",
       " Document(page_content='     [4223.81689453125, 1395.7369384765625, \\n934.896240234375]\\n    ]\\n  >,\\n   bias:   #Nx.Tensor< \\n    f32\\n    0.0\\n  >,\\n   mode:   :multinomial \\n}\\nSimilar to your linear regression model, Scholar produces a struct specific\\nto logistic regression, which can be used for making predictions later on.\\nYou’ll notice the shape of the coefficients tensor is {13, 3}. That maps to\\nthirteen features per class. In a multi-class logistic regression model, you\\nactually train a model to predict multiple binary probabilities. For each\\nclass, you predict a probability between 0 and 1 of whether or not the input\\nis a member of that class, and then you normalize across all classes.\\nYou can now use this model to make predictions on your test set:\\ntest_preds = \\nScholar.Linear.LogisticRegression.predict(model, \\ntest_inputs)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 228}),\n",
       " Document(page_content='In addition to implementing a number of machine learning algorithms,\\nScholar also implements routines for evaluating models. For example, you\\ncan compute the accuracy of your model:\\nScholar.Metrics.Classification.accuracy(test_target\\ntest_preds)\\nAfter running that cell, you’ll see the following output:\\n#Nx.Tensor<\\n  f32\\n  0.75\\n>\\nAnother common metric for classification problems is a confusion matrix. A\\nconfusion matrix is a table that lays out the performance of a model with\\nrespect to each class in a classification problem. The table is two-\\ndimensional, where the columns represent the predicted class, and the rows\\nrepresent the actual class. You can think of each cell in the table being\\nrepresented by a tuple, such as {0, 0}, {0, 1}, and so on. The value at {0, 0}\\nrepresents the number of times the model correctly predicted an input\\nbelonged to class 0 when it actually belonged to class 0. The value at {0, 1}\\nrepresents the number of times the model predicted the input belonged to\\nclass 0 when it actually belonged to class 1. This pattern repeats for every', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 229}),\n",
       " Document(page_content='class in your dataset. You can compute the confusion matrix for this\\nproblem using the following:\\nScholar.Metrics.Classification.confusion_matrix(\\n  test_targets,\\n  test_preds,\\n   num_classes:  3\\n)\\nAfter running this code, you’ll see this:\\n#Nx.Tensor<\\n  s64[3][3]\\n  [\\n    [9, 0, 0],\\n    [19, 0, 0],\\n    [8, 0, 0]\\n  ]\\n>\\nYou can also turn this confusion matrix into a plot by running the following\\ncode:\\nVl.new( title:   \"  Confusion Matrix\" ,  width:  1440,  \\nheight:  1080)\\n|> Vl.data_from_values(%{', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 230}),\n",
       " Document(page_content='  predicted:  Nx.to_flat_list(test_preds),\\n   actual:  Nx.to_flat_list(test_targets),\\n})\\n|> Vl.mark( :rect )\\n|> Vl.encode_field( :x ,  \"  predicted\" )\\n|> Vl.encode_field( :y ,  \"  actual\" )\\n|> Vl.encode( :color ,  aggregate:   :count )\\nAfter running that code, you’ll see this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 231}),\n",
       " Document(page_content='Confusion matrices are good visualizations of classification performance\\nfor your models.\\nDealing with Nonlinear Data', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 232}),\n",
       " Document(page_content='A lot of real-world data is not linear. That means linear models often don’t\\nhave sufficient capacity or modeling power to represent real-world\\nrelationships. There are a few ways around this problem.\\nFirst, you can simply deal with it. In reality, no model is perfect. If you have\\na linear model such as a linear regression model that does a decent job at\\npredicting values from your data, then there’s no reason to mess with it. You\\nknow what they say, “If it’s not broken, don’t fix it.” Linear models are\\nsimple, fast, and interpretable, which makes them excellent candidates for\\nbusiness applications. A lot of times, you can deploy a linear regression\\nmodel that’s much simpler and faster than an alternative model and get 90%\\nof the value of using a more expensive or complex model.\\nAnother way to deal with nonlinear data is to perform transformations on\\nyour data until a linear relationship arises. This is, in a sense, feature\\nengineering.\\nFeature engineering is the process of manipulating features by hand to\\nprovide better information for your model. If you’re a sports fan, you might\\nbe familiar with advanced analytics. For example, in basketball, it’s\\ncommon to look at a player’s true shooting percentage over only their field\\ngoal percentage to determine their value as a shooter. Over time, teams have\\nrealized advanced analytics, which often combine many surface-level\\nmetrics, are much more indicative of a player’s performance. Advanced', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 233}),\n",
       " Document(page_content='analytics are features that have been engineered from expert knowledge of\\nthe game of basketball.\\nWith linear models, such as linear and logistic regression models, if you\\nknow your input data is more accurately represented by a power model or\\nlogarithm model, then you can transform the inputs to reflect that. Look at\\nthe following graph:\\n\\xa0\\n\\xa0\\nNotice that data, which previously couldn’t be represented linearly, now can\\nbe with a simple transformation applied. For example, rather than looking at\\ny vs. x in the first graph, you look at log(y) vs. x. These types of', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 234}),\n",
       " Document(page_content='transformations are a bit of an oversimplification of a famous technique in\\nmachine learning known as the kernel trick.\\nThe kernel trick is usually mentioned in reference to support vector\\nmachines, which are a type of linear model outside the scope of this book.\\nWith the kernel trick, you apply high-dimensional transformations to the\\noriginal data, such that it becomes linearly separable. The kernel trick\\nallows you to model nonlinear data with linear models.\\nAs it turns out, not all data can be transformed in such a way that it’s\\nlinearly separable. Linear separability is literally the ability for a dataset to\\nbe partitioned or separated by a line. In some cases, linear models simply\\nwon’t cut it, and you need to find other ways to solve a problem. In the rest\\nof this chapter, you’ll briefly look at a few of these approaches before\\nmoving away from traditional machine learning methods and into deep\\nlearning.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 235}),\n",
       " Document(page_content='Learning from Your Surroundings\\nLinear models are one of many traditional machine learning models\\nimplemented in Scholar. Another common model you’ll come across in the\\nwild is the K-Nearest Neighbors (KNN) model. The idea behind KNN\\nmodels is relatively simple: you are likely very similar to the things nearest\\nto you. Intuitively this makes sense. If you were to pluck a random person\\noff the streets of Philadelphia and ask them their favorite football team,\\nthey’d likely say the Eagles. That’s because the Eagles are the hometown\\nteam, and they’re close in proximity to where the Eagles play. As you drift\\naway from Philadelphia, the distribution of favorite NFL teams changes.\\nKNN works simply by classifying new points as belonging to the most\\ncommon class of its K-closest neighbors. You can use KNN for both\\nregression and classification. In both cases, the K-Nearest Neighbors\\ndetermine the class or final value of new data points. KNN is simple and\\npowerful. You can quickly train and evaluate a KNN model in Scholar and\\ncompare it against your Logistic Regression model on the wine dataset.\\nRun the following code to fit a new KNN classifier:\\nmodel = Scholar.Neighbors.KNearestNeighbors.fit(\\n  train_inputs, train_targets,  num_classes:  3\\n)\\nAfter running the code, you’ll see the following output:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 236}),\n",
       " Document(page_content='%Scholar.Neighbors.KNearestNeighbors{\\n   data:   #Nx.Tensor< \\n    f32[142][13]\\n    [\\n      [0.00769107136875391, 9.762659901753068e-4, \\n...]\\n      ...\\n    ]\\n  >,\\n   labels:   #Nx.Tensor< \\n    s64[142]\\n    [0, 0, 2, 2, 2, 2, 1, ...]\\n  >,\\n   default_num_neighbors:  5,\\n   weights:   :uniform ,\\n   num_classes:  3,\\n   p:  nil,\\n   task:   :classification ,\\n   metric:  { :minkowski , 2}\\n}\\nNote that running the KNN ﬁt method doesn’t actually fit anything. KNN\\ndoesn’t require any training because you only sample from existing data', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 237}),\n",
       " Document(page_content='during inference time. The model simply keeps track of points in the\\ntraining set to make predictions about the test set.\\nLet’s start making predictions on test data and evaluating your model. Start\\nby predicting classes for the test inputs:\\ntest_preds = \\nScholar.Neighbors.KNearestNeighbors.predict(model, \\ntest_inputs)\\nYou can now predict the accuracy of your model:\\nScholar.Metrics.accuracy(test_targets, test_preds)\\nAfter running the code, you’ll see the following output:\\n#Nx.Tensor<\\n  f32\\n  0.75\\n>\\nYou can also compute a confusion matrix, like so:\\nScholar.Metrics.confusion_matrix(test_targets, \\ntest_preds,  num_classes:  3)\\nRun the code, and you’ll see the following:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 238}),\n",
       " Document(page_content='#Nx.Tensor<\\n  s64[3][3]\\n  [\\n    [9, 0, 0],\\n    [2, 11, 6],\\n    [0, 1, 7]\\n  ]\\n>\\nNote that your nearest neighbor model was pretty much identical in\\nperformance to your logistic regression model. You could continue fiddling\\nwith hyperparameters or choose to deploy one model over another. You’ll\\nfind that Scholar makes it easy to prototype a broad range of classifiers and\\nthen evaluate each to determine which one is best for deployment in\\nproduction.\\nYou can see that Scholar is an excellent tool for creating supervised\\nlearning methods, but it’s not limited to just that. Scholar also provides a\\nnumber of tools for performing unsupervised learning and data analysis.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 239}),\n",
       " Document(page_content='Using Clustering\\nIn addition to supervised methods, Scholar offers a number of tools for\\nunsupervised learning and analysis. Recall from Chapter 1,  Make Machines\\nThat Learn , that unsupervised learning is a type of machine learning where\\nyou learn only from inputs without access to any target information.\\nPerhaps the most common type of unsupervised learning is clustering.\\nClustering is the process of identifying clusters or groups of similar data\\npoints in a dataset. There are many approaches to clustering, such as K-\\nMeans clustering, hierarchical clustering, spectral clustering, and more.\\nThe most common type of clustering you’ll likely see in practice is K-\\nMeans clustering, which randomly assigns K centroids to random points in\\nthe dataset and iteratively updates each centroid until an optimal\\nconfiguration is reached.\\nClustering can be useful for identifying general patterns from groups in\\nyour dataset. For example, if you have a dataset derived from shopper\\nbehavior on your online store, you can cluster shopper behavior and get an\\nidea of the general types of shoppers that visit your store. One cluster might\\nrepresent heavy spenders, while another might represent noncommittal\\nbrowsers.\\nOne of the central challenges in clustering is determining an appropriate\\nnumber of clusters to use. Typically, you need to visualize your dataset and', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 240}),\n",
       " Document(page_content='do some sort of high-level analysis to determine an appropriate number of\\nclusters. Real-world data doesn’t always cluster nicely. Real-world data is\\nmessy.\\nYou can also use K-Means as an unsupervised approach to classification.\\nFor example, you can apply K-Means clustering to your wine problem by\\nrunning the following code:\\nmodel = Scholar.Cluster.KMeans.fit(train_inputs,  \\nnum_clusters:  3)\\nNote that, here, you only need to pass the train inputs because K-Means is\\nan unsupervised approach. It relies only on input data without access to\\ntargets. After running the code, you’ll see an output like the following:\\n%Scholar.Cluster.KMeans{\\n  num_iterations: #Nx.Tensor<\\n    s64\\n    4\\n  >,\\n  clusters: #Nx.Tensor<\\n    f32[3][13]\\n    [\\n      [0.008120134472846985, 0.001020378083921969, \\n...]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 241}),\n",
       " Document(page_content='     ...\\n    ]\\n  >,\\n  inertia: #Nx.Tensor<\\n    f32\\n    0.6914953589439392\\n  >,\\n  labels: #Nx.Tensor<\\n    s64[142]\\n    [2, 2, 2, 1, 1, 1, 1, 0, 2, 2, 1, 1, 2, 2, \\n...]\\n  >\\n}\\nThis struct contains metadata about the algorithm, as well as the cluster\\ncentroids and cluster assignments for each member in the training dataset.\\nK-Means finds centroids for each cluster and assigns data points to their\\nclosest centroid. You can visualize the centroids against features in your\\ndataset. The wine dataset has thirteen dimensions, so we’ll need to reduce\\nthat down to two or three dimensions for easier visualization. You can run\\nthe following code to produce a visual of the centroids relative to each\\npoint:\\nwine_features = %{', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 242}),\n",
       " Document(page_content='  \"  feature_1\"  => train_inputs[[.., 1]] |> \\nNx.to_flat_list(),\\n   \"  feature_2\"  => train_inputs[[.., 2]] |> \\nNx.to_flat_list(),\\n   \"  class\"  => train_targets |> Nx.to_flat_list()\\n}\\ncoords = [\\n   cluster_feature_1:  model.clusters[[.., 1]] |> \\nNx.to_flat_list(),\\n   cluster_feature_2:  model.clusters[[.., 2]] |> \\nNx.to_flat_list()\\n]\\ntitle =\\n   \"  Scatterplot of data samples pojected on plane \\nwine\" \\n    <>  \"   feature 1 x wine feature 2\" \\nVl.new(\\n   width:  1440,\\n   height:  1080,\\n   title:  [', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 243}),\n",
       " Document(page_content='    text:  title,\\n     offset:  25\\n  ]\\n)\\n|> Vl.layers([\\n  Vl.new()\\n  |> Vl.data_from_values(wine_features)\\n  |> Vl.mark( :circle )\\n  |> Vl.encode_field( :x ,  \"  feature_1\" ,  type:   \\n:quantitative )\\n  |> Vl.encode_field( :y ,  \"  feature_2\" ,  type:   \\n:quantitative )\\n  |> Vl.encode_field( :color ,  \"  class\" ),\\n  Vl.new()\\n  |> Vl.data_from_values(coords)\\n  |> Vl.mark( :circle ,  color:   :green ,  size:  100)\\n  |> Vl.encode_field( :x ,  \"  cluster_feature_1\" ,  \\ntype:   :quantitative )\\n  |> Vl.encode_field( :y ,  \"  cluster_feature_2\" ,  \\ntype:   :quantitative )\\n])\\nThis will create a scatterplot of wine features 1 and 2 with a green dot\\nrepresenting where the centroid falls with respect to both of those features.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 244}),\n",
       " Document(page_content='After running this code, you’ll see the following plot:\\nNote that this isn’t very pretty, and the clusters aren’t easily discernible.\\nThat’s because your clusters were made against thirteen features—so while\\nthe data isn’t clearly separable with respect to these two features, it may be\\nseparable with respect to all thirteen features.\\nNow you can assign test inputs to clusters by running the following:\\ntest_preds = Scholar.Cluster.KMeans.predict(model, \\ntest_inputs)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 245}),\n",
       " Document(page_content='This code assigns test inputs to one of the clusters created in the previous\\nstep. After running the code, you can evaluate the accuracy of your clusters\\nusing the following code:\\nScholar.Metrics.accuracy(test_targets, test_preds)\\nAfter running this code, you’ll see the following output:\\n#Nx.Tensor<\\n  f32\\n  0.8333333134651184\\n>\\nSurprisingly, your unsupervised approach performed better than both\\nsupervised approaches.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 246}),\n",
       " Document(page_content='Making Decisions\\nPerhaps the most popular non-deep-learning algorithms in use today are\\ndecision trees and their ensemble variants, such as gradient boosting.\\nDecision trees behave exactly as they sound: they construct nested trees\\nbased on input features. Decision trees construct a hierarchical decision\\nflow that partitions input features into one of a desired number of classes.\\nFor example, imagine you were trying to create a model to predict whether\\nor not a patient was at risk of heart disease. A decision tree would construct\\na tree that analyzes data in an interpretable way. Does the patient smoke?\\nDoes the patient have high blood pressure? Is the patient physically active?\\nDecision trees are incredibly popular because they are interpretable, and\\nthey perform very well on certain classes of data. Decision trees are one of\\nthe few classes of algorithms that outperform deep learning models,\\nspecifically on tabular data and time-series data. As tabular data contains a\\nmajority of business intelligence data, decision trees have a wide use in\\nbusiness applications.\\nGradient boosting is a type of ensemble method that constructs many weak\\nclassifiers iteratively by building classifiers to cover the weaknesses of\\nprevious classifiers. Ensemble methods are a class of machine learning\\nmethods that construct a model, which is an aggregate of many models. You\\ncan construct ensembles in many ways. You can think of ensembling as', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 247}),\n",
       " Document(page_content='getting multiple diverse opinions on the same topic. Each model brings its\\nown strengths and weaknesses to the table, so generally, the final decision is\\nmore sound. Gradient boosting is an ensemble technique that commonly\\nuses decision trees as the underlying classifier.\\nElixir has a new decision tree library in EXGBoost. EXGBoost\\nintegrates directly with Nx tensors and is relatively simple. Decision trees\\nare outside the scope of this book, but they are important to be aware of and\\nexplore as a blooming machine learning engineer.\\n[10]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 248}),\n",
       " Document(page_content='[9]\\n[10]\\nWrapping Up\\nIn this chapter, you implemented a few types of traditional machine\\nlearning algorithms—supervised and unsupervised—using Scholar. You\\nmodeled linear data with linear regression. You predicted wine type in three\\nways with logistic regression, KNN, and K-Means.\\nYou also learned a little about decision trees and boosting. This chapter is a\\nshort introduction to the functionality available in Scholar and the power of\\ntraditional machine learning algorithms. As you continue your machine\\nlearning journey, try some of the other algorithms available in Scholar, and\\ncompare them to the models you’ll create in the rest of this book.\\nAt this point, you’ve completed your introduction to the foundations of\\nmachine learning, and you’re ready to go deeper with Axon. In the next\\nchapter, you’ll start to unveil the mystery behind the techniques that power\\nthe likes of ChatGPT, DALL-E 2, and Stable Diffusion: deep learning.\\nFOOTNOTES\\nhttps://github.com/elixir-nx/scholar\\nhttps://hex.pm/packages/exgboost\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 249}),\n",
       " Document(page_content='', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 250}),\n",
       " Document(page_content='Part 2\\nDeep Learning', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 251}),\n",
       " Document(page_content='Chapter 6\\nGo Deep with Axon\\n\\xa0\\nIn the previous chapter, your focus was primarily on understanding\\n“traditional” or “shallow” machine learning approaches. The approaches\\nyou used, such as linear and logistic regression, ensemble methods, and\\ndecision trees, are well-suited for creating valuable, performant solutions to\\nmany machine learning problems you will encounter in production.\\nHowever, the rise of deep learning over the past decade has led to\\nsignificant progress in the field of artificial intelligence and machine\\nlearning.\\nToday, variants of deep learning models boast state-of-the-art results on\\ncountless tasks in computer vision, natural language processing,\\nreinforcement learning, and more. The rapid adoption of deep learning and\\nthe progress it has brought to machine learning mean that being familiar\\nwith how to create, train, and deploy neural networks is an absolute must\\nfor any aspiring machine learning practitioner. In this chapter, you’ll dive\\nhead-first into deep learning in Elixir and set the stage for the coming\\nchapters where you’ll learn to apply deep learning to different data\\nmodalities. You’ll start by breaking down neural networks in Nx and then', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 252}),\n",
       " Document(page_content='move on to Axon and learn more about what Axon is and what\\nconveniences it supplies.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 253}),\n",
       " Document(page_content='Understanding the Need for Deep Learning\\nImagine you’ve been asked to create an algorithm that automatically\\nscrapes handwritten phone numbers from a field on a paper form. Your\\nteam has already automated the process of extracting individual digits from\\nthe form into a 28x28 grayscale image. How would you go about solving\\nthis problem using the approaches you’ve used in the previous chapters of\\nthis book?\\nYour initial thoughts might be that it’s easy enough to throw this problem at\\na logistic regression model or a decision tree. But what would you use for\\nthe feature space of your model? Would you treat each individual pixel\\nvalue as a feature?\\nYou could potentially divide the image into a grid, as shown in the\\nfollowing image.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 254}),\n",
       " Document(page_content='From there, you could then reduce each cell into a scalar metric and use the\\nvector of the scalar metrics as your feature space. However, this approach\\nhas the potential to lose critical information about the image.\\nThe algorithms you worked with in the previous chapter rely on in-depth\\nfeature engineering on the part of the programmer. Traditional machine\\nlearning algorithms require heavy investment in discovering rich-enough\\nrepresentations of the data prior to the learning process—but discovering\\nrepresentations by hand can be challenging.\\nDomains that are rich in information and those that have seemingly\\ncomplex, unstructured relationships are often the ones that pose the most\\nchallenges. For data types such as images, text, and audio it’s difficult to\\nextract features by hand. While you could spend hours developing code to\\nextract information about your images, such as corners and contours, the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 255}),\n",
       " Document(page_content='process would still be imperfect, sensitive to distribution changes, and lossy\\nin nature.\\nA better way to extract meaningful representations of your data is to\\ndelegate that task to your machine learning algorithm. By doing so, your\\nalgorithm learns not only how to make predictions from features but also\\nhow to extract features from information-dense inputs. The ability to extract\\nrepresentations from high-dimensional inputs—and learn to make\\npredictions from those representations—is the draw of deep learning.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 256}),\n",
       " Document(page_content='Deep Learning’s Kodak Moment\\nDeep learning has been around for a long time. But, until the\\nlast decade, deep learning was an afterthought of the machine\\nlearning community. Deep learning researchers and\\npractitioners made up a fraction of a fraction of the machine\\nlearning community. Collectively, the machine learning\\ncommunity believed deep learning was a ridiculous idea and\\ncould never be used to solve anything of merit. This all\\nchanged in 2012 when graduate students Alex Krizhevsky and\\nIlya Sutskever, along with their advisor Geoffrey E. Hinton,\\nwon the “ImageNet” competition with their model dubbed\\nAlexNet [KSH12]. Their model outperformed competitors by\\nsuch a large margin that both industry and academics started\\nopening their eyes to the power of deep learning.\\nThe Curse of Dimensionality\\nComplex inputs, such as images, audio, and text, are often represented in\\nhigh-dimensional space. Recall from Chapter 2,  Get Comfortable with Nx,\\nhow each of these inputs is often encoded as tensors. Images, for example,\\nare represented as rank-3 tensors of {height, width, channels}. When flattened\\nto a vector for input into a model, an image has the following dimensions:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 257}),\n",
       " Document(page_content='height * width * height. With the 28x28 grayscale images, a single image has\\n784 pixels, or 784 features given by 28 * 28 * 1.\\nThe complexity of a machine learning problem increases significantly as\\nthe dimensionality of the inputs increases. Consider for inputs with a single\\nfeature, you only need a model to capture how the variations in that single\\nfeature map to the variations in the output. As you increase the number of\\nfeatures, your model needs to capture how the variations in many features,\\nwhich may or may not be related, map to the variations in the output.\\nIn machine learning, as the number of dimensions of the input space\\nincreases, the quality of the model increases too. However, at a certain\\npoint, the dimensionality becomes too high, and the quality of the model\\ndiminishes. This phenomenon is known as the curse of dimensionality.\\nTo better understand this concept, think of your model as a conductor in an\\norchestra. The conductor’s job is relatively easy when only one instrument\\nis in the band. Adding more instruments may improve the sound quality and\\nmusic, but the conductor’s job becomes more challenging with each\\naddition. At a certain point, adding instruments leads to diminishing returns,\\nwhich means the conductor’s job becomes too difficult, and the sound\\nquality diminishes. This relationship between instruments and conductors\\nillustrates the curse of dimensionality in machine learning.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 258}),\n",
       " Document(page_content='The differentiator between deep learning and the algorithms you learned\\nabout in the previous chapters is that deep learning is able to overcome the\\ncurse of dimensionality. But how?\\nCascading Representations\\nUnderstanding why deep learning works so well is still an active area of\\nresearch. One theory on how deep learning is able to overcome the curse of\\ndimensionality lies in how a neural network transforms input data. Neural\\nnetworks transform inputs into hierarchical representations via composing\\nlinear and nonlinear transformations. A neural network has a series of layers\\n—each of which takes the previous layer’s representation as input and\\ntransforms it into its own representation before finally outputting a\\nprediction. You’ll break down the anatomy of a layer in  Breaking Down a\\nNeural Network. For now, you need only to understand that a layer is just a\\nfunction.\\nGoing back to the conductor/orchestra analogy, deep learning overcomes\\nthe problem of “too many instruments” by adding additional conductors.\\nOne conductor is responsible for the violins, another for the trumpets, and\\nanother for the flutes. Each conductor only needs to pay attention to their\\nsection, and as a whole, the orchestra sounds significantly better than if a\\nsingle conductor was responsible for getting everybody on the same sheet\\nof music.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 259}),\n",
       " Document(page_content='More concretely, the theory of why deep learning works so well is deep\\nmodels are able to learn successive, hierarchical representations of input\\ndata. For example, if you train a model to identify images of cats from input\\ndata, the first layer might learn to extract colors, the second might learn to\\nextract corners and edges, and so on. The first few layers extract simple\\nrelationships from the input data, while later layers start to extract more\\ncomplex relationships from those simple relationships. Later layers might\\nlearn to identify ears or paws. By composing simple representations, a\\nneural network is able to learn more complex representations and,\\nconsequently, make predictions on complex inputs.\\nIn a sense, neural networks eliminate the need for methodical feature\\nengineering by making feature extraction a part of the learning process.\\nThat’s not to say that training a neural network does not require quality data\\n—and time spent discovering new features isn’t necessary to train better\\nneural networks. It’s just that neural networks perform well with complex,\\nhigh-dimensional inputs and eliminate the need to perform complex\\ndimensionality reduction or feature extraction on those inputs.\\nRepresenting Any Function\\nTheoretically, neural networks are said to be universal function\\napproximators. A universal approximator is a model that can approximate\\nany complex function when given the correct parameters. At first, this idea\\nseems a bit silly. You’re not interested in approximating functions; you’re', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 260}),\n",
       " Document(page_content='interested in making models that make predictions. But, if you consider the\\nidea that the data you want to make predictions on comes from some black-\\nbox function, the universal approximation becomes much more appealing.\\nIn theory, a neural network—when given the right configuration—can\\napproximate any of the functions you want to learn.\\nIn reality, it’s left to be said whether or not universal approximation is all\\nthat useful. The theory behind deep learning is still well behind the\\nempirical results of the field, and therefore, theoretical assertions aren’t\\nbacked by much mathematical rigor.\\nAnother way to think of neural networks is they’re learned computer\\nprograms. Imagine each layer in a neural network as an instruction, and the\\ninput to each layer as the state of the program at the time of instruction\\nexecution. You may be familiar with the sarcastic quip that artificial\\nintelligence in production is nothing more than a bunch of if-else\\nstatements. Viewing a neural network as a learned program might support\\nthis assertion—the neural network may just learn the right configuration of\\nif-else statements to be an effective model.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 261}),\n",
       " Document(page_content='Breaking Down a Neural Network\\nWith an understanding of why neural networks are necessary and why\\nthey’re so powerful, it’s time to dive into the question of what is a neural\\nnetwork. In this section, you’ll break down the vocabulary surrounding\\ndeep learning, the anatomy of neural networks, and what a neural network\\nactually looks like in Elixir. Understanding the building blocks of neural\\nnetworks will help build your intuition as you start to use Axon.\\nGetting the Terminology Right\\nDeep models, neural networks, artificial neural networks (ANNs), multi-\\nlayer perceptrons (MLPs)—if you’ve spent some time reading about deep\\nlearning, you likely have encountered all of these terms used almost\\ninterchangeably. The terminology of deep learning can be more daunting\\nthan implementing the models themselves.\\nDeep learning refers to a subset of machine learning algorithms that make\\nuse of deep models, or artificial neural networks. These models are\\nconsidered “deep” as opposed to other models with respect to their layers of\\nsuccessive computation. If you roll out the computation graph of operations\\nthat take place in a deep model, the computation graph would appear deep\\n(for example, lots of operations). You can also consider models deep with\\nrespect to the number of intermediate layers—with each successive layer\\nincreasing the depth of the model.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 262}),\n",
       " Document(page_content='Artificial neural networks (ANNs) are one term for deep models. The term\\nartificial neural network probably invokes the thought of images similar to\\nthis:\\nANNs are named for their brain-inspired design. The transformation of\\ninputs in an ANN is meant to simulate the firing of neurons passing\\ninformation around the brain. The usage of the term ANN is probably a bit', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 263}),\n",
       " Document(page_content='of a misnomer, as there’s little evidence to suggest the brain works in the\\nsame way that neural networks do.\\nMulti-layer perceptrons (MLPs) are a class of deep learning models that\\nmake use of fully connected layers or densely connected layers. You will\\nsee what this means in  The Anatomy of a Neural Network . All you need to\\nknow is that MLPs are a specific class or architecture of neural networks.\\nYou might also see them referred to as feed-forward networks because\\ninformation flows from previous layers forward toward output layers. You\\nwill implement many other architecture types in this book, including\\nConvolutional Neural Networks (CNNs) in Chapter 7,  Learn to See ,\\nRecurrent Neural Networks (RNNs) in Chapter 9,  Understand Text , and\\nGenerative Adversarial Networks (GANs) in Chapter 12,  Learn Without\\nSupervision .', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 264}),\n",
       " Document(page_content='The Rebranding\\nBefore deep learning had its watershed moment in 2012, the\\nfield was led by a relatively small number of researchers.\\nMost top machine learning conferences would accept only one\\nor two deep learning papers per year—if they accepted any. At\\nthe time, researchers working on deep learning were referred\\nto as “connectionists” because of the connections between\\nlayers when visualizing deep models. Deep learning came\\nabout as a strategic rebranding by connectionists in an attempt\\nto overcome the bias against neural networks at the time.\\nThe Anatomy of a Neural Network\\nMost neural networks can be simplified down to a few key components.\\nThe most common abstraction for a unit of computation or work in a neural\\nnetwork is a layer. Typically, a layer represents a transformation of the input\\nwhich is to be forwarded to the next layer. The number of layers in the\\nmodel is typically referred to as the depth of the model. Generally,\\nincreasing the depth of the model also increases the capacity of the model.\\nHowever, at a certain point, making a model too deep can hinder the\\nlearning process.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 265}),\n",
       " Document(page_content='You’ll use many different types of layers throughout the rest of this book. In\\na neural network, you’ll generally have three classes of layers: input layers,\\nhidden layers, and output layers.\\nInput Layers\\nInput layers are placeholders for model inputs. Certain operations on a\\nneural network require a known input shape. You can refer back to Chapter\\n2,  Get Comfortable with Nx , to get a better idea of how certain real-world\\ndata maps to tensor inputs.\\nHidden Layers\\nHidden layers are where the magic happens in a neural network. They are\\nintermediate layers of computation that transform the input into a useful\\nrepresentation for the output layer. They are the additional conductors in a\\nlarge orchestra that make high-dimensional inputs manageable for the\\noutput layer.\\nThe most common hidden layer is the densely connected, fully connected,\\nor simply dense layer. The dense layer is named for the dense connections it\\ncreates between two layers—in other words, every input to a dense layer\\nmaps to an output in the dense layer. Dense layers have a number of output\\nunits, which represent the dimensionality of the dense layer. If you like the\\nanalogy of neural networks to the brain, you can think of an individual unit\\nas a neuron. A dense layer with 128 units has 128 neurons.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 266}),\n",
       " Document(page_content='The number of units in a dense layer is referred to as the width of the layer.\\nWider dense layers have more representational capacity. However, there’s\\nalso a point of diminishing returns. It’s common to use hidden widths that\\nare multiples of two. This is because of how memory layouts work on\\nmodern accelerators.\\nMathematically, dense layers are matrix multiplications or linear\\ntransformations. Dense layers learn to project inputs in such a way that\\nextracts a useful representation for successive layers.\\nActivations\\nHidden layers often have an activation function that applies a nonlinear\\nfunction to the output. The introduction of nonlinearities into the neural\\nnetwork is what makes it a universal approximator. It’s common to use\\nactivation functions that scale or squeeze inputs into some useful output\\nrange. For example, the sigmoid function is often used as an activation\\nbecause it squeezes outputs between 0 and 1. Because neural networks are\\ntrained with gradient descent, it’s important that activation functions be\\ndifferentiable.\\nYou can think of activation functions as a means of signaling certain input\\nfeatures. For example, your neural network might learn to only have certain\\nneurons firing on certain input features. A neuron’s activation can be\\ninterpreted as its importance to the final output. Some neurons that aren’t\\nimportant will be entirely “turned off.”', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 267}),\n",
       " Document(page_content='You can use a number of activation functions in a neural network. Finding\\nbetter activation functions is a popular area of research. The activation\\nfunctions you should be familiar with are ReLU, sigmoid, and softmax.\\nReLU\\nThe Rectified Linear Unit (ReLU) activation function is a popular\\nintermediate activation that computes the function:\\ndefn relu(x)  do \\n  Nx.max(0, x)\\nend \\nReLU takes all negative inputs to 0 and maps positive inputs to the same\\nvalue.\\nSigmoid\\nThe sigmoid activation function is a popular output activation because it\\nsqueezes outputs to the range 0-1. It computes the logistic sigmoid function:\\ndefn sigmoid(x)  do \\n  1 / (1 + Nx.exp(-x))\\nend \\nThe sigmoid function is especially useful when you’re trying to compute an\\noutput probability between 0 and 1.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 268}),\n",
       " Document(page_content='Softmax\\nThe softmax function is a popular output activation for multi-class\\nclassification problems. It outputs a categorical probability distribution.\\nOutput Layers\\nFrom an implementation perspective, output layers are no different than\\ninput layers. Output layers are the final result of your neural network. After\\ntransforming your inputs into useful representations with hidden layers,\\noutput layers transform those representations into something you can\\nmeaningfully use or interpret, such as a probability.\\nFor classification problems, it’s common to use a dense layer with a\\nsigmoid or softmax activation as the final output layer. For binary\\nclassification problems, the final layer will usually be a dense layer with\\none output unit and sigmoid activation. For multi-class classification\\nproblems, the final layer will usually be a dense layer with N output units\\nand softmax activation, where N is the number of possible classes.\\nFor scalar regression problems, it’s common to use a dense layer with one\\noutput unit and no activation—so the output neuron just maps to a scalar.\\nThe form of an output layer is problem-dependent. You’ll see lots of\\ndifferent output layers throughout the rest of this book.\\nUsing Nx to Create a Simple Neural Network', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 269}),\n",
       " Document(page_content='While neural networks might seem conceptually challenging to understand,\\nthey’re actually surprisingly easy to implement. To illustrate this point,\\nyou’ll implement a simple neural network using Nx. Open a Livebook and\\nadd the following dependencies:\\nMix.install([\\n  { :nx ,  \"  ~> 0.5\" }\\n])\\nRemember from the previous section that a neural network consists only of\\ninputs, hidden layers, intermediate activations, and output layers. Start by\\ncreating a new module named NeuralNetwork:\\ndefmodule  NeuralNetwork  do \\n   import  Nx.Defn\\nend \\nNext, you need to implement a hidden layer. Your hidden layer will be a\\ndensely connected or a dense layer that performs a matrix multiplication of\\ntwo dense matrices followed by a bias add. A bias add is just a shift in the\\noriginal input. The dense layer takes two parameters—weight and bias—and\\ntransforms the input using both weight and bias. Implement the dense layer\\nlike so:\\ndefn dense(input, weight, bias)  do \\n  input', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 270}),\n",
       " Document(page_content=' |> Nx.dot(weight)\\n  |> Nx.add(bias)\\nend \\nHidden layers are followed by intermediate activation functions. To keep\\nthings simple, you’ll implement only the sigmoid activation function using\\nNx. Implement your activation function like this:\\ndefn activation(input)  do \\n  Nx.sigmoid(input)\\nend \\nYou can combine both dense and activation into a single hidden layer like this:\\ndefn hidden(input, weight, bias)  do \\n  input\\n  |> dense(weight, bias)\\n  |> activation()\\nend \\nFinally, you need an output layer. Your output layer will simply be another\\ndense layer, followed by your activation function. The implementation is\\nidentical to hidden, but you should be explicit here to get an idea of what’s\\ngoing on:\\ndefn output(input, weight, bias)  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 271}),\n",
       " Document(page_content=' input\\n  |> dense(weight, bias)\\n  |> activation()\\nend \\nNow, add a predict function to your NeuralNetwork module:\\ndefn predict(input, w1, b1, w2, b2)  do \\n  input\\n  |> hidden(w1, b1)\\n  |> output(w2, b2)\\nend \\nThe predict function shown here actually implements a simple neural\\nnetwork. It takes an input and some trainable parameters and produces a\\ntransformed output. Notice that the neural network is simply a composition\\nof functions. You can add additional hidden layers by adding some\\nadditional parameters and additional calls to the hidden function.\\nYour final NeuralNetwork module will look like this:\\ndefmodule  NeuralNetwork  do \\n   import  Nx.Defn\\n  defn dense(input, weight, bias)  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 272}),\n",
       " Document(page_content='   input\\n    |> Nx.dot(weight)\\n    |> Nx.add(bias)\\n   end \\n  defn activation(input)  do \\n    Nx.sigmoid(input)\\n   end \\n  defn hidden(input, weight, bias)  do \\n    input\\n    |> dense(weight, bias)\\n    |> activation()\\n   end \\n  defn output(input, weight, bias)  do \\n    input\\n    |> dense(weight, bias)\\n    |> activation()\\n   end \\n  defn predict(input, w1, b1, w2, b2)  do \\n    input\\n    |> hidden(w1, b1)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 273}),\n",
       " Document(page_content='   |> output(w2, b2)\\n   end \\nend \\nYour next step is to generate the intermediate parameters w1, b1, w2, and b2\\nfor your hidden and output layers. To keep things simple, you’ll only input\\nscalar shapes to your neural network, so your parameters can all be scalars\\nas well:\\nkey = Nx.Random.key(42)\\n{w1, new_key} = Nx.Random.uniform(key)\\n{b1, new_key} = Nx.Random.uniform(new_key)\\n{w2, new_key} = Nx.Random.uniform(new_key)\\n{b2, new_key} = Nx.Random.uniform(new_key)\\nFinally, you can start generating predictions for random inputs:\\nNx.Random.uniform_split(new_key,  shape:  {})\\n|> NeuralNetwork.predict(w1, b1, w2, b2)\\nYou’ll see outputs similar to the following:\\n#Nx.Tensor<\\n  f32\\n  0.6635995507240295', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 274}),\n",
       " Document(page_content='>\\nOne thing you’ll notice is that it’s trivial to implement neural networks\\nwithout any additional dependencies other than Nx. Combined with Nx’s\\nautomatic differentiation capabilities—and some of the optimization\\nconcepts you learned about in Chapter 4,  Optimize Everything —you can\\nadd training functionality and train complicated neural networks on real\\ntraining data.\\nAnother thing you’ll notice is that there’s a lot of boilerplate code that goes\\ninto creating neural networks in Nx. While you could theoretically write\\ncomplicated neural networks without anything other than Nx, it would be a\\nbit of a cumbersome process. Fortunately, you can use Axon.[11]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 275}),\n",
       " Document(page_content='Creating Neural Networks with Axon\\nAxon is a library for creating and training neural networks in Elixir and is\\nthe primary tool for deep learning in the Elixir ecosystem. For the\\nremainder of this book, you’ll spend time diving deep into the Axon API\\nand using Axon to create and train neural networks on complex problems.\\nAs you saw in the previous section, there’s a lot of boilerplate code\\nassociated with creating neural networks in Nx. Axon abstracts all of the\\nboilerplate code and offers a simplified API for building and training neural\\nnetworks.\\nAs an introduction to deep learning in Elixir, you’ll train an algorithm that\\nsolves the problem of classifying handwritten digits, as described earlier in  \\nUnderstanding the Need for Deep Learning . The best way to follow along\\nis to fire up another Livebook and install the following dependencies:\\nMix.install([\\n  { :axon ,  \"  ~> 0.5\" },\\n  { :nx ,  \"  ~> 0.5\" },\\n  { :exla ,  \"  ~> 0.5\" },\\n  { :scidata ,  \"  ~> 0.1\" },\\n  { :kino ,  \"  ~> 0.8\" },\\n  { :table_rex ,  \"  ~> 3.1.1\" }\\n])', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 276}),\n",
       " Document(page_content='Most of these dependencies should look familiar because you’ve already\\nused Nx, EXLA, and Scidata extensively throughout this book. Nx acts as\\nthe core library for working with numerical data, and EXLA accelerates\\nyour code. Scidata contains the dataset you’ll need for training, and Axon is\\nwhat you’ll use to create and train your model. Both Kino and TableRex are\\nuseful for visualizing Axon models in Livebooks and consoles directly.\\nBefore moving on, you’ll need to set some notebook options:\\nNx.default_backend(EXLA.Backend)\\nThis ensures that all of your defn-compiled code makes use of the EXLA\\nbackend. A decent CPU is all you need for this problem, but GPU\\nacceleration never hurts.\\nWorking with Data\\nWith all of the administrative work done, it’s time to prepare your data for\\ntraining. This problem uses the MNIST dataset, which consists of 60,000\\n28x28 grayscale images of handwritten digits from 0 to 9. MNIST is an\\nincredibly popular dataset in the field of machine learning and often serves\\nas the “Hello world” example of deep learning.\\nScidata has built-in functionality for downloading MNIST. You can get both\\nMNIST images and labels like this:\\n[12]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 277}),\n",
       " Document(page_content='{images, labels} = Scidata.MNIST.download()\\nRemember that Scidata isn’t designed to be Nx-aware. Both images and\\nlabels consist of tuples of the form {data, type, shape}, which can be used with\\nNx functions to convert raw data into Nx functions. If you inspect\\nimage_type, you’ll notice it’s {:u, 8}, which is an unsigned 8-bit integer. The\\nMNIST images are grayscale, with each pixel having a value between 0 and\\n255. You’ve learned throughout this book that it’s important to normalize\\ndata before passing it through a model. The same concept holds true for\\ndeep learning, so you’ll need to rescale pixel values to be between 0 and 1.\\nYou can do that by dividing the entire images tensor by 255.\\nAdditionally, image_shape is a tuple of {60_000, 1, 28, 28}. You can design\\nneural networks to work with this image representation, but for this\\nexample, you’ll flatten each image into a vector.\\nFinally, you might notice that labels consists of 60,000 labels from 0 to 9.\\nThat’s not necessarily a bad thing, but in Axon, it’s easier to work with\\nlabels that are one-hot encoded, so you’ll turn them into one-hot labels.\\nWith all of these requirements in mind, you can transform the raw data\\nScidata returned into Nx tensors like so:\\n{image_data, image_type, image_shape} = images\\n{label_data, label_type, label_shape} = labels', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 278}),\n",
       " Document(page_content='images =\\n  image_data\\n  |> Nx.from_binary(image_type)\\n  |> Nx.divide(255)\\n  |> Nx.reshape({60000,  :auto })\\nlabels =\\n  label_data\\n  |> Nx.from_binary(label_type)\\n  |> Nx.reshape(label_shape)\\n  |> Nx.new_axis(-1)\\n  |> Nx.equal(Nx.iota({1, 10}))\\nNotice that you use Nx.from_binary on the raw data and type for both images\\nand labels to transform them into an Nx tensor. You divide image data by\\n255 to rescale pixel values between 0 and 1 before reshaping the entire\\nimages tensor into a collection of 60,000 image vector representations. You\\none-hot encode the labels by taking advantage of Nx’s broadcasting and\\ncomparing each label to a vector of numbers from 0 to 9.\\nWith preprocessed data in hand, you’ll need to divide your data into training\\nand test sets. To do this, you’ll take the first 50,000 images in the dataset as\\nyour training set and the last 10,000 images in the dataset as your test set.\\nYou can slice the dataset using Nx’s slice notation to accomplish this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 279}),\n",
       " Document(page_content='train_range = 0..49_999//1\\ntest_range = 50_000..-1//1\\ntrain_images = images[train_range]\\ntrain_labels = labels[train_range]\\ntest_images = images[test_range]\\ntest_labels = labels[test_range]\\nAs you’ll see later on in this section, Axon offers a training API that\\nperforms minibatch stochastic gradient descent. Remember from Chapter 4,\\n Optimize Everything , that machine learning algorithms often iteratively\\nupdate models on the training set using optimization techniques, such as\\ngradient descent. To perform gradient descent on minibatches, you need to\\nturn your data into minibatches as well:\\nbatch_size = 64\\ntrain_data =\\n  train_images\\n  |> Nx.to_batched(batch_size)\\n  |> Stream.zip(Nx.to_batched(train_labels, \\nbatch_size))\\ntest_data =', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 280}),\n",
       " Document(page_content=' test_images\\n  |> Nx.to_batched(batch_size)\\n  |> Stream.zip(Nx.to_batched(test_labels, \\nbatch_size))\\nThis code creates both train and test datasets that consist of minibatches of\\ntuples {input, target}—which is the format expected by Axon.\\nNow that you’ve preprocessed train and test sets, it’s time to create the\\nmodel.\\nBuilding the Model\\nAxon’s key feature is its model-creation API. The API allows you to build\\ncomplex models by composing Axon layers. All models start with an input\\nlayer that specifies the shape Axon should expect inputs to be. You then\\npass input layers through successive transformations until you have your\\ndesired output. Start by implementing a basic Axon model, like this:\\nmodel =\\n  Axon.input( \"  images\" ,  shape:  {nil, 784})\\n  |> Axon.dense(128,  activation:   :relu )\\n  |> Axon.dense(10,  activation:   :softmax )\\nAfter executing this cell, you’ll see some summary information about your\\nmodel:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 281}),\n",
       " Document(page_content='#Axon<\\n  inputs: %{\"images\" => {nil, 784}}\\n  outputs: \"softmax_0\"\\n  nodes: 5\\n>\\nDesigning Models\\nYou specify a model that takes an input shape of {nil, 784}. Axon allows you\\nto use nil as a placeholder for values that will be filled at inference time. The\\ninput layer is passed through a hidden dense layer with 128 units and a :relu\\nactivation before going through another dense layer with 10 units and\\n:softmax activation.\\nCreating models with Axon is relatively straightforward—the difficult part\\nis developing an understanding of what combination of layers to use to\\ncreate the best model.\\nFor this example, the input layer with shape {nil, 784} maps directly to input\\nimages that are batches of vectors of dimensionality 784. Axon needs to\\nknow input shapes ahead of time because it affects how successive layers\\nare built. If you pass an input with a shape that doesn’t match what Axon\\nexpects, it will raise an error.\\nThe intermediate dense layer with 128 units and :relu activation is an\\narbitrary choice. You can freely change the input units and activation', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 282}),\n",
       " Document(page_content='function and even add extra hidden layers. Hypothetically, more hidden\\nunits and more hidden layers increase the representational capacity of the\\nmodel. However, there’s a point where your neural network becomes too\\nwide or too deep for your training data, and the model will struggle to learn.\\nYou’ll often want to experiment with multiple configurations to find one\\nthat produces the best result on your data.\\nThe output layer has ten units and :softmax activation. Remember that your\\ngoal is to map input images to a label between 0 and 9. This layer will\\noutput a probability at each index that represents the probability that the\\ninput digit matches that index. More concretely, the probability at index 0\\nrepresents the probability that the input is a 0; the probability at index 1\\nrepresents the probability that the input is a 1, and so on.\\nDesigning neural networks is often more art than science. You need to keep\\nin mind certain principles and architectures that work well for specific\\nproblem types. However, designing an effective model often just takes\\nsome intuition and experimentation. The rest of this book will teach you\\nbroadly what kind of models to use in various situations, but you won’t find\\nany magic recipes for solving specific problems.\\nInside the Model Representation\\nAssuming you ran this in a Livebook, you can visualize the Axon model\\nusing Kino and Axon’s Axon.Display module:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 283}),\n",
       " Document(page_content='template = Nx.template({1, 784},  :f32 )\\nAxon.Display.as_graph(model, template)\\nAxon.Display.as_graph/2 traces the execution of the model and outputs a\\nMermaid graph so you can easily visualize the execution. You’ll see the\\nfollowing output:\\nYou can also display the model as a table using Axon.Display.as_table/2:\\nAxon.Display.as_table(model, template)\\n|> IO.puts\\nAfter running the code, you’ll see this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 284}),\n",
       " Document(page_content='The description tells you layer names and inputs, layer shapes, trainable\\nparameters, and options in each layer. If you’re curious about what this\\nmodel object actually is, you can inspect it again and pass structs: false:\\nIO.inspect model,  structs:  false\\nAnd you’ll see a longer version:\\n%{\\n   __struct__:  Axon,\\n   nodes:  %{\\n    12 => %{\\n       __struct__:  Axon.Node,\\n       args:  [],\\n       hooks:  [],', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 285}),\n",
       " Document(page_content='      id:  12,\\n       name:   #Function<90.106397181/2 in \\nAxon.unique_identifiers/2>, \\n       op:   :input ,\\n       op_name:   :input ,\\n       opts:  [ shape:  {nil, 784},  optional:  false],\\n       parameters:  [],\\n       parent:  [],\\n       policy:  %{\\n         __struct__:  Axon.MixedPrecision.Policy,\\n         compute:  { :f , 32},\\n         output:  { :f , 32},\\n         params:  { :f , 32}\\n      },\\n       stacktrace:  ...\\n    },\\n    ...\\n  },\\n   output:  20\\n}\\nThe model is actually a regular Elixir struct. Axon models are mere\\nrepresentations of computation graphs that you can manipulate and', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 286}),\n",
       " Document(page_content='compose before passing to Axon’s execution and training APIs. You build\\nthe graph lazily and then use it when you actually need to.\\nThis intermediate representation is useful because it allows Axon to\\naccomplish all sorts of seemingly “magic” things. You’ll get exposed to\\nmore and more of this magic throughout the rest of this book. It also allows\\nAxon to convert to and from other neural network representations such as\\nONNX.  This comes in handy for deploying models and using models\\nfrom the Python ecosystem, as you’ll see in Chapter 8,  Stop Reinventing the\\nWheel .\\nTraining the Model\\nYou now have data and a model in hand. The next step is to train your\\nmodel on your training data. While you could write a gradient descent\\nimplementation from scratch, it would be cumbersome and error-prone.\\nFortunately, Axon has conveniences for training neural networks using\\ngradient descent.\\nAxon’s training abstraction lies in the Axon.Loop module. Axon.Loop contains\\nfunctions for building up an %Axon.Loop{} data structure, which you can then\\nrun on input data using Axon.Loop.run/3. You can do some pretty complicated\\nthings with the loop API. But most of the time you only want to implement\\na simple supervised training loop:\\ntrained_model_state =\\n[13]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 287}),\n",
       " Document(page_content=' model\\n  |> Axon.Loop.trainer( :categorical_cross_entropy , \\n:sgd )\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.run(train_data, %{},  epochs:  10,  \\ncompiler:  EXLA)\\nAxon.Loop.trainer/3 is a factory function for a supervised training loop that\\ntakes an Axon model, loss function, and optimizer as input. Note that\\n“supervised” here means supervised in the machine learning sense, not the\\nOTP sense. The trainer will optimize model by minimizing loss using\\noptimizer. In this example, you pass the model you created earlier, the\\n:categorical_cross_entropy loss function, which tells Axon you’re working with\\na multi-class classification problem, and the :sgd optimizer, which tells\\nAxon to use stochastic gradient descent.\\nAxon.Loop.metric/2 attaches metrics to the supervised training loop. This tells\\nAxon to monitor and compute specific metrics throughout the training loop.\\nFor this example, you’re primarily concerned with the accuracy of your\\npredictions because you’re dealing with a classification problem.\\nFinally, Axon.Loop.run/3 tells Axon to execute the given training loop on the\\ngiven data with the given options. Axon will iterate through train_data for\\nten epochs, and finally output a trained model state. Passing compiler: EXLA\\nensures that Axon will JIT-compile intermediate train steps using EXLA.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 288}),\n",
       " Document(page_content='This will speed up training significantly. When you execute this cell, you’ll\\nsee this model:\\nEpoch: 0, Batch: 750, accuracy: 0.9439270 loss: \\n0.9746160\\nEpoch: 1, Batch: 750, accuracy: 0.9759654 loss: \\n0.7108651\\nEpoch: 2, Batch: 750, accuracy: 0.9799850 loss: \\n0.5978507\\nEpoch: 3, Batch: 750, accuracy: 0.9817263 loss: \\n0.5321623\\nEpoch: 4, Batch: 750, accuracy: 0.9830977 loss: \\n0.4878981\\nEpoch: 5, Batch: 750, accuracy: 0.9840345 loss: \\n0.4553202\\nEpoch: 6, Batch: 750, accuracy: 0.9849032 loss: \\n0.4299005\\nEpoch: 7, Batch: 750, accuracy: 0.9856064 loss: \\n0.4092346\\nEpoch: 8, Batch: 750, accuracy: 0.9862743 loss: \\n0.3919075\\nEpoch: 9, Batch: 750, accuracy: 0.9868216 loss: \\n0.3770282', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 289}),\n",
       " Document(page_content='Notice how the model iterated through your dataset ten times, with each\\niteration leading to higher accuracy and lower loss. You might also notice\\nthat the training loop returned something that looks like this:\\n%{\\n  \"dense_0\" => %{\\n    \"bias\" => #Nx.Tensor<\\n      f32[128]\\n      EXLA.Backend<host:0, \\n0.2489291390.1016463368.89685>\\n      [...]\\n    >,\\n    \"kernel\" => #Nx.Tensor<\\n      f32[784][128]\\n      EXLA.Backend<host:0, \\n0.2489291390.1016463368.89686>\\n      [\\n        [...],\\n        ...\\n      ]\\n    >,\\n  },\\n  \"dense_1\" => %{...}\\n}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 290}),\n",
       " Document(page_content='This is the trained_model_state. The Axon struct is a stateless representation\\nof a neural network. It doesn’t carry any of the model parameters or state\\ninternally. That means when performing inference and evaluation, you\\nalways need access to both the model and a model state compatible with the\\ngiven model. Axon’s supervised training loop will always output a model\\nstate that’s compatible with the model you’re training.\\nThe model state is a nested map of namespaces and parameters. Each top-\\nlevel map key represents an Axon namespace. Most models are namespaced\\nby layers. In this example, you’ll see that there’s a key for each layer\\ndense_0 and dense_1, and within each namespace, there’s a key for each layer\\nparameter kernel and bias.\\nEvaluating the Model\\nRemember, in machine learning, you’re only concerned with your model’s\\nperformance on an unseen test set. 98% accuracy on the training set isn’t\\nindicative of your model’s true performance. Fortunately, Axon also offers\\nconveniences for evaluating models against test data. To evaluate your\\nmodel, implement the following supervised evaluation loop:\\nmodel\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 291}),\n",
       " Document(page_content='|> Axon.Loop.run(test_data, trained_model_state,  \\ncompiler:  EXLA)\\nAxon.Loop.evaluator/2 is a factory function, which creates a supervised\\nevaluation loop using an Axon model and a model state. The evaluator tells\\nAxon you want to evaluate model at the state trained_model_state. You also\\nneed to tell Axon what metrics you want to evaluate your model with. In\\nthis case, you attach accuracy to the loop, so Axon knows to track accuracy.\\nFinally, you need to tell Axon to execute the loop on your test_data.\\nAfter running the code, you’ll see this:\\nBatch: 156, accuracy: 0.9881768\\nThis indicates your model correctly predicts around 99% of digits in the test\\ndata. That’s pretty good. Of course, the purpose of a model is to actually\\nmake predictions on new data, so how can you do that with Axon?\\nExecuting Models with Axon\\nAxon offers an execution API for compiling and running Axon models.\\nStart by grabbing an individual batch of images from your test_data:\\n{test_batch, _} = Enum.at(test_data, 0)\\nTo make things easier, take a single image from the test batch:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 292}),\n",
       " Document(page_content='test_image = test_batch[0]\\nYou can visualize the image using Nx.to_heatmap:\\ntest_image\\n|> Nx.reshape({28, 28})\\n|> Nx.to_heatmap()\\nYou’ll see something that looks like the image.\\nIn this example, the test image is a 3, so the model should predict 3 as the\\nmost probable label.\\nYou now have everything you need to make predictions: a model, a\\ncompatible model state, and an input. The easiest way to query your model\\nfor predictions is to first build your model using Axon.build/2 and then call\\nthe returned predict function. Axon.build/2 converts your model into a tuple', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 293}),\n",
       " Document(page_content='of {init_fn, predict_fn}. init_fn is an arity-2 function that can be used to\\ninitialize your model’s parameters. predict_fn is an arity-2 function that takes\\nmodel parameters and a tensor or collection of tensors as input and returns\\nthe result of running the full model.\\nRun the following code to try and query your model for predictions:\\n{_, predict_fn} = Axon.build(model,  compiler:  \\nEXLA)\\npredict_fn.(trained_model_state, test_image)\\nYou’ll see the following error message:\\n** (Axon.CompileError) exception found  when  \\ncompiling layer\\n   Axon.Layers.dense/4 named  dense_0: \\n  ** (ArgumentError) Axon.Layers. dense:  expected \\ninput shape to have at\\n   least rank 2, got rank 1\\n   (axon 0.5.1) lib/axon/shared. ex: 92: \\nAxon.Shared.assert_min_rank!/4\\n   (nx 0.5.1) lib/nx/defn/compiler. ex: 203: \\nNx.Defn.Compiler.__remote__/4', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 294}),\n",
       " Document(page_content='  (axon 0.5.1) lib/axon/layers. ex: 125: \\nAxon.Layers. \"  __defn:dense_impl__\" /4\\nWhat’s going on here? Remember Axon requires input shapes to be\\nsomewhat static and to match the form you specified during model creation.\\nIn this case, your test_image is missing an additional dimension, so you can\\nadd one in using Nx.new_axis:\\nprobabilities =\\n  test_image\\n  |> Nx.new_axis(0)\\n  |> then(&predict_fn.(trained_model_state, &1))\\nYou’ll see an output that looks like the following:\\n#Nx.Tensor< \\n  f32[1][10]\\n  EXLA.Backend< host: 0, \\n0.4078460682.4224843782.56655>\\n  [\\n    [6.16645411355421e-5, 0.011110336519777775, \\n0.07432620227336884, ...]\\n  ]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 295}),\n",
       " Document(page_content='Remember, your model outputs ten probabilities associated with the labels\\n0 to 9. You can get the discrete label from these probabilities by computing\\nthe Nx.argmax of the probabilities like so:\\nprobabilities |> Nx.argmax()\\nThis gives you the following output:\\n#Nx.Tensor< \\n  s64\\n  EXLA.Backend< host: 0, \\n0.4078460682.4224843782.56658>\\n  3\\n>\\nWhat you see is the exact output you were looking for. Congratulations, you\\nnow have a model, a trained model state, and the ability to query the trained\\nmodel using any inputs you’d like.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 296}),\n",
       " Document(page_content='[11]\\n[12]\\n[13]\\nWrapping Up\\nIn this chapter, you were introduced to deep learning and why it’s much\\nmore effective than other approaches to certain classes of problems. You\\nbroke down the anatomy of a neural network and implemented one of the\\nmost common types of neural networks: the basic feed-forward or dense\\nnetwork. You used Axon to create, train, and evaluate neural networks.\\nWhile feed-forward networks can be used to solve a number of complex\\nproblems, there are certain classes of problems where feed-forward\\nnetworks are not enough. In the next few chapters, you’ll solve problems in\\ndifferent areas where feed-forward networks come up short. You’ll also\\nimplement different types of neural networks that overcome the limitations\\nof basic feed-forward networks on specific classes of inputs.\\nFOOTNOTES\\nhttps://github.com/elixir-nx/axon\\nhttp://yann.lecun.com/exdb/mnist/\\nhttps://github.com/onnx/onnx\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 297}),\n",
       " Document(page_content='Chapter 7\\nLearn to See\\n\\xa0\\nIn the previous chapter, you used Axon to create and train neural networks\\nin Elixir, including one that recognizes handwritten digits. You also learned\\nabout the types of problems neural networks are well-suited to handle and\\nwhy they outperform shallow machine learning approaches in many areas.\\nMore specifically, you implemented a type of neural network architecture\\nknown as the multi-layer perceptron (MLP) or deep feed-forward network.\\nWhile MLPs are capable of learning and modeling any kind of data, other\\ndeep learning architectures are even more powerful when applied to\\nspecific problem types. In this chapter, you’ll implement domain-specific\\narchitecture known as the convolutional neural network (CNN) and learn\\nhow it outperforms traditional MLPs in computer vision.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 298}),\n",
       " Document(page_content='Identifying Cats and Dogs\\nImagine you run a social media website where users can post and view\\npictures of animals. To simplify the site as much as possible, you don’t want\\nusers to attach any additional information to these images, such as whether\\nor not the image is that of a dog or a cat. However, you do want users to be\\nable to filter their feeds based on such criteria. So, how can you accomplish\\nthis without requiring users to tag their images with a specific category?\\nYou guessed it: machine learning.\\nIn this example, you’ll train a model to classify images into one of two\\ncategories: cats or dogs. While you can easily extend the model to\\ndistinguish between any kind of animal, by reducing the number to two,\\nyou reduce the amount of data required to train the model—and in turn, the\\nlength of time to train that model.\\nBefore you dive in, head over to Kaggle and grab the Cats vs. Dogs\\ntraining dataset. This dataset contains a directory of 25,000 images—12,500\\nimages of cats and 12,500 images of dogs—nicely wrapped up in a single\\nzip file. After you download the dataset, extract it to a train directory in a\\nworking directory of your choosing.\\nWith the dataset downloaded and extracted, you’re ready to fire up a\\nLivebook and start training the model.\\nInstalling Dependencies\\n[14]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 299}),\n",
       " Document(page_content='First, you need to bring some dependencies into your notebook\\nenvironment. Run the following code to install the dependencies needed to\\ncreate an image classification model:\\nMix.install([\\n  { :axon ,  \"  ~> 0.5\" },\\n  { :nx ,  \"  ~> 0.5\" },\\n  { :exla ,  \"  ~> 0.5\" },\\n  { :stb_image ,  \"  ~> 0.6\" },\\n  { :kino ,  \"  ~> 0.8\" }\\n])\\nBy now, you should be familiar with all of the dependencies listed here.\\nWith your dependencies set up, it’s time to set some configuration defaults\\nfor Nx and EXLA. Run the following line to configure EXLA as the default\\nNx backend:\\nNx.global_default_backend(EXLA.Backend)\\nThat’s all you need to do to configure your notebook environment for this\\nexample. The next step is to create a pipeline to load images from a\\ndirectory into batches of tensors.\\nBuilding an Input Pipeline', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 300}),\n",
       " Document(page_content='As you saw in  Training the Model, to train an Axon model, you need to\\ncreate a pipeline of input data for Axon to iterate through. While Axon can\\nrun a loop over any Enumerable, such as a list, there are often significant\\nadvantages to using lazy data structures, such as streams. Streams are lazy\\nenumerables that generate elements one by one. Compared to an eager data\\nstructure (for example, a list), streams are often more performant as training\\ninput pipelines, especially when using an accelerator, such as a GPU. In\\npractice, streams offer the following advantages over lists in a training\\npipeline: memory efficiency and overlapping execution.\\nMemory Efficiency\\nNeural networks are extremely data-hungry. Practical datasets are often too\\nlarge to fit entirely in memory. For example, the popular ImageNet\\ndataset is over 150GB in size. If you were using a list as your training\\npipeline, you would need to load all 150GB of that dataset into memory at\\nonce. Doing so is both impractical and inefficient. Streams only yield\\nresults when requested, which means you can consume batches of images\\none by one and avoid loading an entire dataset into memory.\\nOverlapping Execution\\nWhen using an external accelerator, such as a GPU, for training, the CPU is\\noften idle for long periods of time as its only responsibility is feeding inputs\\nto the GPU. Also, GPUs are so fast that data transfer is often the most\\nexpensive operation. For this reason, it’s a good idea to run training and\\n[15]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 301}),\n",
       " Document(page_content='data loading concurrently to avoid starving the GPU. GPU starvation\\nhappens when the input pipeline is IO-bound rather than compute-bound.\\nThe biggest bottleneck is the GPU waiting for data, not the actual training\\ncomputations.\\nFortunately, you can combine streams with some of Elixir’s concurrency\\nprimitives to create pipelines that maximize both the GPU and CPU usage.\\nFor example, you can make use of Elixir’s Task.async_stream/3 to create a\\nstream that processes inputs in parallel. This is especially useful when\\nperforming IO operations, such as reading files. You can build up\\npreprocessing pipelines using stream operations, and Axon will consume\\nthe stream as batches become ready—overlapping input IO, preprocessing,\\nand computation—maximizing the computing power available to you.\\nIn this example, you’ll create a lazy, parallel input pipeline using a stream.\\nStart by creating a new module named CatsAndDogs:\\ndefmodule  CatsAndDogs  do \\nend \\nInside the CatsAndDogs module, create a function named pipeline/1 that\\naccepts a list of input paths from which to create a pipeline:\\ndef  pipeline(paths)  do \\n  paths\\n  |> Enum.shuffle()', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 302}),\n",
       " Document(page_content='end \\nFor now, this function simply shuffles a list of paths, which is an important\\nstep. You need to shuffle the dataset to avoid feeding your training\\nalgorithm only pictures of cats or only pictures of dogs for extended periods\\nof time.\\nNow, run the following code outside of the declared module to test what\\nyou have so far:\\ntrain_path = Path.wildcard( \"  train/*.jpg\" )\\nCatsAndDogs.pipeline(train_path)\\nAnd you’ll see the following output:\\n[\"train/cat.11662.jpg\", \"train/dog.9221.jpg\", \\n\"train/dog.10370.jpg\",\\n  \"train/cat.3705.jpg\", \"train/dog.3366.jpg\", ...]\\n\\xa0\\nAt this point in the pipeline, you have a list of image paths that you need to\\nparse into tensors. You can accomplish this using StbImage. Start by\\ncreating a new private function in the CatsAndDogs module named\\nparse_image/1:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 303}),\n",
       " Document(page_content='defp  parse_image(path)  do \\n  label =  if  String.contains?(path,  \"  cat\" ),  do : 0, \\nelse : 1\\n   case  StbImage.read_file(path)  do \\n    { :ok , img} -> {img, label}\\n    _error ->  :error \\n   end \\nend \\nThis function parses paths into a tuple of {img, label}, where img is an\\n%StbImage{} object and label is a 0 when the image is a cat or a 1 when the\\nimage is a dog. The label is determined directly from the file path. Notice\\nthat this function handles the possibility that some images are corrupt by\\nreturning :error atoms when StbImage encounters a file it can’t successfully\\nread. You can discard these errors later by applying a filter.\\nNext, you need to add this parsing to your pipeline. At this point, you could\\napply parse_image/1 with Enum.map/2, however, your pipeline should be both\\nconcurrent and lazy. To create a stream of parallel processes, you can use\\nTask.async_stream/3, which returns a stream that runs a function concurrently\\non each element that it’s given. You can add Task.async_stream/3 directly to\\nyour pipeline, like this:\\ndef  pipeline(paths)  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 304}),\n",
       " Document(page_content=' paths\\n  |> Enum.shuffle()\\n  |> Task.async_stream(&parse_image/1)\\nend \\nNow, run the following code:\\ntrain_path = Path.wildcard( \"  train/*.jpg\" )\\nCatsAndDogs.pipeline(train_path)\\nAnd you’ll see this:\\n#Function<3.23692026/2 in Task.build_stream/3>\\nNotice that the pipeline doesn’t yield any results; it simply returns a\\nfunction that builds a stream data structure. To yield some results, you need\\nto consume a part of the stream. Try consuming part of the pipeline by\\nrunning the following code:\\ntrain_path = Path.wildcard( \"  train/*.jpg\" )\\ntrain_pipeline = CatsAndDogs.pipeline(train_path)\\nEnum.take(train_pipeline, 5)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 305}),\n",
       " Document(page_content='And you’ll see something similar to this:\\n[\\n  ok: {%StbImage{\\n     data: <<127, 136, 133, 126, 135, 132, 126, \\n...>>,\\n     shape: {300, 399, 3},\\n     type: {:u, 8}\\n   }, 1},\\n  ok: {%StbImage{\\n     data: <<4, 4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 2, \\n...>>,\\n     shape: {373, 500, 3},\\n     type: {:u, 8}\\n   }, 1},\\n  ...\\n]\\nNotice that Task.async_stream/3 wraps each result in an {:ok, result} tuple. This\\nresult is important to keep in mind as you add more steps to your pipeline\\nbecause you’ll need to handle success cases ({:ok, result}) and error cases\\n({:error, reason}) later on.\\nAt this point, you have tuples of StbImage structs and integer labels. For\\ntraining, Axon requires that your inputs are tuples of batched tensors. To get', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 306}),\n",
       " Document(page_content='there, you need to implement some basic parsing for converting StbImage\\nstructs to tensors:\\ndefp  to_tensors({ :ok , {img, label}}, \\ntarget_height, target_width)  do \\n  img_tensor =\\n    img\\n    |> StbImage.resize(target_height, \\ntarget_width)\\n    |> StbImage.to_nx()\\n    |> Nx.divide(255)\\n  label_tensor = Nx.tensor([label])\\n  {img_tensor, label_tensor}\\nend \\nThis function accepts a successful result from Task.async_stream/3, resizes\\neach image to a uniform height and width, converts the image to a tensor,\\nand scales the input pixel values to be between 0 and 1.\\nRecall from Chapter 2,  Get Comfortable with Nx , image tensors commonly\\norder data in dimensions with color channels as the first or last dimension,\\nwhich are denoted with channels first and channels last representations,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 307}),\n",
       " Document(page_content='respectively. StbImage yields tensors with channels last configurations, so\\nthere’s no need to transpose the input data.\\nThe to_tensor/3 function also converts integer labels to a tensor by wrapping\\nthe integer in a tensor before returning a tuple of {img_tensor, label_tensor}.\\nOf course, it’s entirely possible that parse_image/1 didn’t successfully load\\nan image from a file. To prevent these errors from cascading down to\\nto_tensors/3, you need to filter them out before applying to_tensors/3. You\\nalso need to add the target_height and target_width arguments to your\\npipeline, like this:\\ndef  pipeline(paths, target_height, target_width)  \\ndo \\n  paths\\n  |> Enum.shuffle()\\n  |> Task.async_stream(&parse_image/1)\\n  |> Stream.filter( fn \\n    { :ok , {%StbImage{}, _}} -> true\\n    _ -> false\\n   end )\\n  |> Stream.map(&to_tensors(&1, target_height, \\ntarget_width))\\nend ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 308}),\n",
       " Document(page_content='Notice that your filter function pattern matches on results that return {:ok,\\nresult} and discards everything else. Additionally, applying to_tensors/3 is as\\neasy as using Stream.map/2. Run the following code to inspect what your\\npipeline looks like at this point:\\ntrain_path = Path.wildcard( \"  train/*.jpg\" )\\ntarget_height = 96\\ntarget_width = 96\\ntrain_pipeline = CatsAndDogs.pipeline(\\n  train_path, target_height, target_width\\n)\\nEnum.take(train_pipeline, 5)\\nNotice you have to specify the additional target_height and target_width\\narguments. The choice of target_height and target_width is arbitrary. Just\\nremember that lower resolutions encode less information than higher\\nresolutions and might be more difficult for your neural network to train on,\\nwhereas higher resolutions require more processing power. After running\\nthis code, you’ll see something similar to this:\\n[\\n  {#Nx.Tensor<\\n     f32[channels: 3][height: 96][width: 96]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 309}),\n",
       " Document(page_content='    EXLA.Backend<cuda:0, \\n0.2231265192.3745644570.217183>\\n     [\\n       [\\n         [0.5490195751190186, 0.5647058486938477, \\n0.5568627119064331, ...],\\n         ...\\n       ],\\n       ...\\n     ]\\n   >,\\n   #Nx.Tensor<\\n     s64[1]\\n     EXLA.Backend<cuda:0, \\n0.2231265192.3745644570.217184>\\n     [0]\\n   >},\\n  ...\\n]\\nYour pipeline is almost finished. At this point, you have entries that are\\ntuples of single image and label pairs. Rather than processing single images\\nat once, it’s often more efficient to process batches of images. Fortunately,\\nyou can implement batching relatively easily by chunking the input stream', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 310}),\n",
       " Document(page_content='and applying a transformation to the chunks. Modify your pipeline to match\\nthe following:\\ndef  pipeline(paths, batch_size, target_height, \\ntarget_width)  do \\n  paths\\n  |> Enum.shuffle()\\n  |> Task.async_stream(&parse_image/1)\\n  |> Stream.filter( fn \\n    { :ok , {%StbImage{}, _}} -> true\\n    _ -> false\\n   end )\\n  |> Stream.map(&to_tensors(&1, target_height, \\ntarget_width))\\n  |> Stream.chunk_every(batch_size, batch_size,  \\n:discard )\\n  |> Stream.map( fn  chunks ->\\n    {img_chunk, label_chunk} = Enum.unzip(chunks)\\n    {Nx.stack(img_chunk), Nx.stack(label_chunk)}\\n   end )\\nend \\nYour pipeline now takes a batch_size argument, which controls the batch size\\nof yielded entries in the stream. To generate batches, you chunk results\\nusing Stream.chunk_every/4 with a chunk size equal to the batch size, which', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 311}),\n",
       " Document(page_content='yields a list of batch_size entries. You discard leftover images so you’re\\nnever left with uneven batches. You then add a transformation, which uses\\nEnum.unzip/1 to unzip the list of {img, label} tuples into two lists of tensors.\\nFinally, you pass these lists to Nx.stack/1 to stack the list of tensors into a\\nsingle tensor.\\nNow, run the following code to verify your pipeline is working as expected:\\ntrain_path =\\n  Path.wildcard( \"  train/*.jpg\" )\\n  |> Enum.shuffle()\\nbatch_size = 128\\ntarget_height = 96\\ntarget_width = 96\\ntrain_pipeline = CatsAndDogs.pipeline(\\n  train_path, batch_size, target_height, \\ntarget_width\\n)\\nEnum.take(train_pipeline, 1)\\nAnd you’ll see this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 312}),\n",
       " Document(page_content='[\\n  {#Nx.Tensor<\\n     f32[128][channels: 3][height: 96][width: 96]\\n     EXLA.Backend<cuda:0, \\n0.3445614888.3462266912.105686>\\n     [\\n       [\\n         [\\n           [0.9999999403953552, \\n0.9999999403953552, ...],\\n           ...\\n         ],\\n         ...\\n       ],\\n       ...\\n     ]\\n   >,\\n   #Nx.Tensor<\\n     s64[128][1]\\n     EXLA.Backend<cuda:0, \\n0.3445614888.3462266912.105719>\\n     [\\n       [1],\\n       [0],', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 313}),\n",
       " Document(page_content='      [0],\\n       [1],\\n       [0],\\n       [0],\\n       ...\\n     ]\\n   >}\\n]\\nFinally, you’ll need to create a test set to test your results on. You can do\\nthis by shuffling the input paths and then splitting the train set:\\n{test_paths, train_paths} =\\n  Path.wildcard( \"  train/*.jpg\" )\\n  |> Enum.shuffle()\\n  |> Enum.split(1000)\\nbatch_size = 128\\ntarget_height = 96\\ntarget_width = 96\\ntrain_pipeline = CatsAndDogs.pipeline(\\n  train_paths, batch_size, target_height, \\ntarget_width', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 314}),\n",
       " Document(page_content=')\\ntest_pipeline = CatsAndDogs.pipeline(\\n  test_paths, batch_size, target_height, \\ntarget_width\\n)\\nEnum.take(train_pipeline, 1)\\nCongratulations, the hardest part is over! You’ve successfully created an\\nimage input pipeline, and you’re ready to start training a neural network.\\nTrying to See with MLPs\\nIn  Building the Model , you created the simplest kind of neural network to\\nrecognize handwritten digits—a multi-layer perceptron (MLP) or dense\\nfeed-forward network. Your first instinct might be to try to apply an MLP to\\nthis problem—after all, it’s just another image classification task, right?\\nAs you’ll see in this chapter, MLPs are not always the best choice for\\ndealing with image data—but you’ll still train one anyway so that you can\\nestablish a baseline level of performance.\\nStart by running the following code to create a new MLP model:\\nmlp_model =', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 315}),\n",
       " Document(page_content=' Axon.input( \"  images\" ,  shape:  {nil, target_height, \\ntarget_width, 3})\\n  |> Axon.flatten()\\n  |> Axon.dense(256,  activation:   :relu )\\n  |> Axon.dense(128,  activation:   :relu )\\n  |> Axon.dense(1,  activation:   :sigmoid )\\nForking Cells\\nThe models you’ll create and train in the next few sections are\\nindependent. To avoid adding an unnecessary dependency\\nbetween individual training runs, you can tell Livebook that\\nthese sections only depend on the data section you defined\\npreviously. To do so, simply create a new Livebook section,\\nand then specify that the section is forked from the section\\nthat contains your training pipeline.\\nYou’re welcome to add more hidden layers, adjust the hyperparameters of\\nthe hidden layers, and change the hidden activation. However, don’t change\\nthe input or output layers.\\nNotice the input shape in this case is a tensor of {nil, target_height,\\ntarget_width, 3}, where nil is the batch size and 3 is the number of color\\nchannels. The output layer is a dense layer with one unit and a :sigmoid', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 316}),\n",
       " Document(page_content='activation function. Determining whether an image is a cat or a dog is a\\nbinary classification problem, which means you’re trying to map inputs to\\none of two classes. Your output layer will return a single probability\\nbetween 0 and 1, where probabilities closer to 0 map to the label cat and\\nprobabilities closer to 1 map to the label dog.\\nYou should also notice the new layer Axon.ﬂatten, which takes a two- or\\nmore-dimensional input and flattens the trailing dimensions into a single\\ndimension. In this case, you want to pass a two-dimensional tensor to a\\ndense layer so the flatten layer combines the channel, height, and width\\ndimensions into a single dimension.\\nNow that you have data and a model, you need to implement the loop. The\\ntraining loop for this problem should look similar to the one from Chapter\\n6,  Go Deep with Axon . Copy the following code to create and run a training\\nloop for your MLP model:\\nmlp_trained_model_state =\\n  mlp_model\\n  |> Axon.Loop.trainer( :binary_cross_entropy ,  \\n:adam )\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.run(train_pipeline, %{},  epochs:  5, \\ncompiler:  EXLA)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 317}),\n",
       " Document(page_content='If you compare this training loop to the one from Chapter 6,  Go Deep with\\nAxon , you’ll notice two slight differences:\\nThis training loop uses :binary_cross_entropy rather than\\n:categorical_cross_entropy. Because this is a binary classification problem,\\n:binary_cross_entropy is the appropriate loss function to use here.\\nThis training loop uses :adam as an optimizer rather than :sgd. :adam is a\\nshortcut to tell Axon to use the Adam optimizer. Adam is a gradient-\\ndescent-based algorithm that makes slight adaptations to traditional\\ngradient descent to improve convergence. Adam is an algorithm that may\\nimprove the training time of your neural network when compared to\\nother optimizers.\\nThe rest of this loop is identical to the one from Chapter 6, Go Deep with\\nAxon . You track :accuracy during training, pass your pipeline as input data,\\nand train for five epochs—JIT-compiling train steps with the EXLA\\ncompiler.\\nAfter some time, you’ll see this:\\nEpoch: 0, Batch: 150, accuracy: 0.5474960 loss: \\n1.0835106\\nEpoch: 1, Batch: 150, accuracy: 0.5723824 loss: \\n0.8659902', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 318}),\n",
       " Document(page_content='Epoch: 2, Batch: 150, accuracy: 0.6005284 loss: \\n0.7973230\\nEpoch: 3, Batch: 150, accuracy: 0.6123245 loss: \\n0.7604570\\nEpoch: 4, Batch: 150, accuracy: 0.6156876 loss: \\n0.7374102\\nAlthough your model improved after every epoch, it quickly plateaued at\\naround 60% training accuracy. You may not care about training accuracy,\\nbut the inability of a model to perform well on training data is a tell-tale\\nsign of underfitting, as you learned in  Overfitting, Underfitting, and\\nCapacity . To see how well (or poorly), your model did, run the following\\ncode:\\nmlp_model\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )\\n|> Axon.Loop.run(test_pipeline, \\nmlp_trained_model_state,  compiler:  EXLA)\\nEventually, you’ll see this:\\nBatch: 0, accuracy: 0.5333334', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 319}),\n",
       " Document(page_content='What’s going on here? Your model is barely doing better than random\\nguessing. You might be tempted to go back and increase the capacity of\\nyour model by adding more layers or increasing the size of the existing\\nhidden layers. You could spend hours on hyperparameter tuning, adding\\nmore layers and increasing the capacity of your model. While that might\\nimprove performance, increasing the size of the model isn’t always the\\nanswer. Sometimes you need to change your approach or (in this case)\\nchange your model.\\nScaling Laws\\nRecent advances in deep learning have suggested that adding\\nmore capacity to a model might have a direct positive\\ncorrelation to model performance. Some of the large language\\nmodels out of Google, OpenAI, and Facebook have hundreds\\nof billions of parameters. Does this imply that scaling any old\\nmodel will result in better performance? No, it implies that\\nscaling the right model will lead to better performance.\\n\\xa0', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 320}),\n",
       " Document(page_content='Introducing Convolutional Neural Networks\\nMLPs are powerful enough to do well on many different problems, but\\nthey’re not always the best choice. In the previous section, you saw how a\\nbasic MLP struggled to outperform random guessing when identifying\\nimages of cats and dogs. In this section, you’ll see how you can do much\\nbetter than an MLP by introducing a new type of neural network: the\\nconvolutional neural network (CNN).\\nConvolutional neural networks are neural networks that replace traditional\\nmatrix multiplications in dense layers with convolution operations. If you\\nhave an engineering background, you’re likely familiar with the\\nconvolution operation. Although the convolution operation used in neural\\nnetworks is similar to the definition of a convolution you’d get from a pure\\nmathematician or signals engineer, it differs slightly in some ways.\\nImagine you have a two-dimensional input, such as an image without any\\ncolor depth. You can represent this image as a grid, where each grid square\\nis a pixel:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 321}),\n",
       " Document(page_content='Now imagine you also have a two-dimensional kernel, which is a smaller\\ngrid:\\nYou can “slide” this kernel over each valid window in the input and map the\\ninput to a feature map using a relatively simple weighted sum operation. A\\nsingle grid square in your new feature map is equal to the weighted sum of\\noverlapping grid squares between the input grid and the kernel grid:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 322}),\n",
       " Document(page_content='You can repeat this process for every valid, or full, window in the input\\ngrid. To traverse the grid, you move one grid square in the X direction until\\nyou reach the end, and then move one grid square in the Y direction,\\nrepeating this process until you’ve traversed the entire image. The size of\\neach step you take is often referred to as the stride of the convolution. In the\\nend, you’ll have a fully transformed feature map.\\nSo, what’s the point of doing all of this? Well, if there’s anything you\\nshould take away from this book, it’s that a few seemingly simple matrix\\noperations can produce magical results. You can think of your kernel as a\\nfilter. With some clever mathematics, you can design filters by hand that are\\ncapable of extracting useful representations from an input image. For\\nexample, try running the following code:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 323}),\n",
       " Document(page_content='path =  \"  train/dog.5.jpg\" \\nimg =\\n  path\\n  |> StbImage.read_file!()\\n  |> StbImage.to_nx()\\n  |> Nx.transpose( axes:  [ :channels ,  :height ,  \\n:width ])\\n  |> Nx.new_axis(0)\\nkernel = Nx.tensor([\\n  [-1, 0, 1],\\n  [-1, 0, 1],\\n  [-1, 0, 1]\\n])\\nkernel = kernel |> Nx.reshape({1, 1, 3, 3}) |> \\nNx.broadcast({3, 3, 3, 3})\\nimg\\n|> Nx.conv(kernel)\\n|> Nx.as_type({ :u , 8})\\n|> Nx.squeeze( axes:  [0])\\n|> Nx.transpose( axes:  [ :height ,  :width ,  :channels \\n])', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 324}),\n",
       " Document(page_content='|> Kino.Image.new()\\nAnd you’ll see the following output:\\nThis code uses Nx.conv to implement a basic edge detector. In this instance,\\nyou can see how edges might be a useful set of features for a model to\\nknow. You can probably sit around and hand-engineer a number of feature\\ndetectors using convolutional operations by hand, but that would defeat the\\npurpose of the machine learning process.\\nRemember, the power of deep learning is in its ability to learn useful\\nrepresentations for you. In a convolutional layer, you start with a randomly\\ninitialized kernel, and during training, the kernel starts to converge toward a\\nparameterization capable of extracting useful features from the input. The\\nkey insight is that you can learn filters rather than engineer them by hand.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 325}),\n",
       " Document(page_content='A real convolutional kernel for image data will have four dimensions. It will\\nhave two spatial dimensions that map directly to the X and Y coordinates of\\nan input image. It will also have a depth or input channel dimension that\\ncorresponds to the color depth of the input image. In addition, it will have a\\nnumber of output filters. Rather than learn a single filter per kernel, it’s\\nuseful to learn multiple filters or multiple transformations of the input\\nimage. One filter can learn to extract edges, another can learn to extract\\ncorners, and so on. Typically, the output depth is considered the\\ndimensionality of the convolutional layer.\\nConvolutional layers are powerful feature extractors, especially on data\\nsuch as images, which have a natural grid-like structure. The convolutional\\nlayer is the fundamental unit of computation in a CNN, but there are a few\\nother key pieces that increase the representational capacity of CNNs.\\nThe Anatomy of a CNN\\nTechnically speaking, a convolutional neural network is any type of neural\\nnetwork that contains one or more convolutional layers. However, most\\nCNNs share a similar structure. They consist of one or more blocks or\\nstages with a convolutional layer, an activation function, and a pooling\\noperation. These blocks make up a convolutional base. The convolutional\\nbase is typically followed by a fully-connected network that takes learned\\nfeatures from the convolutional base and maps them to an output label.\\nLet’s have a closer look at each of these portions and what they do.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 326}),\n",
       " Document(page_content='Convolutional Layers\\nYou’ve already learned the ins and outs of the convolution operation and\\nhow convolutions extend to neural networks. A typical block in a\\nconvolutional base starts with a convolutional layer that performs a linear\\noperation and forwards activations to a nonlinear activation function.\\nActivation Functions\\nAs with feed-forward networks, you need to apply nonlinearities after a\\nconvolutional layer to unlock the true power of deep learning. Activation\\nfunctions in the context of CNNs can be thought of as detectors [GBC16]—\\nthey fire on certain features. For example, in a CNN, one layer’s activations\\nmight fire significantly on the presence of edges in an image, while another\\nlayer’s activations might fire on the presence of certain colors.\\nPooling Layers\\nPooling layers aggregate spatial information in windows using some\\naggregate operation. Similar to convolutions, pooling layers slide a grid\\nover an input tensor and compute an output tensor using the result of an\\naggregate operation for each valid spatial window in the tensor. Pooling\\noperations share many of the same hyperparameters that control the size\\nand traversal of a convolutional kernel, including kernel size, padding, and\\nstrides. Pooling layers, however, don’t include a learned kernel. The intent', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 327}),\n",
       " Document(page_content='of a pooling layer is to reduce the dimensionality of an input tensor while\\nretaining some amount of useful information to be used in later layers.\\nMax pooling and average pooling are two of the most common pooling\\noperations used in CNNs. Max pooling maps an input tensor to an output\\ntensor by computing the max of every spatial window in the input. Average\\npooling maps an input tensor to an output tensor by computing the average\\nof every spatial window in the input.\\nFully-Connected Head\\nCNNs are commonly used as feature extractors. Remember that the\\nstrength of deep learning is in its ability to learn useful representations of\\ninputs. Convolutional bases are really good at extracting features from\\ncertain types of data such as images. However, to make use of those\\nfeatures for practical problems, you typically need some type of fully-\\nconnected head to map features to labels. The head of a model is simply the\\nfinal layer of the model. Most CNNs have a flatten or global pooling layer\\nthat aggregates the learned convolutional features into a form suitable for a\\ndense network. As you’ll see in Chapter 8,  Stop Reinventing the Wheel , the\\ndistinction between a feature extractor and a fully-connected classifier\\nallows you to engineer neural networks that take advantage of the\\nknowledge of more powerful models.\\nImplementing CNNs with Axon', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 328}),\n",
       " Document(page_content='At this point, you might be a bit overwhelmed trying to grok concepts like\\nconvolutions, padding, and pooling. It’s okay if you don’t fully understand\\nthese concepts yet—you don’t need to have a deep understanding to take\\nadvantage of them in Axon.\\nThe convoluted concepts of convolutions map to relatively simple APIs in\\nAxon. To get the hang of how different operations in a CNN interact with\\nan input, start by declaring a variable cnn_model that consists of a single\\nAxon input layer:\\ncnn_model = Axon.input( \"  images\" ,  shape:  {nil, 96, \\n96, 3})\\nYou can visualize the model by using Axon.Display.as_graph/2:\\ntemplate = Nx.template({1, 96, 96, 3},  :f32 )\\nAxon.Display.as_graph(cnn_model, template)\\nAfter running the code, you’ll see this:\\nAxon.conv/3 is Axon’s API for adding convolutional layers to a neural\\nnetwork. This API accepts an input Axon graph, a number of output filters,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 329}),\n",
       " Document(page_content='and a list of options that control the convolution. To see it in action, rebind\\nthe variable cnn_model to the following:\\ncnn_model =\\n  cnn_model\\n  |> Axon.conv(32,\\n     kernel_size:  {3, 3},\\n     padding:   :same ,\\n     activation:   :relu \\n  )\\nThe arguments to Axon.conv/3 map directly to how the convolution operation\\ntransforms the input tensor. The first argument after the input graph\\nrepresents the number of output filters this layer should learn, in this case,\\n32. The :kernel_size option controls the spatial size of the kernel. If you think\\nof the spatial dimensions of the input as a matrix, the input tensor is a\\n96x96 matrix, while the kernel is a 3x3 matrix.\\nPadding controls how the input is altered before the convolution happens.\\nIn a valid or full convolution, the output feature map only has outputs for\\nvalid windows in the input tensor. The spatial size of the feature map will\\nbe smaller than the input. You can ensure the output of the convolution has\\nthe same size as the input by using :same padding.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 330}),\n",
       " Document(page_content='Same padding tells the convolution to pad the input tensor’s spatial\\ndimensions such that the spatial size doesn’t change. Padding simply adds\\nnew grid squares around the original input grid so the output has the same\\nsize as the original input.\\nNow, display the new model:\\nAxon.Display.as_graph(cnn_model, template)\\nAnd you’ll see this graph:\\nDo you notice how your model has changed? The graph now consists of an\\ninput, followed by a convolution, and then an activation function. What\\nabout the shapes? Notice the number of color channels or depth of the input\\ntensor has gone from 3 to 32. Also, notice that the size of your spatial\\ndimensions remained the same.\\nNext, rebind the variable cnn_model to the following:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 331}),\n",
       " Document(page_content='cnn_model =\\n  cnn_model\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\nAnd display the model:\\nAxon.Display.as_graph(cnn_model, template)\\nAfter running it, you’ll see this graph:\\nOnce again, notice how your model changed. The graph has an additional\\nmax pooling layer, and the output height and width both are halved. You\\ncan repeat this process with more convolutional stages, and once again,\\nobserve how the model changes:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 332}),\n",
       " Document(page_content='cnn_model =\\n  cnn_model\\n  |> Axon.conv(64,  kernel_size:  {3, 3},  \\nactivation:   :relu ,  padding:   :same )\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\n  |> Axon.conv(128,  kernel_size:  {3, 3},  \\nactivation:   :relu ,  padding:   :same )\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\nNow, use Axon.ﬂatten to flatten the features and implement the fully-\\nconnected head:\\ncnn_model =\\n  cnn_model\\n  |> Axon.flatten()\\n  |> Axon.dense(128,  activation:   :relu )\\n  |> Axon.dense(1,  activation:   :sigmoid )\\nOverall, the entire model architecture is represented in the following code:\\ncnn_model =\\n  Axon.input( \"  images\" ,  shape:  {nil, 96, 96, 3})', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 333}),\n",
       " Document(page_content=' |> Axon.conv(32,  kernel_size:  {3, 3},  \\nactivation:   :relu ,  padding:   :same )\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\n  |> Axon.conv(64,  kernel_size:  {3, 3},  \\nactivation:   :relu ,  padding:   :same )\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\n  |> Axon.conv(128,  kernel_size:  {3, 3},  \\nactivation:   :relu ,  padding:   :same )\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\n  |> Axon.flatten()\\n  |> Axon.dense(128,  activation:   :relu )\\n  |> Axon.dense(1,  activation:   :sigmoid )\\nIf you display this model as a table, you’ll notice a few things.\\nOne difference you’ll notice between this model and your MLP model is\\nthat this model is significantly smaller in terms of the number of parameters\\nand parameter size, even though the number of layers in your model has\\nincreased. CNNs take advantage of sparsity—a phenomenon you’ll learn\\nmore about in  Why CNNs Work . You might be worried that this decrease in\\nthe number of trainable parameters will result in worse performance.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 334}),\n",
       " Document(page_content='However, it’s important to know that bigger is only better if you’ve chosen\\nthe right tool for the job. A sledgehammer won’t do better than a claw\\nhammer at tightening a screw.\\nNow that you’ve finished building a CNN with Axon, you need to train it.\\nYou might be wondering if the change in architecture necessitates a change\\nin the training loop. In some cases, changing the model requires changes to\\nthe training loop. However, in this case, you can copy the same training\\nloop you used in  Trying to See with MLPs . But make sure you replace\\nmlp_model with cnn_model:\\ncnn_trained_model_state =\\n  cnn_model\\n  |> Axon.Loop.trainer( :binary_cross_entropy ,  \\n:adam )\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.run(train_pipeline, %{},  epochs:  5, \\ncompiler:  EXLA)\\nAfter some time, you’ll see this:\\nEpoch: 0, Batch: 150, accuracy: 0.6360207 loss: \\n0.6347136\\nEpoch: 1, Batch: 150, accuracy: 0.7611755 loss: \\n0.5627094', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 335}),\n",
       " Document(page_content='Epoch: 2, Batch: 150, accuracy: 0.8005486 loss: \\n0.5184103\\nEpoch: 3, Batch: 150, accuracy: 0.8290567 loss: \\n0.4833005\\nEpoch: 4, Batch: 150, accuracy: 0.8488207 loss: \\n0.4532791\\nNotice both training loss and training accuracy are significantly better than\\nyour MLP model—and it seems your model hasn’t even plateaued. Of\\ncourse, without the use of a validation set, it’s difficult to know whether or\\nnot your model has started to overfit. Later in this chapter, you’ll use some\\nof Axon’s out-of-the-box training tools to prevent overfitting. For now,\\nverify your trained CNN outperforms your MLP model on the test set:\\ncnn_model\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )\\n|> Axon.Loop.run(test_pipeline, \\ncnn_trained_model_state,  compiler:  EXLA)\\nEventually, you’ll see this:\\nBatch: 6, accuracy: 0.8303571', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 336}),\n",
       " Document(page_content='Your test accuracy is significantly better than your MLP model. So, what’s\\ngoing on here? You know what a convolution is, what a typical CNN looks\\nlike, and how to implement a CNN in Axon, but you don’t know why your\\nCNN can solve this image classification problem so much better than a\\nplain MLP.\\nWhy CNNs Work\\nCNNs improve the performance of traditional MLPs on certain classes of\\ninput data, such as images, because they exploit prior knowledge about the\\nstructure of the input data.\\nRecall from Chapter 1,  Make Machines That Learn , that all models come\\nwith assumptions. CNNs make specific assumptions about the structure of\\ninput data and the types of relationships present in the input data.\\nMathematically speaking, CNNs make use of sparse interactions,\\nparameter sharing, and equivariant representations. The spatial dimensions\\nof the learned kernel in a convolutional layer are typically far smaller than\\nthe entire input image. The size of the input kernel enforces a measure of\\nlocality—a convolutional layer can only learn to extract features on a local\\nscale. Additionally, the same kernel is applied over each valid region in the\\ninput data. Applying the same kernel to each valid region assumes that\\nlearned local representations are consistent globally. For example, the\\nstructure of an edge in an image largely remains the same regardless of\\nwhether it’s in the top right corner or the bottom right corner. Convolutional', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 337}),\n",
       " Document(page_content='layers exploit this fact and reuse the same weights for different spatial\\nregions of the input.\\nPooling layers further improve the performance of CNNs on certain classes\\nof input data by making CNNs translation invariant. Translation invariance\\nmeans that CNNs with pooling aren’t affected by small translations in the\\ninput, which is useful because often you only care about the presence of a\\nfeature, but not necessarily where the feature is. If you have a CNN that\\ndetects images of dogs, the location of the dog in the input doesn’t matter as\\nmuch as the presence of the dog.\\nCNNs also appear to leverage the concept of cascading representations of\\nthe input data. If you were to inspect what features earlier convolutional\\nlayers identify, you’d likely find them activating on primitive features in the\\ninput, such as edges and colors. The deeper you go in the network, the\\nhigher-level extracted features become. For example, later layers in a CNN\\nmay appear to form higher-level representations of the visual\\nrepresentations of a dog or a cat. By composing more primitive extracted\\nfeature representations, CNNs are able to capture the complex spatial\\nrelationships present in images.\\nIntuitively, this learning process might make you think that it’s possible to\\ntake advantage of a universal CNN for all image recognition tasks. After all,\\nmost images share many of the same primitive input features. As it turns\\nout, it’s possible to leverage feature detectors from pre-trained models in', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 338}),\n",
       " Document(page_content='other models. You’ll use pre-trained models in Chapter 8,  Stop Reinventing\\nthe Wheel .', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 339}),\n",
       " Document(page_content='Improving the Training Process\\nWhile a model that predicts cats and dogs with around 80% accuracy is\\npretty good, there’s still a lot of room to improve. The winning model for\\nthis exact dataset achieved 98% accuracy in 2013. Since then, there have\\nbeen significant advances in the field of computer vision, and you have yet\\nto exhaust all of the training tricks in the book. In this section, you’ll make\\na few small tweaks to your model and training process, and you’ll see how\\nthey help you push your model toward 90% accuracy.\\nAugmenting Data\\nRight now you only have about 25,000 images total for training. For a\\ntypical deep learning problem, that’s not a lot. You could go out and collect\\nmore labeled images of cats and dogs, but that’s tedious and time-\\nconsuming. What if there was a way to artificially increase the size of your\\ndataset, without needing to collect more images? As it turns out, there is.\\nIntroducing data augmentation.\\nData augmentation is the process of slightly modifying input data to\\nartificially increase the size of your dataset. While data augmentation is\\npossible with all kinds of data, it’s most common with images because\\nimages are mostly translation-invariant—a picture of a dog is still a picture\\nof a dog, regardless of it being upside down or right side up.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 340}),\n",
       " Document(page_content='With images, you can perform a small number of random translations and\\nend up with an almost infinite number of different possible images. For\\nexample, if you randomly flip an input image upside down with some\\nprobability, you’ve essentially artificially doubled the size of your dataset.\\nIf you add another augmentation that randomly flips an image left or right,\\nyou’ve doubled the dataset again. Adding more augmentations leads to\\nexponentially increasing variations of input data.\\nIn addition to artificially increasing the size of data, data augmentation\\nmakes your model more resilient to transformations. In essence, you\\nartificially challenge your model to learn relationships that capture the\\nessence of a dog or the essence of a cat.\\nFortunately, data augmentation is as easy as applying random\\ntransformations to each input in your pipeline. For this example, you will\\nadd transformations that randomly flip an input image up and down and left\\nand right. You can introduce as many augmentations as you’d like, such as\\nrandomly setting the image hue, contrast, or brightness—but know that\\neach augmentation incurs preprocessing cost.\\nStart by modifying your CatsAndDogs module to include the following\\naugmentation methods:\\ndefp  random_flip({image, label}, axis)  do \\n   if   :rand .uniform() < 0.5  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 341}),\n",
       " Document(page_content='   {Nx.reverse(image,  axes:  [axis]), label}\\n   else \\n    {image, label}\\n   end \\nend \\nThis method randomly reverses the given axis with a probability of 0.5.\\nEssentially, you flip a coin for each image and determine whether or not it\\ngets flipped. Now, adjust your pipeline to include augmentations:\\ndef  pipeline_with_augmentations(\\n  paths,\\n  batch_size,\\n  target_height,\\n  target_width\\n)  do \\n  paths\\n  |> Enum.shuffle()\\n  |> Task.async_stream(&parse_image/1)\\n  |> Stream.filter( fn \\n    { :ok , {%StbImage{}, _}} -> true\\n    _ -> false\\n   end )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 342}),\n",
       " Document(page_content=' |> Stream.map(&to_tensors(&1, target_height, \\ntarget_width))\\n  |> Stream.map(&random_flip(&1,  :height ))\\n  |> Stream.map(&random_flip(&1,  :width ))\\n  |> Stream.chunk_every(batch_size, batch_size,  \\n:discard )\\n  |> Stream.map( fn  chunks ->\\n    {img_chunk, label_chunk} = Enum.unzip(chunks)\\n    {Nx.stack(img_chunk), Nx.stack(label_chunk)}\\n   end )\\nend \\nNow you can recreate your pipelines:\\n{test_paths, train_paths} =\\n  Path.wildcard( \"  train/*.jpg\" )\\n  |> Enum.shuffle()\\n  |> Enum.split(1000)\\nbatch_size = 128\\ntarget_height = 96\\ntarget_width = 96', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 343}),\n",
       " Document(page_content='train_pipeline = \\nCatsAndDogs.pipeline_with_augmentations(\\n  train_paths,\\n  batch_size,\\n  target_height,\\n  target_width\\n)\\ntest_pipeline = CatsAndDogs.pipeline(\\n  test_paths,\\n  batch_size,\\n  target_height,\\n  target_width\\n)\\nEnum.take(train_pipeline, 1)\\nNotice that you don’t want to apply augmentations to your test pipeline.\\nYou don’t want to make classification more difficult for your model at test\\ntime. Now, each image in the train set will be randomly flipped both\\nhorizontally and vertically in the training pipeline.\\nTweaking the Model', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 344}),\n",
       " Document(page_content='In addition to updating the training pipeline, you can make some tweaks to\\nyour model to improve its overall performance, without changing the entire\\narchitecture. In Chapter 4,  Optimize Everything , you learned about the\\nconcept of regularization. Remember, regularization is any strategy applied\\nto the training process designed to improve your model’s generalization\\nability. Dropout is a form of regularization that seeks to prevent a model\\nfrom overfitting by randomly masking some activations during training. For\\nexample, say you have a dense layer that outputs the following tensor:\\n#Nx.Tensor<\\n  f32[10]\\n  EXLA.Backend<cuda:0, \\n0.2231265192.3744595996.119072>\\n  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, \\n9.0]\\n>\\nApplying dropout with a dropout probability or rate of 0.5 would mask, or\\nset to 0, about half of the entries in the activation. So you might end up with\\na new activation tensor that looks like this:\\n#Nx.Tensor<\\n  f32[10]\\n  EXLA.Backend<cuda:0, \\n0.2231265192.3744595996.119072>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 345}),\n",
       " Document(page_content=' [0.0, 1.0, 2.0, 0.0, 0.0, 5.0, 6.0, 0.0, 8.0, \\n0.0]\\n>\\nBy dropping out intermediate activations, your model can no longer use\\nthem as a feature in subsequent layers, and thus your model needs to\\nbecome more robust. It needs to learn a larger range of representations\\nbecause, at any given time, a certain percentage of activations will be\\ncompletely useless. Mathematically, you can think of dropout as forcing\\nyour model to learn an infinite ensemble of submodels—the applied mask\\nchanges during each training step, and thus each training step trains a\\nfundamentally different model.\\nGeoffrey Hinton, one of the original authors of the Dropout paper\\n[SHKS14], said the idea for dropout came from security protocols in banks.\\nAccording to Hinton, banks routinely switch a teller’s shift to prevent any\\ngroup of tellers from colluding to defraud the bank. Dropout routinely\\nswitches which activations are masked at any given time, preventing any\\ngroup of activations from colluding to overfit to some feature in the training\\ndata.\\nIt’s important to note that dropout is only ever applied at training time.\\nDuring model inference and evaluation, dropout layers are completely\\nremoved, meaning the model can make use of its entire capacity. Adding', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 346}),\n",
       " Document(page_content='dropout layers to your model is relatively easy with Axon. Simply use\\nAxon.dropout/2:\\ncnn_model =\\n  Axon.input( \"  images\" ,  shape:  {nil, 96, 96, 3})\\n  |> Axon.conv(32,  kernel_size:  {3, 3},  \\nactivation:   :relu ,  padding:   :same )\\n  |> Axon.batch_norm()\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\n  |> Axon.conv(64,  kernel_size:  {3, 3},  \\nactivation:   :relu ,  padding:   :same )\\n  |> Axon.batch_norm()\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\n  |> Axon.conv(128,  kernel_size:  {3, 3},  \\nactivation:   :relu ,  padding:   :same )\\n  |> Axon.max_pool( kernel_size:  {2, 2},  strides:  \\n[2, 2])\\n  |> Axon.flatten()\\n  |> Axon.dense(128,  activation:   :relu )\\n  |> Axon.dropout( rate:  0.5)\\n  |> Axon.dense(1,  activation:   :sigmoid )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 347}),\n",
       " Document(page_content='Notice this adjusts the original cnn_model by applying a dropout layer\\nbetween the single hidden dense layer and the output layer. The :rate option\\ncontrols the probability or rate of masking in the dropout layer. The higher\\nthe rate, the more difficult you make it for your model to learn. Also, note\\nthat you should only apply dropout after intermediate activations in hidden\\nlayers. You should not apply dropout on an output layer.\\nEarly Stopping and Validation\\nWhen you first trained your CNN, you only allowed it to train for five\\nepochs. You might have noticed that the model’s training accuracy kept\\nimproving from epoch to epoch, indicating the model still had lots of room\\nto improve even after five training epochs.\\nHypothetically, you could increase the number of epochs to a large number\\nlike 100 and just let the model run. The problem is that your model will\\neventually overfit to the training data and likely perform worse than your\\nunderfit MLP model.\\nFortunately, you can make two adjustments to the training loop that allow\\nyou to train your model indefinitely and automatically stop training when\\nthe model starts to overfit: early stopping and validation.\\nAs you might remember from previous chapters, early stopping is a\\nregularization technique that stops model training when the model appears\\nto start overfitting. To determine whether a model is starting to overfit, you', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 348}),\n",
       " Document(page_content='routinely check the model’s performance against a holdout set. The holdout\\nset is generally a small percentage of training data that’s not shown to the\\nmodel during training. If the model’s performance on the validation data\\nstarts to dip, you know the model is probably starting to overfit, and you\\ncan stop training.\\nAxon offers loop event handlers, which takes care of early stopping and\\nvalidation for you. In an Axon loop, certain events are “fired” periodically.\\nFor example, at the end of every epoch, the :epoch_completed event fires, and\\nevery handler registered to run on :epoch_completed executes. You can\\nimplement custom event handlers, but usually, you’ll need only one of the\\nfew that Axon provides out of the box.\\nTo add early stopping and validation to your training loop, you need only to\\nmake a few adjustments to your original training code. First, run the\\nfollowing code to split your original training pipeline into a train and\\nvalidation pipeline:\\n{test_paths, train_paths} =\\n  Path.wildcard( \"  train/*.jpg\" )\\n  |> Enum.shuffle()\\n  |> Enum.split(1000)\\n{test_paths, val_paths} = test_paths |> \\nEnum.split(750)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 349}),\n",
       " Document(page_content='batch_size = 128\\ntarget_height = 96\\ntarget_width = 96\\ntrain_pipeline = \\nCatsAndDogs.pipeline_with_augmentations(\\n  train_paths,\\n  batch_size,\\n  target_height,\\n  target_width\\n)\\nval_pipeline = CatsAndDogs.pipeline(\\n  val_paths,\\n  batch_size,\\n  target_height,\\n  target_width\\n)\\ntest_pipeline = CatsAndDogs.pipeline(\\n  test_paths,\\n  batch_size,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 350}),\n",
       " Document(page_content=' target_height,\\n  target_width\\n)\\nEnum.take(train_pipeline, 1)\\nNext, adjust your training loop code to match the following:\\ncnn_trained_model_state =\\n  cnn_model\\n  |> Axon.Loop.trainer( :binary_cross_entropy , \\nAxon.Optimizers.adam(1.0e-3))\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.validate(cnn_model, val_pipeline)\\n  |> Axon.Loop.early_stop( \"  validation_loss\" ,  mode:  \\n:min )\\n  |> Axon.Loop.run(train_pipeline, %{},  epochs:  \\n100,  compiler:  EXLA)\\nYou should notice two additional lines here when compared to your original\\ntraining loop. Axon.Loop.validate/3 implements a supervised validation loop,\\ntracking all of the metrics from the original training loop against the\\nprovided validation data with the provided model. The validation loop runs\\nat the end of each epoch and tests the current model against the validation\\ndata it has yet to see. Immediately, the validation loop is', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 351}),\n",
       " Document(page_content='Axon.Loop.early_stop/3. This handler implements early stopping. It monitors\\nthe metric \"validation_loss\" and stops if the metric hasn’t improved. The :mode\\nspecifies which direction indicates improvement, with :min indicating you\\nare looking to minimize loss. Note the order of these handlers is important.\\nAxon.Loop.validate/3 registers validation metrics into the training state only\\nafter it runs, and thus if you add validation after early stopping, there won’t\\nbe any valid criteria for early stopping to monitor.\\nYou should also notice that rather than run for only five epochs, you can\\nsafely set :epochs to 100 and not have to worry about your model overfitting.\\nThe early stopping handler will trigger the alarm and stop the training loop\\nfrom running at the first sign of overfitting. You can safely step away from\\nthe computer and let your model run for as long as it needs. Eventually,\\nyou’ll see this:\\n...\\nBatch: 5, accuracy: 0.8711411 loss: 0.6648788\\nEpoch: 52, Batch: 150, accuracy: 0.9880484 loss: \\n0.1459639\\nBatch: 5, accuracy: 0.8589962 loss: 0.7372965\\nEpoch: 53, Batch: 150, accuracy: 0.9914632 loss: \\n0.1436956\\nBatch: 5, accuracy: 0.8685369 loss: 0.7409852\\nEpoch: 54, Batch: 150, accuracy: 0.9899628 loss: \\n0.1416151', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 352}),\n",
       " Document(page_content='Batch: 5, accuracy: 0.8683239 loss: 0.6846517\\nEpoch: 55, Batch: 150, accuracy: 0.9917219 loss: \\n0.1394880\\nBatch: 5, accuracy: 0.8726563 loss: 0.7095366\\nEpoch: 56, Batch: 150, accuracy: 0.9913080 loss: \\n0.1374808\\nBatch: 5, accuracy: 0.8626894 loss: 0.7270263\\nEpoch: 57, Batch: 150, accuracy: 0.9918253 loss: \\n0.1356151\\nBatch: 5, accuracy: 0.8605114 loss: 0.7936438\\nEpoch: 58, Batch: 150, accuracy: 0.9918253 loss: \\n0.1337494\\nNotice that at the first sign of overfitting your training loop stopped. Now,\\nrun the following code to validate that the tweaks you’ve made have\\nactually improved your model’s performance:\\ncnn_model\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )\\n|> Axon.Loop.run(test_pipeline, \\ncnn_trained_model_state,  compiler:  EXLA)\\nAnd you’ll see the following output:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 353}),\n",
       " Document(page_content='Batch: 5, accuracy: 0.8735322\\nSo the few small tweaks you made resulted in a significantly better model.\\nThis final model is miles ahead of the original MLP model you started with.\\nOne thing you should take away from this is that training models is an\\niterative process of trial and error. In a practical setting, especially with\\nlarge amounts of data, you want to test and prototype changes before\\nimplementing them on a full training run. Ideally, you add features one by\\none and only keep them if they show a demonstrable improvement over\\nyour previous model. As you train more and more models, you’ll start to\\ndevelop an intuition about what tweaks are necessary in certain situations.\\nYou’ll also start to develop a standard process for prototyping and training\\nmodels.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 354}),\n",
       " Document(page_content='Going Beyond Image Classification\\nAs you’ve seen in this chapter, CNNs are really good at learning to model\\nimage classification problems. In general, variants of CNN architectures are\\nthe go-to choice for any kind of computer vision task. But what other kinds\\nof computer vision tasks can you expect to encounter in the wild?\\nObject Detection\\nObject detection is a computer vision problem in which the goal is to\\nidentify and locate the objects present in an image. Object detection can\\nsomewhat be seen as an extension of the image classification task. For\\nexample, you might have noticed in some of the training images in the cats\\nvs. dogs dataset that a few images contained humans and other objects\\nalong with a dog. Rather than attempt to classify an image as just a cat or\\ndog, you could apply multiple labels to it such as human and dog or dog and\\ntoy. Object detection takes this task even further by assigning a bounding', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 355}),\n",
       " Document(page_content='box to every object in a picture:\\nThe standard object detection model in use today is YOLO [RDGF16]. Most\\nvariants of YOLO, short for “You Only Look Once,” make use of CNNs for\\nfeature extraction and employ a number of additional strategies to achieve\\nstate-of-the-art performance on object detection tasks. Some new variants\\nmake use of vision transformers for feature extraction. You’ll learn more\\nabout the power of transformers in Chapter 11,  Model Everything with\\nTransformers .\\nImage Segmentation\\nImage segmentation or semantic segmentation is a computer vision task in\\nwhich the goal is to classify every pixel in an image to some class. This', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 356}),\n",
       " Document(page_content='extends the object detection task to an extreme level. Rather than draw a\\ngranular bounding box around each object in the image, the goal is to draw\\nan exact mask around each object in an input image.\\nPerhaps the most famous architecture for image segmentation is U-Net\\n[RFB15], which employs a CNN for the semantic segmentation of\\nbiomedical imagery. Like in object detection, some modern architectures\\nmake use of vision transformers to achieve state-of-the-art performance on\\nsegmentation tasks.\\nBeyond Computer Vision\\nCNNs aren’t only useful on image data. As you’ve learned in this chapter,\\nyou can effectively apply CNNs to anything with a grid-like topology. This\\nincludes audio data, video data, and even some representations of text.\\nIntuitively speaking, CNNs can prove useful in any situation where there’s\\na locality to the relationship being modeled.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 357}),\n",
       " Document(page_content='[14]\\n[15]\\nWrapping Up\\nIn this chapter, you implemented a convolutional neural network (CNN)\\nand compared its performance to a traditional MLP on computer vision\\ntasks. You broke down the convolution operation, convolutional layers, and\\nmax pooling layers. You also learned why convolutional layers are able to\\nlearn to represent images so well. Finally, you used a few model training\\ntricks in Axon to improve the performance of your model.\\nOne thing you might have noticed throughout this chapter is that training\\nneural networks can sometimes be tedious and time-consuming. Pushing\\nthe performance of your model into a state of the art is no joke. Wouldn’t it\\nbe nice if you could save some training time and make use of someone\\nelse’s hard work? Fortunately, you can. In the next chapter, you’ll see how\\nit’s possible to stand on the shoulders of giants—or giant models—to\\nachieve incredible performance on any task.\\nFOOTNOTES\\nhttps://www.kaggle.com/c/dogs-vs-cats\\nhttps://www.image-net.org/\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 358}),\n",
       " Document(page_content='Chapter 8\\nStop Reinventing the Wheel\\n\\xa0\\nIn the previous chapter, you implemented a new type of neural network: the\\nconvolutional neural network. You built and trained a convolutional neural\\nnetwork to accurately classify images of cats and dogs. After applying a few\\nadditional model training tricks, such as data augmentation and dropout,\\nyou were able to train a model to classify an image as a cat or a dog with\\n87% accuracy. While 87% accuracy is great, you can do even better still—\\nwith minimal changes to your training pipeline—by leveraging the power\\nof pre-trained models and transfer learning. In this chapter, you’ll discover\\nwhat transfer learning is, when it’s necessary, and how to perform transfer\\nlearning with Axon.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 359}),\n",
       " Document(page_content='Identifying Cats and Dogs Again\\nTo understand the powers of transfer learning, you’ll make a few\\nadjustments to your model training code from the previous chapter and\\ncompare the performance of your newly trained model with the\\nperformance of the models you trained previously. To start, fire up a\\nLivebook and install the following dependencies:\\nMix.install([\\n  { :axon_onnx ,  \"  ~> 0.4\" },\\n  { :axon ,  \"  ~> 0.5\" },\\n  { :nx ,  \"  ~> 0.5\" },\\n  { :exla ,  \"  ~> 0.5\" },\\n  { :stb_image ,  \"  ~> 0.6\" },\\n  { :kino ,  \"  ~> 0.8\" },\\n])\\nYou should recognize all of these dependencies from the previous chapters\\nof this book with the exception of :axon_onnx, which is a library for\\nimporting and exporting Open Neural Network Exchange (ONNX) models\\nto and from Axon. ONNX is a language-agnostic model serialization\\nprotocol that makes it perfect for bringing pre-trained models from the\\nPython ecosystem into the world of Elixir. ONNX is supported by most\\nmajor deep learning frameworks, and it has a number of runtime\\nimplementations that target edge, mobile, and server deployments. In', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 360}),\n",
       " Document(page_content='Chapter 13,  Put Machine Learning into Practice , you’ll get exposed to\\nsome additional usages of the ONNX format. In this chapter, you’ll use\\nONNX as an intermediary between the Python and Elixir machine learning\\necosystems.\\nONNX\\nONNX is a collaborative effort between Microsoft and Meta\\nto provide a common model serialization format. The ONNX\\nprotocol describes a graph-like data structure along with\\noperator specifications which dictate arguments, behavior, and\\nreturn types of a number of different nodes. While PyTorch\\nand TensorFlow, two of the largest deep learning frameworks,\\nhave their own specialized model serialization protocols, both\\nframeworks support serialization to ONNX.\\nYou’ll also want to set EXLA as the default backend:\\nNx.global_default_backend(EXLA.Backend)\\nWith the necessary dependencies installed, you can move on to\\nimplementing the same input pipeline as you did in the previous chapter.\\nYou don’t need to make any adjustments to your pipeline implementation\\n[16]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 361}),\n",
       " Document(page_content='from the previous chapter, you simply need to add the following module to\\nyour Livebook:\\ndefmodule  CatsAndDogs  do \\n   def  pipeline(paths, batch_size, target_height, \\ntarget_width)  do \\n    paths\\n    |> Enum.shuffle()\\n    |> Task.async_stream(&parse_image/1)\\n    |> Stream.filter( fn \\n      { :ok , {%StbImage{}, _}} -> true\\n      _ -> false\\n     end )\\n    |> Stream.map(&to_tensors(&1, target_height, \\ntarget_width))\\n    |> Stream.chunk_every(batch_size,  :discard )\\n    |> Stream.map( fn  chunks ->\\n      {img_chunk, label_chunk} = \\nEnum.unzip(chunks)\\n      {Nx.stack(img_chunk), Nx.stack(label_chunk)}\\n     end )\\n   end \\n   def  pipeline_with_augmentations(\\n    paths,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 362}),\n",
       " Document(page_content='   batch_size,\\n    target_height,\\n    target_width\\n  )  do \\n    paths\\n    |> Enum.shuffle()\\n    |> Task.async_stream(&parse_image/1)\\n    |> Stream.filter( fn \\n      { :ok , {%StbImage{}, _}} -> true\\n      _ -> false\\n     end )\\n    |> Stream.map(&to_tensors(&1, target_height, \\ntarget_width))\\n    |> Stream.map(&random_flip(&1,  :height ))\\n    |> Stream.map(&random_flip(&1,  :width ))\\n    |> Stream.chunk_every(batch_size,  :discard )\\n    |> Stream.map( fn  chunks ->\\n      {img_chunk, label_chunk} = \\nEnum.unzip(chunks)\\n      {Nx.stack(img_chunk), Nx.stack(label_chunk)}\\n     end )\\n   end \\n   defp  random_flip({image, label}, axis)  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 363}),\n",
       " Document(page_content='    if   :rand .uniform() < 0.5  do \\n      {Nx.reverse(image,  axes:  [axis]), label}\\n     else \\n      {image, label}\\n     end \\n   end \\n   defp  parse_image(path)  do \\n    label =  if  String.contains?(path,  \"  cat\" ),  do : \\n0,  else : 1\\n     case  StbImage.read_file(path)  do \\n      { :ok , img} -> {img, label}\\n      _error ->  :error \\n     end \\n   end \\n   defp  to_tensors({ :ok , {img, label}}, \\ntarget_height, target_width)  do \\n    img_tensor =\\n      img\\n      |> StbImage.resize(target_height, \\ntarget_width)\\n      |> StbImage.to_nx()', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 364}),\n",
       " Document(page_content='     |> Nx.divide(255)\\n      |> Nx.transpose( axes:  [ :channels ,  :height ,  \\n:width ])\\n    label_tensor = Nx.tensor([label])\\n    {img_tensor, label_tensor}\\n   end \\nend \\nRecall from  Building an Input Pipeline , that this pipeline returns a Stream\\nof tuples: {input, target}, where input is a tensor representation of a batch of\\nimages and target is a tensor representation of a batch of labels. You should\\nalso notice that this pipeline augments input images by randomly flipping\\nthem vertically and horizontally.\\nThere’s one minor difference between this pipeline and the pipeline you\\nimplemented here. You’re going to use a model that uses a channels first\\nrepresentation of images, and StbImage returns channels last\\nrepresentations. Therefore, you need to transpose the images to convert\\nthem to a channels first representation.\\nNow, to make sure you’ve copied everything over correctly, run the\\nfollowing code:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 365}),\n",
       " Document(page_content='{test_paths, train_paths} =\\n  Path.wildcard( \"  train/*.jpg\" )\\n  |> Enum.shuffle()\\n  |> Enum.split(1000)\\n{test_paths, val_paths} = test_paths |> \\nEnum.split(750)\\nbatch_size = 32\\ntarget_height = 160\\ntarget_width = 160\\ntrain_pipeline = \\nCatsAndDogs.pipeline_with_augmentations(\\n  train_paths,\\n  batch_size,\\n  target_height,\\n  target_width\\n)\\nval_pipeline = CatsAndDogs.pipeline(\\n  val_paths,\\n  batch_size,\\n  target_height,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 366}),\n",
       " Document(page_content=' target_width\\n)\\ntest_pipeline = CatsAndDogs.pipeline(\\n  test_paths,\\n  batch_size,\\n  target_height,\\n  target_width\\n)\\nEnum.take(train_pipeline, 1)\\nYou’ll notice that in this snippet you’ve changed the target height and target\\nwidth to 160. You can adjust the height and width to whatever you want, but\\nthe model you’re going to use requires larger input images.\\n[\\n  {#Nx.Tensor<\\n     f32[32][channels: 3][height: 160][width: 160]\\n     EXLA.Backend<cuda:0, \\n0.2937518557.1715339297.110861>\\n     [\\n       [\\n         [', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 367}),\n",
       " Document(page_content='          [0.7450979948043823, \\n0.7372548580169678, 0.8078430891036987, ...],\\n           ...\\n         ],\\n         ...\\n       ],\\n       ...\\n     ]\\n   >,\\n   #Nx.Tensor<\\n     s64[32][1]\\n     EXLA.Backend<cuda:0, \\n0.2937518557.1715339297.110894>\\n     [\\n       [0],\\n       [0],\\n       [0],\\n       [0],\\n       [0],\\n       [1],\\n       ...\\n     ]\\n   >}\\n]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 368}),\n",
       " Document(page_content='With your training pipeline in place, it’s time to implement a model. In  The\\nAnatomy of a CNN , you learned about the typical architectural pattern of a\\nconvolutional neural network—a convolutional base followed by a fully-\\nconnected network. Remember that in this paradigm, the convolutional base\\nserves as a feature extractor, passing relevant features for use in\\ndownstream tasks by the fully-connected network. Assuming that you have\\na convolutional base that can extract useful features from your input data, it\\ndoesn’t matter where that convolutional base comes from, or what data it\\nwas trained on.\\nIn Chapter 7,  Learn to See , you trained your own convolutional base from\\nscratch on your own training data. In theory, there’s nothing wrong with\\ntraining models from scratch, but trying to do so has disadvantages when\\nyou’re in a low-data regime. In other words, in the absence of a lot of\\ntraining data, training a model from scratch isn’t necessarily a good idea. A\\ncommon solution to this problem is to use transfer learning. In the context\\nof computer vision, transfer learning often makes use of a pre-trained\\nconvolutional base followed by a custom fully-connected network. The pre-\\ntrained convolutional base is usually trained on a large dataset like\\nImageNet and serves as a general visual feature extractor. It doesn’t\\nmatter that the convolutional base hasn’t seen any of your training data\\nbecause visual features are consistent. An edge is an edge, a face is a face,\\nand a dog is a dog no matter the context in which they’re presented.\\n[17]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 369}),\n",
       " Document(page_content='Using a pre-trained model is kind of like buying some-assembly-required\\nfurniture from the store. While you might be able to build your own desk\\nfrom scratch, you probably don’t want to spend the time chopping down\\ntrees, designing and measuring different pieces, and hoping the final\\nproduct turns out okay. Rather, you want to take advantage of the hard work\\nsomebody else has already done. All you need to do is put the pieces\\ntogether—and there’s still room to add some of your own flare.\\nWhile your final model from  Tweaking the Model , performed admirably,\\nyour dataset is small enough that you’ll likely see improvements replacing\\nyour custom convolutional base with a pre-trained convolutional base. To\\ndo so, start by downloading the MobileNet model from the official\\nONNX model repository.\\nMobileNet is a lightweight convolutional neural network specially designed\\nfor use on mobile devices. Although you can choose to use another—\\npossibly more powerful model—MobileNet optimizes for maximum\\nperformance with minimal compute requirements and thus will be faster to\\nwork with.\\nAfter you’ve downloaded the mobilenetv2-7.onnx file, run the following code\\nin your Livebook:\\n{cnn_base, cnn_base_params} = AxonOnnx. import (\\n   \"  mobilenetv2-7.onnx\" ,  batch_size:  batch_size\\n[18]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 370}),\n",
       " Document(page_content=')\\nThis code imports the mobilenetv2-7.onnx model architecture and parameters\\ninto an Axon model and parameter map. You can inspect the architecture by\\nrunning the following code:\\ninput_template = Nx.template({1, 3, target_height, \\ntarget_width},  :f32 )\\nAxon.Display.as_graph(cnn_base, input_template)\\nThe result is a lengthy output graph.\\nYou might notice that the output shape of this model is {1, 1000}—that’s\\nbecause this model was pre-trained on ImageNet. The ImageNet\\nclassification task consists of images from 1,000 different classes. For this\\nexample, you want to train your own classification head, so you need to\\nchop off the original one. Fortunately, Axon offers a robust graph\\nmanipulation API, which makes it easy to extract different portions of the\\ngraph. To chop off the original classification head, run the following code:\\n{_popped, cnn_base} = cnn_base |> Axon.pop_node()\\n{_popped, cnn_base} = cnn_base |> Axon.pop_node()\\nYou can now reinspect the new convolutional base like so:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 371}),\n",
       " Document(page_content='Axon.Display.as_graph(cnn_base, input_template)\\nYou’ll notice that the output shape of the model is {1, 1280, 1, 1} because\\nyou’ve removed the classification head from the original model.\\ncnn_base will serve as your model’s pre-trained convolutional base. The\\noutput shape of the convolutional base is {batch_size, 1280, 1, 1}, which means\\nyou need to insert a fully-connected network on top of the convolutional\\nbase. The imported cnn_base is a regular Axon struct, which means you can\\nbuild and manipulate it like you would if it were a model you built on your\\nown. Before adding any additional layers, you will need to delineate the\\nconvolutional base from other components in your model. You can do this\\nwith an Axon namespace.\\nAxon namespaces are simple metadata layers that provide a mechanism for\\ndistinguishing between components of a model. Namespaces are simply a\\nway to tell Axon to group the parameters and state of multiple layers into a\\nsingle place. By default, all Axon layers belong to a root namespace. You\\ncan see this in the structure of the Axon model state, which is, essentially, a\\nmap.\\nImagine you have a simple model with two layers named \"foo\" and \"bar\".\\nThe model state would look something like this:\\n%{\\n  \"foo\" => %{', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 372}),\n",
       " Document(page_content='   \"param_name\" => #Nx.Tensor<...>,\\n    \"state_name\" => #Nx.Tensor<...>\\n  },\\n  \"bar\" => %{\\n    \"param_name\" => #Nx.Tensor<...>\\n  }\\n}\\nIf you add a namespace to the same model that wraps the layer named \"foo\",\\nthe model state takes on a form that looks something more like this:\\n%{\\n  \"namespace\" => %{\\n    \"foo\" => %{\\n      \"param_name\" => #Nx.Tensor<...>,\\n      \"state_name\" => #Nx.Tensor<...>\\n    },\\n  },\\n  \"bar\" => %{\\n    \"param_name\" => #Nx.Tensor<...>\\n  }\\n}\\nNotice how \"foo\" is no longer at the top level in the model state. That’s\\nbecause its state belongs to the namespace \"namespace\". At first glance, this', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 373}),\n",
       " Document(page_content='might seem like a silly abstraction. But namespaces provide a simple and\\npowerful way of initializing portions of a model from pre-trained\\ncheckpoints, applying different optimization mechanisms to different\\nlayers, and generally manipulating hierarchies of layers in your model.\\nAxon differs explicitly from PyTorch and TensorFlow in that models aren’t\\nbuilt from a hierarchy of modules, but rather composed into a graph-like\\ndata structure. This approach has some advantages, but it can be difficult to\\nexpress ownership or hierarchy in a model. Namespaces offer a mechanism\\nfor expressing hierarchy and logical separation.\\nTo wrap your convolutional base into its own namespace, you need to use\\nthe Axon.namespace/2 function:\\ncnn_base = cnn_base |> Axon.namespace( \"  \\nfeature_extractor\" )\\nAxon.namespace/2 wraps all preceding nodes in the Axon graph under the\\ngiven namespace. In this example, you give your convolutional base to the\\n\"feature_extractor\" namespace, which will come in handy later on.\\nIn addition to namespacing your convolutional base, you need to freeze the\\nconvolutional base. When using pre-trained models, it’s common to freeze\\nor stop training for the pre-trained portion of your model to avoid\\ncatastrophic forgetting in the early stages of training. In other words, the\\npre-trained model remains entirely static during initial training, so it doesn’t', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 374}),\n",
       " Document(page_content='lose the knowledge it has already acquired. The early stages of training are\\ntypically the least stable, which means your pre-trained model is at risk of\\nlosing some of its value.\\nFreezing layers tells Axon to wrap a layer’s parameters in stop_grad. This\\nmeans that when updating the model during gradient descent, the gradient\\nof frozen parameters will be 0 and therefore won’t be updated. In Axon,\\nfreezing happens with respect to the model rather than the training loop or\\nparameters. You can mark certain layers as frozen using the Axon.freeze/2\\nfunction. Axon.freeze/2 by default will freeze all proceeding layers. But it\\noffers an API for more fine-grained freezing in models. For this example,\\nyou’ll freeze your convolutional base like so:\\ncnn_base = cnn_base |> Axon.freeze()\\nYou now have a frozen, pre-trained convolutional base wrapped in an\\nidentifiable namespace. The output shape of the model at this point is\\n{batch_size, 1280, 1, 1}, so you need to flatten the features before passing them\\nto a classification head. You can flatten the features using Axon.ﬂatten/2, or\\nyou can use a global pooling layer. Because the amount of output features\\nin this model is relatively large, a global pooling layer works better because\\nit reduces the amount of input features to the classification head.\\nAdditionally, you’ll want to add some regularization by using a dropout\\nlayer between your global average pooling and classification head. For this', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 375}),\n",
       " Document(page_content='example, your classification head needs a single unit because this is a\\nbinary classification problem:\\nmodel =\\n  cnn_base\\n  |> Axon.global_avg_pool( channels:   :first )\\n  |> Axon.dropout( rate:  0.2)\\n  |> Axon.dense(1)\\nNotice that you need to specify channels: :ﬁrst for this example because the\\nmodel you’re using defaults to channels first representations. If you put all\\nof these steps together, the code for building your model should look\\nsomething like this:\\n{cnn_base, cnn_base_params} = AxonOnnx. import ( \"  \\nmobilenetv2-7.onnx\" )\\n{_popped, cnn_base} = Axon.pop_node(cnn_base)\\n{_popped, cnn_base} = Axon.pop_node(cnn_base)\\nmodel =\\n  cnn_base\\n  |> Axon.namespace( \"  feature_extractor\" )\\n  |> Axon.freeze()\\n  |> Axon.global_avg_pool( channels:   :first )\\n  |> Axon.dropout( rate:  0.2)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 376}),\n",
       " Document(page_content=' |> Axon.dense(1)\\nWith your input pipeline and model in place, you can move forward with\\ncreating a training loop. By now, you should have a pretty good idea of how\\nto create and manipulate Axon training loops. For this example, start by\\nimplementing the same training loop, as you did in  Early Stopping and\\nValidation , with some minor adjustments:\\nloss = &Axon.Losses.binary_cross_entropy(&1, &2,\\n   reduction:   :mean ,\\n   from_logits:  true\\n)\\noptimizer = Axon.Optimizers.adam(1.0e-4)\\ntrained_model_state =\\n  model\\n  |> Axon.Loop.trainer(loss, optimizer)\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.validate(model, val_pipeline)\\n  |> Axon.Loop.early_stop( \"  validation_loss\" ,  mode:  \\n:min ,  patience:  5)\\n  |> Axon.Loop.run(\\n    train_pipeline,\\n    %{ \"  feature_extractor\"  => cnn_base_params},', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 377}),\n",
       " Document(page_content='    epochs:  100,\\n     compiler:  EXLA\\n  )\\nWhen comparing this code to your previous training loop, you’ll see a few\\ndifferences.\\nFirst, you use a parametrized version of Axon.Losses.binary_cross_entropy/3 as\\nyour loss function. You may have noticed your output dense layer doesn’t\\nuse a sigmoid activation—it doesn’t squeeze the output to a probability\\nbetween 0 and 1. The outputs for this dense layer are referred to as logits or\\nlog-probabilities. When you don’t specify a final sigmoid activation (or\\nsoftmax for multi-class classification problems), you need to tell your loss\\nfunction that you are passing logits rather than probabilities. You can do this\\nby passing from_logits: true to your loss function. There’s no difference\\nbetween using the logit form or probability form—other than you can skip\\nthe extra sigmoid activation. Axon actually has an optimization that\\ninternally always uses logits to compute the cross-entropy loss. When you\\nuse logits, you need to remember to apply a sigmoid or softmax to your\\npredictions in order to compute a probability.\\nAnother difference you’ll see is the use of the Axon.Optimizers functional\\nform to declare an optimizer. This is just to use a learning rate that differs\\nfrom the Adam optimizer’s default.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 378}),\n",
       " Document(page_content='The final difference you’ll see is that the empty map after train_pipeline has\\nbeen replaced with a map containing the parameters for your model’s\\nfeature extractor. This map represents your training loop’s initial state.\\nIn all of the previous chapters, you trained models from scratch, so you\\ndidn’t need any initial state. Because you want to make use of a pre-trained\\nmodel here, you need to tell Axon what pre-trained parameters to use. Axon\\nwill take this initial state and initialize a model with the given fixed\\nparameters for your feature extractor.\\nYou could provide all of the model’s initial parameters here if you wanted\\nto. Under the hood, Axon initializes all of the parameters not given in the\\ninitial state for you. The initial state argument is kind of like the\\naccumulator for Axon’s training loop. There’s no difference between you\\ninitializing a model’s parameters explicitly and providing them to a training\\nloop and you letting Axon do it for you. Letting Axon do the work for you\\nonly saves you a little bit of boilerplate code.\\nIt’s important to understand that building on top of a pre-trained model\\ndoesn’t necessarily need to change much about your model training process.\\nAside from a few tweaks in the model creation process, the addition of\\nsome initial state in the training loop, and some tweaks to the loss function\\nand optimizer, not much has changed from the previous chapter. Many of\\nthe principles and strategies that apply when training a model from scratch', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 379}),\n",
       " Document(page_content='also apply when training on top of a pre-trained model. You can take\\nadvantage of someone else’s hard work with minimal changes.\\nAs with any machine learning algorithm, you’ll have to consider some\\ncaveats and limitations, which you’ll learn about in  Knowing When to Use\\nTransfer Learning . However, transfer learning is an incredibly powerful\\ntool. In a lot of cases, it makes more sense to reach for a pre-trained model\\nthan to attempt to train one from scratch.\\nAfter some time running your training loop, you’ll see the following output:\\n...\\nBatch: 23, accuracy: 0.9348959 loss: 0.1361023\\nEpoch: 15, Batch: 700, accuracy: 0.9220337 loss: \\n0.2185298\\nBatch: 23, accuracy: 0.9414064 loss: 0.1326849\\nEpoch: 16, Batch: 700, accuracy: 0.9208744 loss: \\n0.2164259\\nBatch: 23, accuracy: 0.9361981 loss: 0.1386008\\nEpoch: 17, Batch: 700, accuracy: 0.9210071 loss: \\n0.2145825\\nBatch: 23, accuracy: 0.9375002 loss: 0.1397082\\nEpoch: 18, Batch: 700, accuracy: 0.9189568 loss: \\n0.2129661\\nBatch: 23, accuracy: 0.9388022 loss: 0.1403071', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 380}),\n",
       " Document(page_content='Your model is touching 93-94% training and validation accuracy, but does it\\nhold up on the test set? Run the following evaluation loop to find out:\\neval_model = model |> Axon.sigmoid()\\neval_model\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )\\n|> Axon.Loop.run(test_pipeline, \\ntrained_model_state,  compiler:  EXLA)\\nIn this example, you adjust your model by applying a sigmoid on top of the\\noriginal model to output probabilities rather than logits.\\nAfter running the code, you’ll see this result:\\nBatch: 23, accuracy: 0.9449407\\nWow, by simply attaching a single dense output layer on top of a pre-trained\\nmodel, you were able to significantly outperform your custom model. But\\nthere’s still some performance left on the table. In the next section, you’ll\\nfine-tune your model for even better performance.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 381}),\n",
       " Document(page_content='Fine-Tuning Your Model\\nIn the previous section, you made use of a pre-trained model for feature\\nextraction. You attached a classification head on top of a frozen pre-trained\\nmodel—taking advantage of the general features extracted from the pre-\\ntrained model for your specific problem. Remember, freezing the model\\ninitially was important because the early stages of training are unstable and\\nyour model was at risk of losing all of its prior knowledge. But now that\\nyou have a trained model, you can unfreeze some of the layers of the pre-\\ntrained model and force them to learn features specific to your problem.\\nThis process is called fine-tuning.\\n\\xa0\\nRather than freeze the entire pre-trained model, during fine-tuning you\\nunfreeze the top-most layers of the pre-trained model. Remember that early\\nlayers of a convolutional neural network learn more general features, while\\nlater layers learn features specific to your dataset. By unfreezing a small\\namount of the top-most layers, you allow your model to learn features that\\nare specific to your dataset. To start the fine-tuning process, run the\\nfollowing code to unfreeze the top-most layers of your convolutional base:\\nmodel = model |> Axon.unfreeze( up:  50)\\nThis code unfreezes the top 50 layers of your model—meaning that the top\\n50 layers in your model will be trainable. Now, rewrite your training', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 382}),\n",
       " Document(page_content='pipeline to look like this:\\nloss = &Axon.Losses.binary_cross_entropy(&1, &2,\\n   reduction:   :mean ,\\n   from_logits:  true\\n)\\noptimizer = Axon.Optimizers.rmsprop(1.0e-5)\\ntrained_model_state =\\n  model\\n  |> Axon.Loop.trainer(loss, optimizer)\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.validate(model, val_pipeline)\\n  |> Axon.Loop.early_stop( \"  validation_loss\" ,  mode:  \\n:min ,  patience:  5)\\n  |> Axon.Loop.run(\\n    train_pipeline,\\n    trained_model_state,\\n     epochs:  100,\\n     compiler:  EXLA\\n  )\\nThis training pipeline is essentially identical to the one you implemented in\\nthe previous section. Only two minor differences exist. First, you changed\\nthe learning rate in the optimizer from 1.0e-4 to 1.0e-5 and switched to the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 383}),\n",
       " Document(page_content='RMSProp optimizer. When fine-tuning, it’s important to keep the learning\\nrate low—larger learning rates when fine-tuning make the model\\nsusceptible to overfitting and possibly unstable updates.\\nFinally, rather than only passing the pre-trained model’s parameters, you\\npass the entire trained model state from the previous training iteration as the\\ninitial state of your loop. Again, this is just telling Axon that you don’t want\\nto start training from scratch—you already have a trained model and you\\nwant to use it.\\nAfter running the training loop, you’ll see this:\\n...\\nBatch: 23, accuracy: 0.9739585 loss: 0.0710718\\nEpoch: 5, Batch: 700, accuracy: 0.9660777 loss: \\n0.1285189\\nBatch: 23, accuracy: 0.9739584 loss: 0.0710575\\nEpoch: 6, Batch: 700, accuracy: 0.9713383 loss: \\n0.1209094\\nBatch: 23, accuracy: 0.9765627 loss: 0.0717105\\nEpoch: 7, Batch: 700, accuracy: 0.9726299 loss: \\n0.1142527\\nBatch: 23, accuracy: 0.9778647 loss: 0.0800760\\nEpoch: 8, Batch: 700, accuracy: 0.9770432 loss: \\n0.1082386', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 384}),\n",
       " Document(page_content='Batch: 23, accuracy: 0.9752605 loss: 0.0922253\\nEpoch: 9, Batch: 700, accuracy: 0.9777576 loss: \\n0.1030978\\nBatch: 23, accuracy: 0.9709822 loss: 0.1120541\\nYour model already had some impressive performance after the previous\\niteration of training, and that shows in the training logs here. Your model\\nrecorded slight improvements on the training and validation data, but you\\nneed to see if fine-tuning increased its performance on the test set:\\neval_model = model |> Axon.sigmoid()\\neval_model\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )\\n|> Axon.Loop.run(test_pipeline, \\ntrained_model_state,  compiler:  EXLA)\\nAfter running this cell, you’ll see the following:\\nBatch: 23, accuracy: 0.9709823\\nJust when you start to think you can’t do any better on this problem, you’re\\nable to squeeze even more performance out of your model. By fine-tuning', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 385}),\n",
       " Document(page_content='the top layers of your pre-trained model, you were able to increase the\\nperformance of your model by a few percentage points.\\nNow that you’ve seen the power of transfer learning and fine-tuning, you\\nmight be wondering why they work so well, and if you should use them in\\nevery context. In the next section, you’ll dive deeper into the intuition\\nbehind transfer learning and learn more about when you should use it.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 386}),\n",
       " Document(page_content='Understanding Transfer Learning\\nTransfer learning is a technique for repurposing a pre-trained model for use\\non a related task. In the previous section, you saw transfer learning in the\\ncontext of a deep learning problem—specifically a computer vision\\nproblem. It is possible to make use of transfer learning with other machine\\nlearning techniques and in other domains, but transfer learning is incredibly\\ncommon in computer vision problems.\\nThe power of transfer learning should make some intuitive sense. Rather\\nthan start from scratch for every problem, you inject a model with some\\npast knowledge of the problem to speed up training. In Chapter 6,  Go Deep\\nwith Axon , you learned that the power of deep learning is all about learning\\nuseful representations of data. If you have a pre-trained model that already\\nhas some useful representations of input data that is similar to yours, it\\nmakes sense that it would allow your model to learn faster and with better\\nfinal performance.\\nIn this section, you’ll dive deeper into why transfer learning works and\\nwhen to use it.\\nWhy Transfer Learning Works\\nRecall from Chapter 7,  Learn to See , that earlier layers in convolutional\\nneural networks often appear to learn general features, such as edges in an\\nimage. These general features often apply to any image recognition task.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 387}),\n",
       " Document(page_content='The features of an edge remain the same regardless of whether the purpose\\nof the model is to identify cats and dogs or detect letters on street signs.\\nWhat this means is that you can treat portions of pre-trained models as\\ngeneral feature detectors and apply them to your particular use case.\\nPre-trained models are often trained on large datasets such as ImageNet,\\nwhich means they are exposed to a wide range of input data. In other words,\\nthey are capable of generalizing to lots of different use cases because\\nthey’ve been trained on a broad range of data. In a sense, you can consider\\nthe representation learned by these models to be a general representation of\\nvisual data. But it might be a stretch to say these models apply to all classes\\nof visual data. The implications of using a model which already has some\\nuseful representations of visual data are that your model doesn’t have to\\nspend time struggling to learn its own representations and instead can learn\\nhow to make use of those representations.\\nConsider the model you trained from scratch in the previous chapter. It was\\ntrained on around 25,000 images of cats and dogs. If you compare the\\nfeatures learned by your model from the previous chapter to the features\\nlearned by MobileNet—which was trained on 14 million images of 1,000\\ndifferent image classes—what model do you think learned the most robust,\\ngeneralizable features of visual data? Your trained-from-scratch model had\\nto not only learn representations of visual data from a small amount of input\\ndata but also make use of those representations for making predictions. The\\nmodel you trained in this chapter has a much simpler task—learn to make', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 388}),\n",
       " Document(page_content='predictions from a robust representation. You can think of it as giving your\\nmodel a significant head start in a race.\\nUnderstanding why transfer learning works so well might lead you to\\nwonder if you should use pre-trained models for every problem you\\nencounter. In the next section, you’ll learn a little bit more about when it\\nmakes sense to use transfer learning and when it doesn’t.\\nKnowing When to Use Transfer Learning\\nLike most questions in machine learning, there’s no definitive answer to\\nwhen to use transfer learning. The real answer is “it depends.” It depends on\\nmany factors such as the kind of data you have, how much data you have,\\nhow many resources you have, and what your objectives are.\\nIn some domains, such as computer vision and natural language\\nprocessing, transfer learning is the standard approach to training new\\nmodels on specialized applications. This is largely due to the abundance of\\ngeneral models and the significant performance increases they bring to a\\nwide variety of problems—even without much data.\\nGenerally speaking, if you don’t have a lot of data, you’ll probably benefit\\nfrom making use of pre-trained models. Even if you do have a lot of data,\\nyou might still benefit from them. But certain domains necessitate novel\\napproaches. Some domains don’t have any useful pre-trained models.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 389}),\n",
       " Document(page_content='It’s also important to note you can’t use any pre-trained model and expect it\\nto work on your input data. For example, a MobileNet trained on ImageNet\\nis a terrible choice for the convolutional base of a model intended for use on\\nbiomedical images. ImageNet doesn’t have this imagery, and biomedical\\nimages don’t share many similarities with other types of images. For\\ntransfer learning to work, you need to ensure you choose a model that was\\ntrained on data similar enough to your use case to be effective—you\\nwouldn’t expect a model trained on Portuguese to have a useful\\nrepresentation of the English language.\\nWhen choosing a pre-trained model, you need to consider the problem\\nyou’re trying to solve and your success criteria for solving it. While you can\\nchoose the model with the best metrics on some common machine learning\\nbenchmark, that’s not always the right thing to do. For example, if you’re\\nplanning to deploy your model on mobile or edge devices, choosing the best\\nmodel usually also means the model is larger and more compute-intensive,\\nwhich means it will be slow. You need to take into account all of your\\nperformance objectives when choosing a pre-trained model.\\nFortunately, pre-trained models are abundant in the machine learning\\necosystem. But you might be wondering how it’s even possible to use them\\nconsidering the Elixir machine learning ecosystem is so new. Most pre-\\ntrained models you’ll find were built in Python with Python-specific\\nmachine learning frameworks. How can you take advantage of these models', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 390}),\n",
       " Document(page_content='in your problems? In the next section, you’ll use some of the tools and\\nrepositories available in Python to bring models into your Elixir projects.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 391}),\n",
       " Document(page_content='Taking Advantage of the Machine Learning\\nEcosystem\\nYou may have noticed that the title of this chapter is a bit ironic given it\\nappears in a book titled Machine Learning in Elixir. After all, isn’t the Nx\\necosystem a rehash of the Python machine learning ecosystem? In many\\nways, projects like Axon in the Elixir Nx ecosystem model themselves after\\ntheir counterparts in the Python ecosystem—with sensible deviations and\\ncomplete break-offs in design where necessary. The breaks from Python are\\ndeliberate, but that doesn’t mean Elixir Nx completely shies away from the\\nPython ecosystem. From the beginning, Nx was designed for flexibility.\\nRather than depend entirely on projects with a popular Python front-end\\nlike TensorFlow or PyTorch, Nx implements a modular approach that\\nmakes it capable of taking advantage of any tensor manipulation library.\\nThis philosophy of flexibility extends to libraries like Axon. The key feature\\nof Axon is the Axon data structure, which represents neural networks in a\\ngraph-like data structure. You can manipulate and build this data structure\\ndirectly from Elixir. The beauty is that if you know how to traverse the\\nAxon graph, you know how to convert it to an external format. Similarly, if\\nyou have a serialized format from an external framework such as ONNX or\\nTensorFlow, you can just as easily convert that into an Axon graph for use\\ndirectly in Elixir. Axon isn’t tied to any particular runtime, and thus you\\ndon’t have to worry about being tied to a particular model format.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 392}),\n",
       " Document(page_content='The Python ecosystem is massive, and just as there’s an abundance of web\\nframeworks and stacks to choose from across languages, there’s an\\nabundance of machine learning frameworks and stacks to choose from\\nwithin the Python ecosystem. Axon and its related projects have an explicit\\ngoal of seamless import of any model from the Python ecosystem for use\\nwithin Elixir without forcing you to implement a Port or tying you to a\\nparticular runtime.\\nNow you might be wondering how you can take advantage of existing\\nmodels in your own projects. In the rest of this section, you’ll convert\\nexisting models from popular frameworks into the ONNX format for use\\nwithin Elixir.\\nExporting Models from TensorFlow\\nTensorFlow’s blessed serialized model format is the Saved Model,\\nthough some older versions of TensorFlow Keras supported a variety of\\nserialization formats for models and weights. This section focuses\\nspecifically on the saved model format.\\nUsing tf2onnx\\ntf2onnx  is a Python library for converting TensorFlow models to the\\nONNX format. tf2onnx supports TensorFlow saved models, TensorFlow JS\\nmodels, and TensorFlow Lite models. The easiest way to use tf2onnx is\\n[19]\\n[20]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 393}),\n",
       " Document(page_content='through the CLI. For example, if you have a TensorFlow saved model in a\\ndirectory, you can run the following to convert it to an ONNX model:\\n$ python3 -m tf2onnx.convert  \\\\ \\n  --saved-model path-to-model  \\\\ \\n  --output model.onnx\\nTo see tf2onnx in action, create a new Python file named\\nsave_tensorﬂow_model.py and copy the following code into a newly created\\nfile:\\nimport   tensorflow   as   tf \\nmodel = tf.keras.applications.ResNet50(weights= \\n\"imagenet\" )\\nmodel.save( \"resnet\" )\\nSave the file you copied the code into as save_tensorﬂow_model.py. Then, run\\nthe script with this:\\n$ python3 save_tensorflow_model.py\\nThis script imports a pre-trained ResNet50 model using the\\ntf.keras.applications submodule. There are a number of other pre-trained\\nmodels you can use under the Keras Applications module. After[21]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 394}),\n",
       " Document(page_content='importing ResNet50, the script saves the model using the saved model\\nformat to a directory resnet. You’ll see the resnet directory in your working\\ndirectory after running this script.\\nNext, run the following command to convert your model to ONNX:\\n$ python3 -m tf2onnx.convert  \\\\ \\n  --saved-model resnet/  \\\\ \\n  --output resnet_tensorflow.onnx\\nAfter you run this script, you’ll see the resnet_tensorﬂow.onnx file appear in\\nyour current working directory.\\nNow, create a new Elixir script named resnet.exs and copy the following\\ncode into the newly created file:\\nMix.install([\\n  { :axon_onnx ,  \"  ~> 0.4\" },\\n  { :axon ,  \"  ~> 0.5\" }\\n])\\n{model, params} = AxonOnnx. import ( \"  \\nresnet_tensorflow.onnx\" )\\nIO.inspect model', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 395}),\n",
       " Document(page_content='Then, in the terminal, run the following command:\\n$ elixir resnet.exs\\nRunning this script installs Axon and AxonOnnx and imports the exported\\nResNet model into the Axon model format before inspecting the model.\\nAfter running the command, you’ll see the following output:\\n09:36:43.537 [warning] unk__613 has no specified \\ndimension, assuming nil\\n#Axon<\\n  inputs: [\"input_1\"]\\n>\\nThis output indicates that your model is ready for use from Elixir. The\\nlogged warning indicates that one of the dimensions has an unspecified\\nsize, so Axon filled it in as nil for you. In this case, the unk__613 is the\\nmodel’s batch dimension. If you see warnings like this and end up with\\nerrors during import, you can usually get rid of them by providing explicit\\nvalues for these dimensions when importing your model:\\n{model, params} = AxonOnnx. import ( \"  \\nresnet_tensorflow.onnx\" ,  unk__613:  32)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 396}),\n",
       " Document(page_content='Once you pass an explicit value for a dimension, you must stick to using\\ndimension sizes with the same value.\\nFinding Pre-trained Tensorflow Models\\nWhile this example made use of tf.keras.applications, you can find\\nTensorFlow saved models in some other places too. TensorFlow Hub is a\\nTensorFlow-specific repository of pre-trained models for a variety of tasks.\\nYou can find models which are in a variety of different formats. Generally,\\nmost models are supported by tf2onnx, though you might need to dig into\\nthe tf2onnx documentation to ensure models export correctly from the\\nvariety of formats supported on TensorFlow Hub.\\nAs you’ll see in  Exporting Models from HuggingFace , you can also export\\nsaved models yourself from high-level application-specific libraries such as\\nHuggingFace transformers.\\nExporting Models from PyTorch\\nBecause ONNX is an open collaborative between Microsoft and Meta, and\\nPyTorch is a product of Meta’s AI laboratory, PyTorch has first-class\\nsupport for exporting models to ONNX. The torch.onnx module\\nprovides functions for exporting models to ONNX. The documentation\\nprovides an in-depth overview of how to export models and some of the\\ncommon sharp edges encountered when exporting models. Given a pre-\\n[22]\\n[23]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 397}),\n",
       " Document(page_content='trained PyTorch module, you can typically export without issue using\\ntorch.onnx.export:\\nimport   torch \\ndummy_input = torch.randn(10, 3, 224, 224)\\ntorch.onnx.export(model, dummy_input,  \"model.onnx\" \\n)\\nAll you need to do is provide an explicit input for PyTorch to use to build\\nthe ONNX graph. To see this in action, create a new Python script named\\nsave_pytorch_model.py and copy the following code into the newly created\\nfile:\\nimport   torch \\nimport   torchvision \\ndummy_input = torch.randn(10, 3, 224, 224)\\nmodel = \\ntorchvision.models.resnet50(pretrained=True)\\ntorch.onnx.export(model, dummy_input,  \\n\"resnet_pytorch.onnx\" )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 398}),\n",
       " Document(page_content='Then run the script with this:\\n$ python3 save_pytorch_model.py\\nThis script uses both PyTorch and TorchVision  to import a pre-trained\\nResNet and save it to the ONNX format using torch.onnx.export. After\\nrunning the script, you’ll see resnet_pytorch.onnx in your current working\\ndirectory. Next, change your resnet.exs script to import your PyTorch\\nversion of the ResNet ONNX model:\\nMix.install([\\n  { :axon_onnx ,  \"  ~> 0.4\" },\\n  { :axon ,  \"  ~> 0.5\" }\\n])\\n{model, params} = AxonOnnx. import ( \"  \\nresnet_pytorch.onnx\" )\\nIO.inspect model\\nThen, run your script:\\n$ elixir resnet.exs\\nAnd you’ll see this output:\\n[24]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 399}),\n",
       " Document(page_content='#Axon<\\n  inputs: [\"input.1\"]\\n>\\nThis output indicates that you’ve successfully imported a pre-trained model\\nfrom PyTorch for use within Elixir.\\nFinding Pre-trained PyTorch Models\\nSimilar to TensorFlow, there are a number of places to go for finding pre-\\ntrained models in PyTorch. For example, there are application-specific\\nlibraries, such as TorchVision,  TorchText,  and TorchAudio with\\nsome pre-trained models available for export.\\nGenerally, if you’re able to find a PyTorch implementation of a pre-trained\\nmodel on GitHub, you should be able to export the model to the ONNX\\nformat. As the PyTorch documentation mentions, there are a number of\\nconsiderations when exporting models to ONNX, and not all\\nimplementations are written with these considerations in mind. You might\\nhave to make some minor modifications to certain models, but ONNX\\ncovers a significant percentage of the models you’d be interested in\\nexporting.\\nExporting Models from HuggingFace\\n[25] [26] [27]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 400}),\n",
       " Document(page_content='Perhaps the most popular machine learning library in existence right now is\\nHuggingFace Transformers. This library provides conveniences and pre-\\ntrained implementations of transformer models. You’ll use transformer\\nmodels in Chapter 11,  Model Everything with Transformers . HuggingFace\\nalso has a pre-trained model hub that hosts models from a large number of\\norganizations and individuals. Elixir has its own library which interfaces\\ndirectly with the HuggingFace Hub known as Bumblebee. Bumblebee\\nprovides native Elixir implementations of models, so they’re a bit cleaner\\nand more robust than the auto-imported versions you’d get from ONNX. If\\nyou want to use a model not available in Bumblebee, it’s possible to export\\nsome transformer models from HuggingFace using the transformers.onnx\\nPython module.\\nAssuming you have the transformers library installed, you only need to run\\na command like this from the CLI to extract an ONNX model from an\\nimplementation on HuggingFace:\\n$ python3 -m transformers.onnx --model=org/model \\npath\\norg/model points to the specific model you’d like to export. Only a subset of\\nall models on the hub are officially supported. To see how exporting\\nfrom transformers works hands-on, run the following command to export a\\npre-trained ResNet model from Microsoft:\\n[28]\\n[29]\\n[30]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 401}),\n",
       " Document(page_content='$ python3 -m transformers.onnx --\\nmodel=microsoft/resnet-50 resnet/\\nThis code exports a pre-trained ResNet model with weights provided by\\nMicrosoft to the path resnset/model.onnx. Now you can modify your\\nresnet.exs to import this model:\\nMix.install([\\n  { :axon_onnx ,  \"  ~> 0.4\" },\\n  { :axon ,  \"  ~> 0.5\" }\\n])\\n{model, params} = AxonOnnx. import ( \"  \\nresnet/model.onnx\" )\\nIO.inspect model\\nAnd then, run the script:\\n$ elixir resnet.exs\\nAfter running the script, you’ll see the following output:\\n13:40:05.534 [warning] batch has no specified \\ndimension, assuming nil', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 402}),\n",
       " Document(page_content='13:40:05.538 [warning] sequence has no specified \\ndimension, assuming nil\\n#Axon<\\n  inputs: [\"pixel_values\"]\\n>\\nThis output means that the import was successful. You now know how to\\nimport models from TensorFlow and PyTorch and how to take advantage of\\nthe largest model repository in existence from Elixir. This should set you on\\na path for using almost any pre-trained model you can find without having\\nto jump through too many hoops.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 403}),\n",
       " Document(page_content='[16]\\n[17]\\n[18]\\nWrapping Up\\nIn this chapter, you used pre-trained models to significantly improve your\\nmodel performance over one trained from scratch. You made use of pre-\\ntrained models in Axon for feature extraction and fine-tuning, and you\\nlearned how to take most models from the Python ecosystem and use them\\nin Elixir. Moving forward, you should always ask yourself if training a\\nmodel from scratch is worth it, or if you can benefit from using a pre-\\ntrained model. More often than not, using a pre-trained model will save you\\ntime and improve the performance of your models.\\nIn the previous few chapters, you learned quite a bit about deep learning\\nand Axon. But you’ve been primarily focused on image data. Fortunately,\\nmany of the lessons you’ve learned in these chapters still apply, but there\\nare some additional considerations when working with certain kinds of data.\\nIn the next two chapters, you’ll apply deep learning to different types of\\nsequential data—namely text and time-series data.\\nFOOTNOTES\\nhttps://github.com/onnx/onnx\\nhttps://www.image-net.org/\\nhttps://github.com/onnx/models/blob/main/vision/classification/mobilenet/model/mobilenetv\\n2-7.onnx', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 404}),\n",
       " Document(page_content='[19]\\n[20]\\n[21]\\n[22]\\n[23]\\n[24]\\n[25]\\n[26]\\n[27]\\n[28]\\n[29]\\n[30]\\nhttps://www.tensorflow.org/guide/saved_model\\nhttps://github.com/onnx/tensorflow-onnx\\nhttps://keras.io/api/applications/\\nhttps://tfhub.dev/\\nhttps://pytorch.org/docs/stable/onnx.html\\nhttps://pytorch.org/vision/stable/index.html\\nhttps://pytorch.org/vision/stable/index.html\\nhttps://pytorch.org/text/stable/models.html\\nhttps://pytorch.org/audio/stable/models.html\\nhttps://huggingface.co/docs/transformers/index\\nhttps://github.com/elixir-nx/bumblebee\\nhttps://huggingface.co/docs/transformers/serialization\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 405}),\n",
       " Document(page_content='Chapter 9\\nUnderstand Text\\n\\xa0\\nIn the past few chapters, the problems you solved with neural networks\\nhave dealt exclusively with image data. While deep learning has had a\\nsignificant effect on the field of computer vision, deep learning is versatile\\nenough for use in a number of other domains. As a budding machine\\nlearning engineer, it’s important to understand how to apply deep learning\\nto solve different problems in different domains.\\nNatural language processing is one field that has significantly benefitted\\nfrom the rise of deep learning. As a subfield of artificial intelligence and\\ncomputer science, natural language processing deals with human language.\\nIf your problem deals with text, you’re probably going to be doing some\\nnatural language processing.\\nNeural networks have resulted in significant breakthroughs in every\\nlanguage-understanding task, from text classification to question answering\\nto machine translation. While most of these breakthroughs have come as a\\nresult of ever larger transformer models, it’s still important to understand\\nsome of the foundational approaches to sequence processing—namely\\nrecurrent neural networks (RNNs). RNNs are a class of deep learning', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 406}),\n",
       " Document(page_content='models used specifically for processing sequences. In this chapter, you’ll\\nlearn what RNNs are, why they’re better than traditional feed-forward\\nnetworks for sequence processing, and some of the pitfalls that lead to the\\nrise of transformers.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 407}),\n",
       " Document(page_content='Classifying Movie Reviews\\nImagine you work for a movie theater, and your task is to decide which\\nmovies to show based on expert reviews about the movies. While you could\\nspend the time sifting through all of these reviews, it’s both time-consuming\\nand tedious. Instead, you decide to create an algorithm that classifies movie\\nreviews as positive or negative and then show only the movies with the\\nmost positive reviews. This problem is a textbook application of sentiment\\nanalysis.\\nThe goal of sentiment analysis is to extract sentiment from natural\\nlanguage. Sentiment is simply the attitude of a given text. In other words,\\nsentiment analysis is concerned with extracting the overall attitude or\\nopinion of a given text. Typically, sentiment analysis deals with discrete\\nlabels, such as positive, negative, or neutral, as well as the sentiment of the\\ntext in the abstract rather than sentiment towards a certain topic. More\\nadvanced sentiment analysis tasks might focus on the degree of positivity or\\nnegativity of a text and also the object of the opinion.\\nFor this problem, you’re concerned with the sentiment of a given review\\ntoward a given movie. This is the ideal sentiment analysis problem—the\\nobject of the opinion is clear, and the reviews are explicitly meant to convey\\nan opinion about a subject. To train your model, you’ll be using the IMDB\\nMovie Reviews Dataset. The IMDB Movie Reviews Dataset contains[31]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 408}),\n",
       " Document(page_content='25,000 training reviews and 25,000 testing reviews. Each example includes\\nan English language review and a sentiment label, such as this:\\nreview = \"The Departed is Martin Scorsese\\'s best \\nwork, and anybody\\nwho disagrees is wrong. This movie is amazing.\"\\nsentiment = 1\\nTo get started, fire up a Livebook and add the following dependencies:\\nMix.install([\\n  { :scidata ,  \"  ~> 0.1\" },\\n  { :axon ,  \"  ~> 0.5\" },\\n  { :exla ,  \"  ~> 0.6\" },\\n  { :nx ,  \"  ~> 0.6\" },\\n  { :table_rex ,  \"  ~> 3.1.1\" },\\n  { :kino ,  \"  ~> 0.7\" }\\n])\\nNothing unexpected here. You’re using SciData to download the IMDB\\ndataset, Axon to create and train your neural network, EXLA to accelerate\\nyour computations, and Nx for some basic numerical computation and\\npreprocessing. As a final bit of set up, run the following code to set EXLA\\nas your default Nx backend:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 409}),\n",
       " Document(page_content='Nx.default_backend(EXLA.Backend)\\nYou know the drill. With your setup complete, it’s time to move on to the\\ndata.\\nGetting the Data\\nThe IMDB Review Dataset is available as a module in SciData. Download\\nthe data by running the following code:\\ndata = Scidata.IMDBReviews.download()\\nAfter running that code, you’ll see the following output:\\n%{\\n  review: [\\n    \"The story centers around Barry McKenzie who \\nmust go to England...,\\n    \"\\'The Adventures Of Barry McKenzie\\' started \\nlife as a satirical...,\\n    ...\\n  ],\\n  sentiment: [1, 1, 1, 1, 1, 1, 1, 1, 1, ...]\\n}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 410}),\n",
       " Document(page_content='Before continuing, you’ll want to split the dataset into a training and test\\nset:\\n{train_data, test_data} =\\n  data.review\\n  |> Enum.zip(data.sentiment)\\n  |> Enum.shuffle()\\n  |> Enum.split(23_000)\\nThe dataset is a map of reviews and sentiment, with corresponding entries\\nmapping to the same review. At this point, it might be unclear how you can\\nmap this raw text to numeric data. Extracting an informative numerical\\nrepresentation of text is a key challenge in natural language processing.\\nThere are many approaches. Perhaps the most common approach in deep\\nlearning is to tokenize and then vectorize the text.\\nTokenization is the process of splitting text into discrete units known as\\ntokens. You might be familiar with tokenization in the context of\\nprogramming languages. Compilers generally tokenize a program into a\\nsequence of tokens. The tokenization process is generally much easier\\nbecause the rules of a programming language are well-defined. On the other\\nhand, natural language is unstructured and differs in significant ways\\nbetween languages. This implies that there are no universally superior\\ntokenization strategies, but there are a few common ones.\\nTokenizing on Whitespace', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 411}),\n",
       " Document(page_content='One of the most common tokenization strategies is to split text at each\\nwhitespace character. This is especially common in languages like English\\nwhere whitespace is a semantically meaningful separator. This natural\\nseparation makes it easy to identify distinct tokens. Tokenizing on\\nwhitespace is not necessarily a good option for all types of text though. For\\nexample, Chinese doesn’t give any significance to whitespace characters, so\\ntokenizing on whitespace doesn’t make any sense. An additional drawback\\nis that whitespace tokenization leads to a large vocabulary of tokens.\\nDepending on the corpus, the potential vocabulary size is unbounded.\\nTokenizing on Characters\\nAnother common strategy is to tokenize at the character level. In other\\nwords, each character, including whitespace, represents a distinct token.\\nThis is useful in a multilingual context, as you are able to capture distinct\\ntokens across languages. Tokenizing at the character level also limits the\\nsize of your vocabulary, though tokens from character-based languages will\\nend up representing a significant portion of your final vocabulary. One\\ndrawback of tokenizing at the character level, especially for languages like\\nEnglish, is that the length of your sequence increases significantly, which\\nhas significant disadvantages during training and inference. You’ll learn\\nmore about these disadvantages later in  Understanding Recurrent Neural\\nNetworks .\\nTokenizing on Bytes', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 412}),\n",
       " Document(page_content='Perhaps the simplest tokenization strategy is to treat each byte in a text\\nsequence as an individual token. Similar to character-level tokenization\\nstrategies, this strategy is great for multilingual contexts. Additionally, the\\nsize of your vocabulary is fixed to 255. A lot of modern natural language\\nalgorithms tokenize at the byte level. Another advantage is that tokenization\\nis simple—you only need to treat the raw text as a byte-string. Of course,\\nthis approach has the same shortcomings as tokenizing at the character\\nlevel. The sequence length balloons and presents problems during training\\nand inference.\\nTokenizing on Pieces\\nModern tokenization algorithms take a probabilistic approach and tokenize\\non pieces of sentences and words. Most modern algorithms employ a\\nstrategy based on byte-pair encoding (BPE). They typically employ a\\npretokenization strategy, which may utilize one of the naive approaches\\npresented in this chapter, to build a frequency map of words or tokens.\\nThen, the algorithm splits each word in the frequency map into pieces and\\nextracts the most likely pieces or chunks from the frequency maps. You’ll\\nsee these more advanced tokenizers in action in Chapter 11,  Model\\nEverything with Transformers .\\nTokenizing and Vectorizing in Elixir\\nGiven that all of your reviews are in English, it makes sense to tokenize on\\nwhitespace. If you’re familiar with Elixir’s String module, you know this', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 413}),\n",
       " Document(page_content='can be accomplished with a simple String.split/2. Before you can go about\\nsplitting strings, you need to do some text normalization. If you were to\\nsplit the text as is, you would end up with tokens that treat punctuation and\\ncasing as significant. In some problems, this makes sense. But in this\\nproblem, it doesn’t. Notice that the following sentences have the same\\nsentiment, regardless of punctuation and casing:\\ni didnt like this movie it was so bad nobody \\nshould ever watch it\\nI didn\\'t like this movie. It was so bad; nobody \\nshould ever watch it.\\nIn terms of sentiment, these sentences have identical meanings, and you\\nwant your model to understand that. If you went about splitting this\\nsentence on whitespace, you’d end up with the following tokens:\\n[\"i\", \"didnt\", \"like\", \"this\", \"movie\", \"it\", \\n\"was\", \"so\", \"bad\",\\n\"nobody\", \"should\", \"ever\", \"watch\", \"it\"]\\n[\"I\", \"didn\\'t\", \"like\", \"this\", \"movie.\", \"It\", \\n\"was\", \"so\", \"bad;\",\\n\"nobody\", \"should\", \"ever\", \"watch\", \"it.\"]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 414}),\n",
       " Document(page_content='Notice how many of the tokens in your sentence, which you know have the\\nsame meaning, end up getting treated differently by your model. Also,\\nwords with different casing get treated as different words. To fix these\\nproblems, you need to convert every character in the sentence to lowercase\\nand remove the punctuation from each review. Fortunately, you can do this\\nusing String.downcase/1 and String.replace/3:\\nreview =  \"  I didn\\'t like this movie. \\nIt was so bad; nobody should ever watch it.\" \\nreview |> String.downcase() |> String.replace( \\n~r/[\\\\p{P}\\\\p{S}]/ ,  \"  \" )\\nRunning this code will first downcase all of the characters in the given\\nbinary and then replace all of the punctuation in the string with an empty\\nstring, effectively removing it from the sequence. If you inspect the output\\nnow, you’ll see the following:\\ni didnt like this movie it was so bad nobody \\nshould ever watch it', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 415}),\n",
       " Document(page_content='Punctuation Matters\\nEnglish teachers are probably cringing at the assertion that\\npunctuation means nothing to a machine learning engineer.\\nYou also can probably come up with any number of\\ncounterexamples where punctuation is significant to the\\nsentiment of a piece of text. But in most cases it won’t make a\\ndifference. You’re not trying to design a model to catch every\\nedge case here; you want something that handles only the\\ncommon case.\\nGiven what you’ve learned so far, you can implement a tokenization\\nstrategy that looks something like this:\\nreview\\n|> String.downcase()\\n|> String.replace( ~r/[\\\\p{P}\\\\p{S}]/ ,  \"  \" )\\n|> String.split()\\nBut, if you run this code, you will end up with only a list of tokens—the\\nindividual tokens don’t map naturally to entries in a tensor. Instead, you\\nneed to apply a vectorization strategy. In this context, vectorization is any\\ntechnique that converts the tokens you have into a vector. For example, you\\ncan replace each token with a vector from a pre-trained word embedding.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 416}),\n",
       " Document(page_content='Word embeddings map tokens to dense vector representations and give\\nmathematical meaning to natural language. The quintessential example of\\nmathematical relationships in word embeddings is that if you were to add\\nthe vector representations of the tokens “king” and “girl,” the resulting\\nvector would be very close to the vector for the word “queen.”\\nAnother approach is to convert each token to a sparse representation using\\none-hot encoding or as a simple index in a lookup table. The advantage of\\nthis approach over a pre-trained embedding is that you’re able to learn a\\nsmaller specialized vocabulary and embedding.\\nSo how can you turn your tokens into a sparse representation? All you need\\nto do is map each unique token in your input to a unique integer index. But\\nremember that one of the drawbacks of your tokenization strategy is that the\\nvocabulary can grow incredibly large, so you’ll want to limit the vocabulary\\nto a certain size. You can do this by keeping the most frequent tokens in the\\ncorpus and discarding the rest. You can use the following code to get the\\nfrequencies of each token in the corpus:\\nfrequencies =\\n  Enum.reduce(train_data, %{},  fn  {review, _}, \\ntokens ->\\n    review\\n    |> String.downcase()\\n    |> String.replace( ~r/[\\\\p{P}\\\\p{S}]/ ,  \"  \" )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 417}),\n",
       " Document(page_content='   |> String.split()\\n    |> Enum.reduce(tokens, &Map.update(&2, &1, 1,  \\nfn  x -> x + 1  end ))\\n   end )\\nNotice that you tokenize each document and then accumulate token counts\\nin a frequency map. If you run this code, you’ll see something like this:\\n%{\\n  \"diplomaswho\" => 1,\\n  \"egyptologists\" => 1,\\n  \"characther\" => 1,\\n  \"nearlyempty\" => 1,\\n  \"blobs\" => 4,\\n  \"doubletwist\" => 1,\\n  \"thingshe\" => 1,\\n  \"loleralacartelort7890\" => 1,\\n  \"placebo\" => 1,\\n  \"betterif\" => 1,\\n  \"smarttalk\" => 1,\\n  \"sorcererin\" => 1,\\n  \"celies\" => 6,\\n  \"tenancier\" => 1,\\n  \"ladies\" => 284,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 418}),\n",
       " Document(page_content=' ...\\n}\\nYou can now create a vocabulary that only keeps the top tokens in your\\nfrequency map by sorting the inputs and keeping the top N most frequent\\ntokens:\\nnum_tokens = 1024\\ntokens =\\n  frequencies\\n  |> Enum.sort_by(&elem(&1, 1),  :desc )\\n  |> Enum.take(num_tokens)\\nThe number of tokens in your vocabulary is arbitrary. This example uses\\n1024 to limit the size of the final model. You still need to map each of the\\ntokens in your vocabulary to a unique integer. The value of a single token\\nrepresents a categorical variable without any significance in order or\\nmagnitude. In other words, you can assign any token to any integer as long\\nas each unique token gets a unique integer. The easiest way to do this is to\\nassign the index of each token in the vocabulary as its value:\\ntokens =\\n  frequencies\\n  |> Enum.sort_by(&elem(&1, 1),  :desc )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 419}),\n",
       " Document(page_content=' |> Enum.take(num_tokens)\\n  |> Enum.with_index( fn  {token, _}, i -> {token, \\ni}  end )\\n  |> Map.new()\\nYou can combine this vocabulary with your tokenization strategy to map\\neach review to a list of integers:\\nreview =  \"  The Departed is Martin Scorsese\\'s best \\nwork, and anybody \\nwho disagrees is wrong. This movie is amazing.\" \\ntokenize =  fn  review ->\\n  review\\n  |> String.downcase()\\n  |> String.replace( ~r/[\\\\p{P}\\\\p{S}]/ ,  \"  \" )\\n  |> String.split()\\n  |> Enum.map(&Map.get(tokens, &1))\\nend \\ntokenize.(review)\\nThis code tokenizes the inputs and then looks up each token in the token\\nmap you’ve prebuilt. If you run this code, you’ll see the following output:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 420}),\n",
       " Document(page_content='[1, nil, 6, nil, nil, 111, 159, 2, nil, 36, nil, \\n6, 361, 10, 18, 6, 465]\\nBut wait, there are a bunch of nil values. You can’t turn this into a tensor.\\nThe issue, here, is by limiting the size of your vocabulary, you’ve created\\nthe possibility of encountering out-of-vocab (OOV) tokens. This is a\\ncommon outcome when dealing with text and is easily solved by\\nintroducing a special value to represent OOV tokens. It’s common for these\\nspecial tokens to occupy the beginning of your vocabulary, so you need to\\nslightly adjust your tokens and vectorization strategy:\\nreview =  \"  The Departed is Martin Scorsese\\'s best \\nwork, and anybody \\nwho disagrees is wrong. This movie is amazing.\" \\nunknown_token = 0\\ntokens =\\n  frequencies\\n  |> Enum.sort_by(&elem(&1, 1),  :desc )\\n  |> Enum.take(num_tokens)\\n  |> Enum.with_index( fn  {token, _}, i -> {token, i \\n+ 1}  end )\\n  |> Map.new()', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 421}),\n",
       " Document(page_content='tokenize =  fn  review ->\\n  review\\n  |> String.downcase()\\n  |> String.replace( ~r/[\\\\p{P}\\\\p{S}]/ ,  \"  \" )\\n  |> String.split()\\n  |> Enum.map(&Map.get(tokens, &1, unknown_token))\\n  |> Nx.tensor()\\nend \\ntokenize.(review)\\nHere, you shift your vocabulary values to the right by one and provide a\\ndefault value to Map.get/3. You also wrap the final tokenized list into an Nx\\ntensor. After running this code, you’ll see this:\\n#Nx.Tensor<\\n  s64[17]\\n  EXLA.Backend<host:0, \\n0.2532065982.2520645648.98884>\\n  [1, 0, 6, 0, 0, 114, 160, 2, 0, 36, 0, 6, 358, \\n10, 17, 6, 472]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 422}),\n",
       " Document(page_content='Perfect! Now you can create an input pipeline that you can pass to Axon for\\ntraining a model. To implement your input pipeline, copy the following\\ncode:\\nbatch_size = 64\\ntrain_pipeline =\\n  train_data\\n  |> Stream.map( fn  {review, label} ->\\n    {tokenize.(review), Nx.tensor(label)}\\n   end )\\n  |> Stream.chunk_every(batch_size, batch_size,  \\n:discard )\\n  |> Stream.map( fn  reviews_and_labels ->\\n    {review, label} = \\nEnum.unzip(reviews_and_labels)\\n    {Nx.stack(review), Nx.stack(label) |> \\nNx.new_axis(-1)}\\n   end )\\ntest_pipeline =\\n  test_data\\n  |> Stream.map( fn  {review, label} ->\\n    {tokenize.(review), Nx.tensor(label)}\\n   end )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 423}),\n",
       " Document(page_content=' |> Stream.chunk_every(batch_size, batch_size,  \\n:discard )\\n  |> Stream.map( fn  reviews_and_labels ->\\n    {review, label} = \\nEnum.unzip(reviews_and_labels)\\n    {Nx.stack(review), Nx.stack(label) |> \\nNx.new_axis(-1)}\\n   end )\\nThis code should feel similar to your input pipeline from the past few\\nchapters. First, you zip reviews and sentiments, and then you do some\\npreprocessing to convert reviews and sentiments to tensors. Next, you\\nchunk adjacent entries in the Stream and stack them together to create\\nbatches of sequence and sentiment pairs. Now, you can run the following\\ncode to grab one batch from your input stream:\\nEnum.take(train_pipeline, 1)\\nAnd you’ll see this:\\n** (ArgumentError) non-concat dims must be equal \\ngot 127 and 304\\nwhile concatenating on axis 0', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 424}),\n",
       " Document(page_content='What happened here? Well, if you were playing around with tokenizing\\ndifferent reviews, you might have noticed that each review has a different\\ninput length. Calling Nx.stack/1 doesn’t work because it would result in\\nwhat’s called a ragged tensor—a tensor with nonuniform sizes for different\\nentries in a given dimension. This problem arises often when dealing with\\nsequences because they often have variable input lengths.\\nYou had a similar problem when dealing with images, which was easily\\naddressed by resizing the images to uniform shapes. Resizing in the context\\nof images is a bit more natural because you can downsample, upsample,\\ncrop, or pad an input image, and it won’t necessarily drastically change the\\ncontent of the original image. Sequences, on the other hand, are a bit more\\ndifficult because it’s possible to truncate a sequence and lose a lot of the\\noriginal meaning of the sequence, so you need to be careful about choosing\\nan appropriate sequence length.\\nWhile some frameworks support working with ragged tensors, Nx requires\\nstatic shapes and doesn’t support ragged tensors, so you need a strategy to\\nconvert all of your input reviews to a uniform shape. The most common\\nway to do this is by padding or truncating each sequence to a fixed length.\\nPadding is normally done by introducing a pad token, which is another\\nspecial token in your vocabulary. You append each sequence with the\\nnumber of pad tokens required to reach the desired sequence length.\\nPadding the Input Sequences', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 425}),\n",
       " Document(page_content='To introduce padding, you need to alter your original tokenization code:\\npad_token = 0\\nunknown_token = 1\\nmax_seq_len = 64\\ntokens =\\n  frequencies\\n  |> Enum.sort_by(&elem(&1, 1),  :desc )\\n  |> Enum.take(num_tokens)\\n  |> Enum.with_index( fn  {token, _}, i -> {token, i \\n+ 2}  end )\\n  |> Map.new()\\ntokenize =  fn  review ->\\n  review\\n  |> String.downcase()\\n  |> String.replace( ~r/[\\\\p{P}\\\\p{S}]/ ,  \"  \" )\\n  |> String.split()\\n  |> Enum.map(&Map.get(tokens, &1, unknown_token))\\n  |> Nx.tensor()\\n  |> then(&Nx.pad(&1, pad_token, [{0, max_seq_len \\n- Nx.size(&1), 0}]))', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 426}),\n",
       " Document(page_content='end \\nIn this code, notice the addition of the pad token and how you shift each\\ntoken index to the right again. But the real magic happens with this line:\\n|> then(&Nx.pad(&1, pad_token, [{0, max_seq_len - \\nNx.size(&1), 0}]))\\nThis line adds padding with the value pad_token to the end of the sequence.\\nThe amount of padding is determined by max_seq_len - Nx.size(sequence),\\nwhich means it will add enough padding for the original input sequence to\\nbe of length max_seq_len. You might be wondering what happens if the size\\nof the original input sequence is greater than max_seq_len. In that case, the\\npadding at the end of the sequence will be negative, and the semantics of\\nNx.pad/3 dictate the input tensor is truncated by the negative padding\\namount. This line works to pad or truncate your sequence to a fixed length\\nregardless of the input size.\\nWith the introduction of padding, you can reimplement and rerun your\\ntraining pipeline:\\nbatch_size = 64\\ntrain_pipeline =\\n  train_data', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 427}),\n",
       " Document(page_content=' |> Stream.map( fn  {review, label} ->\\n    {tokenize.(review), Nx.tensor(label)}\\n   end )\\n  |> Stream.chunk_every(batch_size, batch_size,  \\n:discard )\\n  |> Stream.map( fn  reviews_and_labels ->\\n    {review, label} = \\nEnum.unzip(reviews_and_labels)\\n    {Nx.stack(review), Nx.stack(label) |> \\nNx.new_axis(-1)}\\n   end )\\ntest_pipeline =\\n  test_data\\n  |> Stream.map( fn  {review, label} ->\\n    {tokenize.(review), Nx.tensor(label)}\\n   end )\\n  |> Stream.chunk_every(batch_size, batch_size,  \\n:discard )\\n  |> Stream.map( fn  reviews_and_labels ->\\n    {review, label} = \\nEnum.unzip(reviews_and_labels)\\n    {Nx.stack(review), Nx.stack(label) |> \\nNx.new_axis(-1)}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 428}),\n",
       " Document(page_content='  end )\\nEnum.take(train_pipeline, 1)\\nAnd you’ll see this:\\n[\\n  {#Nx.Tensor<\\n     s64[64][64]\\n     EXLA.Backend<host:0, \\n0.3646858574.2605056008.58802>\\n     [\\n       [10, 208, 1, 57, 1, 1, 8, ...],\\n       ...\\n     ]\\n   >,\\n   #Nx.Tensor<\\n     s64[64][1]\\n     EXLA.Backend<host:0, \\n0.3646858574.2605056008.58868>\\n     [\\n       [0],\\n       [0],\\n       [1],', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 429}),\n",
       " Document(page_content='      ...\\n     ]\\n   >}\\n]\\nYou’ve successfully put together an input pipeline for variable length\\nsequences using a simple tokenization and vectorization strategy. Now, all\\nyou need is a model to train.\\nTrying to Read with MLPs\\nBefore diving into the complex world of sequence processing with recurrent\\nneural networks, you should start with something simple—like a basic feed-\\nforward neural network:\\nmodel =\\n  Axon.input( \"  review\" )\\n  |> Axon.embedding(num_tokens + 2, 64)\\n  |> Axon.flatten()\\n  |> Axon.dense(64,  activation:   :relu )\\n  |> Axon.dense(1)\\nThe only unfamiliar layer here should be Axon.embedding/3. This layer takes\\na sparse collection of tokens like the ones you have in each of your', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 430}),\n",
       " Document(page_content='sequences and maps them to a dense vector representation. The embedding\\noperation is a relatively simple lookup, like the one depicted here:\\nEmbeddings learn dense vector representations of text in your dataset. After\\nembedding, you end up with a sequence of vectors or a three-dimensional\\ninput, so you need to flatten the input before passing the resulting\\nembedding matrix to the next dense layer. Because your goal is to predict a\\nbinary label, your output layer is a dense layer with one output unit.\\nYou could inspect this model as a table with the following code:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 431}),\n",
       " Document(page_content='input_template = Nx.template({64, 64},  :s64 )\\nAxon.Display.as_graph(model, input_template)\\nIf you did so, after running the code, you’ll see this:\\nYou can now train your model using the Axon.Loop API. Copy and run the\\nfollowing code to implement a supervised training loop using your model\\nand input pipeline:\\nloss = &Axon.Losses.binary_cross_entropy(&1, &2,\\n   from_logits:  true,\\n   reduction:   :mean \\n)\\noptimizer = Axon.Optimizers.adam(1.0e-4)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 432}),\n",
       " Document(page_content='trained_model_state =\\n  model\\n  |> Axon.Loop.trainer(loss, optimizer)\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.run(train_pipeline, %{},  epochs:  \\n10,  compiler:  EXLA)\\nIn this example, you pass in a parametrized form of\\nAxon.Losses.binary_cross_entropy/3. You’ve used binary cross-entropy in the\\npast so you should be familiar with it by now. The difference here is you\\nneed to specify from_logits: true. You might have noticed that your output\\ndense layer didn’t have an activation function. Previously, for binary\\nclassification problems, you had used a sigmoid activation to specify that a\\nlayer should output a probability between 0 and 1. In this instance, your\\noutput layer is going to learn to output logits or log-probabilities. This\\nmeasure is literally only supposed to represent log(p) where p is an output\\nprobability.\\nTraining with or without logits doesn’t usually make a difference. Some\\nloss implementations are more stable when using output logits vs.\\nprobabilities. But Axon’s implementations use an optimization that always\\ntakes advantage of the stable path, so training with or without logits is a\\nmatter of personal preference.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 433}),\n",
       " Document(page_content='After training, you’ll see the following output:\\nEpoch: 0, Batch: 350, accuracy: 0.5002229 loss: \\n0.6860522\\nEpoch: 1, Batch: 350, accuracy: 0.6894144 loss: \\n0.6168785\\nEpoch: 2, Batch: 350, accuracy: 0.7786677 loss: \\n0.5573396\\nEpoch: 3, Batch: 350, accuracy: 0.8037750 loss: \\n0.5174768\\nEpoch: 4, Batch: 350, accuracy: 0.8238515 loss: \\n0.4877387\\nEpoch: 5, Batch: 350, accuracy: 0.8453078 loss: \\n0.4631703\\nEpoch: 6, Batch: 350, accuracy: 0.8648504 loss: \\n0.4411991\\nEpoch: 7, Batch: 350, accuracy: 0.8850605 loss: \\n0.4204423\\nEpoch: 8, Batch: 350, accuracy: 0.9082086 loss: \\n0.4002065\\nEpoch: 9, Batch: 350, accuracy: 0.9299769 loss: \\n0.3802384\\nOverall, your model performs decently but struggles to achieve over 90%\\ntraining accuracy. You can see how this translates to the test set by running', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 434}),\n",
       " Document(page_content='the following code:\\nmodel\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )\\n|> Axon.Loop.run(test_pipeline, \\ntrained_model_state,  compiler:  EXLA)\\nAfter running the code, you’ll see this:\\nBatch: 30, accuracy: 0.7676411\\nOverall, your model achieves 76% accuracy on the test set. This isn’t bad,\\nbut you can definitely do better with a model designed for processing\\nsequences. Enter the recurrent neural network (RNN).', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 435}),\n",
       " Document(page_content='Introducing Recurrent Neural Networks\\nYou might be curious why basic feed-forward networks alone aren’t ideal\\nfor processing sequences. The problem is that basic feed-forward networks\\ndon’t have any way to account for temporal dependencies.\\nTemporal dependencies are simply dependencies on time. When dealing\\nwith sequences, it’s common to refer to the entries in the sequence as\\nindividual timesteps, regardless of whether entries map to actual timesteps.\\nA temporal dependency means that events that happen in the future are\\ndependent on what has happened in the past, or what is currently\\nhappening. If you think about this in the context of natural language, it\\nshould make sense.\\nIn a sequence of words from natural language, order matters. The order of\\nwords in a sentence can drastically alter the meaning of the sentence. Your\\ndense network has no way of accounting for this. In essence, your dense\\nneural network is learning to classify sentiment based on the presence of\\ncertain words, but not their location relative to other words in the sentence.\\nThis actually isn’t a bad strategy, but it fails to capture the complex\\nsequential relationships in each review. Consider the following review:\\nThis movie was awfully good. I hated everything \\nabout it (not).', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 436}),\n",
       " Document(page_content='This review employs a couple of quirks of the English language that native\\nspeakers can understand easily but which might trick your naive sentiment\\nanalysis model. First, the phrase “awfully good” is an oxymoron.\\nAdditionally, the second sentence uses sarcasm in the form of a “not” joke.\\nA human can understand that this is a positive review. But a model that\\ndoesn’t account for temporal relationships in a review would struggle to\\nclassify this sequence correctly. Your models need to understand the\\nsignificance of the modifiers, and without accounting for their position, it\\nwould struggle to classify this example correctly.\\nThis is where recurrent neural networks come into play. Recurrent neural\\nnetworks are neural networks that map a transformation over a sequence of\\ninput data, maintaining an internal state or memory that modifies outputs\\nbased on previous inputs. The term recurrent comes from the fact that\\nrecurrent neural networks implement a recurrence relation. In other words,\\nthe output of a recurrent neural network at step t depends on all of the\\nentries in the sequence before t.\\nRecurrent neural networks are designed specifically for sequences. If you\\nremember from Chapter 3,  Harness the Power of Math , the power of certain\\nmodels is often thanks to the assumptions they make. Just as convolutional\\nneural networks work well for data with a grid-like input structure,\\nrecurrent neural networks work well for data with a sequential structure.\\nThe assumptions of input structure allow CNNs and RNNs to perform\\nsignificantly better on certain classes of inputs. Alternatively, feed-forward', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 437}),\n",
       " Document(page_content='networks are more like a swiss army knife—they are universal function\\napproximators, but they don’t make significant assumptions about input\\nstructure and thus might struggle to learn on certain types of data.\\n\\xa0\\nThere are many different kinds of RNNs in academia. But, in this book,\\nyou’ll learn about two of the most common: long short-term memory\\n(LSTM) and gated recurrent units (GRU). Both are variants of recurrent\\nneural networks that overcome the limitations of the basic or simple RNN.\\nAxon offers out-of-the-box implementations of both LSTM and GRU\\nrecurrent neural networks, so you can easily plug them into your neural\\nnetworks.\\nTo see Axon’s RNN implementations in action, you’ll build one up from\\nscratch and get a sense of what each output represents. To start, create an\\nAxon input representing an input sequence from your movie review input\\npipeline:\\nsequence = Axon.input( \"  review\" )\\nThe shape of each input sequence is {batch_size, sequence_length}, and each\\nentry in the sequence is an integer token drawn from your vocabulary. In\\nyour feed-forward network, you converted this sparse representation into a\\ndense vector representation using an embedding layer. Axon’s RNN\\nimplementations also require embedded inputs. However, you don’t need to', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 438}),\n",
       " Document(page_content='flatten them because RNNs are designed to work over an entire sequence.\\nThus, you should add an embedding layer to your network:\\nembedded = sequence |> Axon.embedding(num_tokens + \\n2, 64)\\nAt this point, your neural network has shape {batch_size, sequence_length, 64}.\\nIn other words, for each token, you have a size 64 vector representation of\\nthat token. Next, you want to create a mask tensor from your input sequence\\nby using Axon.mask/2:\\nmask = Axon.mask(sequence, 0)\\nAxon.mask/2 essentially works to tell your model to ignore certain tokens.\\nSpecifically, we ignore padding tokens because we don’t want them to have\\nany effect on the output of the model. Remember that in your input pipeline\\nwe had to pad inputs to the same length, which means your neural network\\nwould have to learn to be robust against padding tokens. You can help your\\nnetwork a bit by completely ignoring these tokens using Axon.mask/2. The 0\\nas the second argument indicates what value specifically should be ignored.\\nNow, with an embedded sequence and a mask, you can go about adding an\\nLSTM layer using Axon.lstm/3:\\n{rnn_sequence, _state} = Axon.lstm(embedded, 64,  \\nmask:  mask,  unroll:   :static )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 439}),\n",
       " Document(page_content='The output form of Axon.lstm/3 is different than any of the layers you have\\nencountered so far. That’s because RNNs are different than any of the other\\nlayers you’ve seen so far. RNNs maintain a state or history from past entries\\nin a sequence, which affects future outputs. RNNs perform an operation that\\nis sometimes called a scan in functional programming. This operation is\\nsimilar to Elixir’s Enum.map_reduce operation. Enum.map_reduce performs a\\nmap operation which transforms values in an Enumerable, while also\\naccumulating and eventually returning a final state.\\nThe state output in the previous tuple represents the LSTM’s final\\n“memory.” The rnn_sequence variable is a transformed sequence obtained\\nfrom mapping the recurrent operation over the entire input sequence with\\nthe accumulated state. At this point, the output rnn_sequence has a shape\\n{batch_size, sequence_length, 64}. In other words, rnn_sequence has taken your\\nsequence of embedded tokens and transformed them into another sequence\\nof embedded tokens. So what’s the big deal?\\nWell, consider that your embedding layer simply looks up a vector\\nassociated with each individual token. It doesn’t account for adjacent\\ntokens. But your recurrent neural network maintains an internal state, which\\nmeans it can account for previous tokens in the input. By the time you reach\\nthe last token in your sequence, your RNN can transform it in such a way\\nthat it encodes information from the entire input sequence. To better\\nunderstand this, consider an easier problem. You have a list of strings and\\nyou want to alter the list of strings such that each entry in the list', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 440}),\n",
       " Document(page_content='corresponds to the amount of times a specific string has occurred in the list\\nup to that point. take for example the following magic word and input list:\\nmagic_word = \"sixers\"\\ninput = [\"rockets\", \"mavericks\", \"sixers\", \\n\"sixers\",\\n\"magic\", \"nets\", \"sixers\"]\\nThe output should be the following:\\n[0, 0, 1, 2, 2, 2, 3]\\nSo, how would you solve this in Elixir? The easiest way is by using\\nEnum.map_reduce:\\n{output, _} = Enum.map_reduce(input, 0,  fn  entry, \\ncount ->\\n   if  entry == magic_word  do \\n    {count + 1, count + 1}\\n   else \\n    {count, count}\\n   end \\nend )\\nIn a way, this is exactly what your RNN is doing. The final token of your\\noutput sequence encodes all of the information from previous tokens in the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 441}),\n",
       " Document(page_content='sequence. But, rather than counting occurrences of certain tokens, your\\nRNN is learning to output a useful representation for future layers. In this\\nsentiment-analysis example, your RNN is learning to transform the\\nsequence in such a way that the final token gives a good indication of\\nsentiment based on all of the tokens in the input.\\nThe implication here is that to make use of rnn_sequence, you need to extract\\nthe final token from the transformed sequence. You can do this with an\\nAxon.nx/3:\\nfinal_token = Axon.nx(rnn_sequence,  fn  seq ->\\n  Nx.squeeze(seq[[0..-1//1, -1, 0..-1//1]])\\nend )\\nAxon.nx/3 allows you to utilize generic Nx functions as Axon layers. In this\\ncode, you use Nx.slice_along_axis/4 to grab the final token—at index\\nmax_seq_len - 1—from axis 1 or the temporal axis. At this point, you should\\nhave a good representation of the input sequence. Similar to convolutional\\nneural networks, recurrent neural networks are typically followed by a\\nfully-connected head specialized to whatever task you’re trying to solve. In\\nother words, you use the recurrent neural network to extract features and the\\nfully-connected portion to inference on those features:\\nmodel =\\n  final_token', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 442}),\n",
       " Document(page_content=' |> Axon.dense(64,  activation:   :relu )\\n  |> Axon.dense(1)\\nRemember your final token contains an extracted representation of the\\nentire sequence, so theoretically, it’s useful for downstream tasks such as\\nclassification. Overall, your final model looks like this:\\ninput_template = Nx.template({64, 64},  :s64 )\\nAxon.Display.as_graph(model, input_template)\\nAnd you’ll see the model as shown.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 443}),\n",
       " Document(page_content='', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 444}),\n",
       " Document(page_content='At this point, you have a model and an input pipeline, so you can move\\nforward with implementing a training loop. Even though you’re using a\\nnew architecture, you don’t need to worry about adjusting your training\\nloop. In fact, you can copy your training loop from your MLP sentiment\\nanalysis model:\\nloss = &Axon.Losses.binary_cross_entropy(&1, &2,\\n   from_logits:  true,\\n   reduction:   :mean \\n)\\noptimizer = Axon.Optimizers.adam(1.0e-4)\\ntrained_model_state =\\n  model\\n  |> Axon.Loop.trainer(loss, optimizer)\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.run(train_pipeline, %{},  epochs:  \\n10,  compiler:  EXLA)\\nAfter running the code, you should see the following output:\\nEpoch: 0, Batch: 358, accuracy: 0.5121433 loss: \\n0.6840463\\nEpoch: 1, Batch: 358, accuracy: 0.7060410 loss: \\n0.6094189', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 445}),\n",
       " Document(page_content='Epoch: 2, Batch: 358, accuracy: 0.7469098 loss: \\n0.5683411\\nEpoch: 3, Batch: 358, accuracy: 0.7578339 loss: \\n0.5442138\\nEpoch: 4, Batch: 358, accuracy: 0.7610549 loss: \\n0.5284106\\nEpoch: 5, Batch: 358, accuracy: 0.7636232 loss: \\n0.5172608\\nEpoch: 6, Batch: 358, accuracy: 0.7654946 loss: \\n0.5089468\\nEpoch: 7, Batch: 358, accuracy: 0.7667568 loss: \\n0.5024669\\nEpoch: 8, Batch: 358, accuracy: 0.7686281 loss: \\n0.4972335\\nEpoch: 9, Batch: 358, accuracy: 0.7695855 loss: \\n0.4928872\\nThis is a definite improvement over your simpler feed-forward model. Now,\\nall you need is to evaluate your model. Copy and run your evaluation code\\nfrom your MLP model:\\nmodel\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 446}),\n",
       " Document(page_content='|> Axon.Loop.run(test_pipeline, \\ntrained_model_state,  compiler:  EXLA)\\nAfter running the code, you’ll see this:\\nBatch: 30, accuracy: 0.7510081\\nOverall, you were able to improve your sentiment-analysis model using a\\nrecurrent neural network. Your final model performance is decent, but you\\nshould know by now that you can always do better.\\nGoing in Two Directions\\nYour current implementation makes use of a straightforward LSTM. As you\\nknow, your LSTM extracts a representation of the input sequence by\\ntransforming the input sequence forward along the temporal axis with an\\naccumulated state or history. Intuitively, this makes sense. Humans\\ngenerally understand sequences in one direction, so it’s natural to enforce\\nthis constraint on neural networks as well. But neural networks aren’t\\nhumans, and can significantly benefit from alterations that don’t seem\\nnatural. One example of this is the use of bidirectional recurrent neural\\nnetworks.\\nBidirectional RNNs employ the same technique as traditional RNNs, but in\\ntwo directions. Your previous neural network only transformed the\\nsequence by unrolling or walking along the temporal axis in a forward', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 447}),\n",
       " Document(page_content='direction. A bidirectional RNN transforms the sequence by unrolling or\\nwalking along the temporal axis in both directions. This might not make\\nmuch sense to you. After all, reversing a movie review doesn’t add any\\nadditional context to your understanding. How could it possibly help a\\nneural network?\\nThe reality is that for sequences that don’t have an explicit causal\\nrelationship, the bidirectional transformation provides additional context for\\na recurrent neural network. They learn sequential relationships in both\\ndirections and are therefore more robust in nature. It’s important to note that\\nbidirectional RNNs are not valid for all sequential data. For example, you\\nshould not use bidirectional RNNs with time-series data because there’s an\\nexplicit causal relationship you’re trying to model. For text, working in both\\ndirections can help improve the generalization ability of your model.\\nBidirectional RNNs are straightforward to implement from scratch with the\\nAxon API. The following is a from-scratch implementation of a\\nbidirectional LSTM:\\nbackward_sequence = Axon.nx(forward_sequence, \\n&Nx.reverse(&1,  axes:  [1]))\\n{forward_state, forward_out} = \\nAxon.lstm(forward_sequence, 64)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 448}),\n",
       " Document(page_content='{backward_state, backward_out} = \\nAxon.lstm(backward_sequence, 64)\\nout_state = Axon.concatenate(\\n  forward_state,\\n  Axon.nx(backward_state, &Nx.reverse(&1,  axes:  \\n[1]))\\n)\\nout_sequence = Axon.concatenate(\\n  forward_out,\\n  Axon.nx(backward_out, &Nx.reverse(&1,  axes:  \\n[1]))\\n)\\nNotice that you need only to reverse the input sequence and apply the\\nLSTM transformation in both directions. Then you merge the resulting\\nstates and sequences to get a final output state and sequence. You can\\nchoose another operation such as Axon.add/2 to merge forward and backward\\noutputs, but concatenation is most common. The implication here is that if\\nyou want a bidirectional LSTM with 64 units, your output sequence will\\nhave 128 units as a result of the concatenation.\\nThe Axon API offers a convenience around bidirectional RNNs with\\nAxon.bidirectional/3. Using this, you can reimplement your RNN from the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 449}),\n",
       " Document(page_content='previous section:\\nsequence = Axon.input( \"  review\" )\\nmask = Axon.mask(sequence, 0)\\nembedded = Axon.embedding(sequence, num_tokens + \\n2, 64)\\n{rnn_sequence, _state} =\\n  Axon.bidirectional(\\n    embedded,\\n    &Axon.lstm(&1, 64,  mask:  mask,  unroll:   :static \\n),\\n    &Axon.concatenate/2\\n  )\\nfinal_token = Axon.nx(rnn_sequence,  fn  seq ->\\n  Nx.squeeze(seq[[0..-1//1, -1, 0..-1//1]])\\nend )\\nmodel =\\n  final_token\\n  |> Axon.dense(64,  activation:   :relu )\\n  |> Axon.dense(1)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 450}),\n",
       " Document(page_content='Now, you can run your original training loop with this new model using the\\nfollowing code:\\nloss = &Axon.Losses.binary_cross_entropy(&1, &2,\\n   from_logits:  true,\\n   reduction:   :mean \\n)\\noptimizer = Axon.Optimizers.adam(1.0e-4)\\ntrained_model_state =\\n  model\\n  |> Axon.Loop.trainer(loss, optimizer)\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.run(train_pipeline, %{},  epochs:  \\n10,  compiler:  EXLA)\\nAfter running the code, you’ll see this:\\nEpoch: 0, Batch: 350, accuracy: 0.5190082 loss: \\n0.6815987\\nEpoch: 1, Batch: 341, accuracy: 0.7035355 loss: \\n0.6067376\\nEpoch: 2, Batch: 332, accuracy: 0.7509853 loss: \\n0.5645106', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 451}),\n",
       " Document(page_content='Epoch: 3, Batch: 323, accuracy: 0.7610434 loss: \\n0.5400541\\nEpoch: 4, Batch: 314, accuracy: 0.7647320 loss: \\n0.5240928\\nEpoch: 5, Batch: 355, accuracy: 0.7672051 loss: \\n0.5114447\\nEpoch: 6, Batch: 346, accuracy: 0.7690023 loss: \\n0.5031880\\nEpoch: 7, Batch: 337, accuracy: 0.7696466 loss: \\n0.4966677\\nEpoch: 8, Batch: 328, accuracy: 0.7717992 loss: \\n0.4912724\\nEpoch: 9, Batch: 319, accuracy: 0.7726074 loss: \\n0.4866871\\nYou should notice your model is performing slightly better, but it took a bit\\nlonger to train, which makes sense because you’ve added an additional\\nLSTM layer to your network with the bidirectional transformation. Now\\nyou can test your model with your evaluation code:\\nmodel\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 452}),\n",
       " Document(page_content='|> Axon.Loop.run(test_pipeline, \\ntrained_model_state,  compiler:  EXLA)\\nAnd you’ll see this:\\nBatch: 30, accuracy: 0.7641129\\n\\xa0\\nWith the simple bidirectional transformation, you were able to eke out some\\nextra accuracy from your model. At this point, you have a pretty solid\\nunderstanding of how to implement recurrent neural networks in Axon, but\\nnow you need to understand a bit about what’s going on under the hood. In\\nthe next section, you’ll dive deeper into what’s actually happening in a\\nrecurrent neural network, and we’ll discuss some of their strengths and\\nweaknesses.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 453}),\n",
       " Document(page_content='Understanding Recurrent Neural Networks\\nYou should already kind of understand the process behind RNNs through\\nthe lens of your string-counting implementation with Enum.map_reduce/3.\\nRNNs transform input sequences with a similar operation. However, they\\nmake use of learned transformations via learned parameters. This\\nvisualization shows the simplest form of an RNN as shown in the diagram.\\nNotice that each subsequent state depends on the current token and previous\\noutput state—outputting a transformed token and new output state. This', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 454}),\n",
       " Document(page_content='visualization shows an unrolled version of the RNN graph. Rather than\\ndepict loops in the computation graph, it shows the operations unrolled\\nacross all timesteps in the sequence. This unrolling operation also takes\\nplace in the RNN computation, either at compile-time or runtime. Axon\\nmakes the distinction by offering both :static and :dynamic unrolling of\\nrecurrent operation.\\nA :dynamic unroll makes use of a runtime loop, whereas a :static unroll\\ninlines all of the RNN computations over a sequence at compile-time. The\\ninlining works well for short sequences but often consumes too much\\nmemory for longer sequences. Regardless, the trip count for the recurrent\\nloop is always known at compile-time, which means the gradient of the\\nRNN transformation is straightforward to compute.\\nThe actual recurrent transformation is often called the recurrent cell. You\\ncan think of the recurrent cell as the fn entry, acc -> ... end operation in Elixir’s\\nEnum.map_reduce. The most basic recurrent cell is often called a simple RNN\\ncell. While powerful in theory, they aren’t particularly useful in practice.\\nThis is because simple RNNs suffer from the vanishing gradient problem.\\nAs you might remember, neural networks are trained with gradient descent.\\nIn order to learn to extract useful representations from sequences, recurrent\\nneural networks need to send sufficient feedback signals during the training\\nprocess. For longer sequences, timesteps in the simple RNN have\\ndecreasing sensitivity to the original inputs. In other words, they are unable', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 455}),\n",
       " Document(page_content='to learn any useful representation of the input because they “forget”\\ninformation passed from previous timesteps. LSTMs overcome this\\nlimitation by introducing some additional internal state and parameters.\\nThe additional “gates” in the LSTM cell allow it to maintain information\\nfrom previous states and thus overcome the limitations of traditional RNNs\\nthanks to the vanishing gradient problem. LSTMs are perhaps the most\\npopular variant of RNNs in use today but are also computationally\\nexpensive and difficult to scale.\\nThe scalability issues and computational complexity of recurrent neural\\nnetworks are part of the reason they’ve fallen out of favor in recent years.\\nRNNs operate in an inherently sequential manner—this is part of what\\nmakes them so powerful. But it also makes them inefficient. You can only\\nvectorize/parallelize RNNs along the batch dimension.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 456}),\n",
       " Document(page_content='Wrapping Up\\nIn this chapter, you were introduced to natural language processing with\\nrecurrent neural networks. You learned how to implement some basic text\\nfeaturization and preprocessing with Elixir and how to implement and train\\nrecurrent neural networks in Axon. You also learned about some of the\\nintuition behind recurrent neural networks and some of the fundamental\\nshortcomings of recurrent neural networks, which have allowed them to be\\nusurped by transformer models.\\nIf you’re familiar with the current state of natural language processing, you\\nmight have questioned the need to cover recurrent neural networks at all.\\nHowever, to better understand the current state of the art, it’s important to\\nunderstand the previous state of the art—and why it ended up failing. While\\nrecurrent neural networks are no longer common in natural language\\nprocessing, they still have some practical use in certain domains.\\nAdditionally, understanding how RNNs work and where they fail is\\nimportant to better understand the rise of transformers.\\nBefore completely closing the door on RNNs in favor of transformers,\\nyou’ll explore one additional application of RNNs known as time-series\\ndata. In the next chapter, you’ll dive deep into time-series analysis with\\nneural networks.\\nFOOTNOTES', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 457}),\n",
       " Document(page_content='[31]https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 458}),\n",
       " Document(page_content='Chapter 10\\nForecast the Future\\n\\xa0\\nIn the previous chapter, you were introduced to the concept of recurrent\\nneural networks for learning on sequential data. Specifically, you created\\nand trained recurrent neural networks on text data using Elixir and Axon.\\nYou saw how recurrent neural networks are capable of learning\\nrelationships in natural language far easier than traditional feed-forward\\nnetworks due to their built-in memory and inherent sequential operation.\\nRecurrent neural networks are well-suited for processing text. However,\\nthat’s not the only thing they’re good for. Anything with a temporal nature\\npresents challenges for traditional feed-forward networks and is better\\nsuited for recurrent neural networks. One example of such data is time-\\nseries data. Time-series data is any collection of data indexed in time order.\\nAt each timestep, there are one or many observations. You can see how\\ntime-series data lends itself naturally to working with recurrent neural\\nnetworks.\\nIn this chapter, you’ll work with time-series data in Elixir, Nx, and Axon.\\nYou’ll learn about some of the challenges of working with time-series data\\nand a bit about why neural networks struggle so much with this data', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 459}),\n",
       " Document(page_content='compared to other approaches. You’ll also train both a convolutional and\\nrecurrent neural network on a time-series analysis problem, comparing the\\nresults and learning the benefits and drawbacks of each strategy.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 460}),\n",
       " Document(page_content='Predicting Stock Prices\\nPerhaps one of the most obvious applications of time-series analysis is\\nforecasting the direction of markets. Given enough historical data, you\\nshould be able to predict the future performance of a given equity or\\nmarket, right? As you’ll find out later in this chapter, the problem of\\nforecasting markets is exceptionally difficult. If it were as easy as throwing\\na neural network at the problem, everybody would be doing it. Despite the\\nchallenges, attempting to forecast stock prices is a good exercise in time-\\nseries analysis and a good demonstration of the pitfalls of putting too much\\nfaith in a model.\\nIn this example, you’ll be working with the historical stock data for 30\\ncompanies in the Dow Jones Industrial Average between 2006 and 2018.\\nThe data is available for download on Kaggle. After you download the\\ndata, you’ll see a collection of CSVs that contain information for the prices\\nof individual stocks and for all of the stocks. To simplify the problem,\\nyou’ll create a model that predicts the future stock prices for a single stock,\\nin this case, AAPL.\\nStart by firing up a new Livebook and adding the following dependencies:\\nMix.install([\\n  { :explorer ,  \"  ~> 0.5.0\" },\\n  { :nx ,  \"  ~> 0.6\" },\\n[32]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 461}),\n",
       " Document(page_content=' { :exla ,  \"  ~> 0.6\" },\\n  { :axon ,  \"  ~> 0.5\" },\\n  { :vega_lite ,  \"  ~> 0.1.6\" },\\n  { :kino ,  \"  ~> 0.8.0\" },\\n  { :kino_vega_lite ,  \"  ~> 0.1.7\" }\\n])\\nIt’s common to alias the VegaLite module to Vl, so run the following code in\\na new cell:\\nalias VegaLite,  as:  Vl\\nTo simplify the process of working with the structured CSV data, you’ll use\\nExplorer, Elixir’s DataFrame library. You’ll use Nx and EXLA for\\nnumerical computing and acceleration, respectively, and you’ll need Axon\\nfor the deep learning implementation. You’ll also use VegaLite, Kino, and\\nKino.VegaLite for providing some functionality to visualize and summarize\\nyour dataset.\\nBefore diving in, take some time to get familiar with the data. Start by\\nloading it into a DataFrame using Explorer, like so:\\ncsv_file =  \"  all_stocks_2006-01-01_to_2018-01-\\n01.csv\" ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 462}),\n",
       " Document(page_content='df = Explorer.DataFrame.from_csv!(csv_file,  \\nparse_dates:  true)\\nAfter running the code, you’ll see the following output:\\n#Explorer.DataFrame<\\n  Polars[93612 x 7]\\n  Date string [\"2006-01-03\", \"2006-01-04\", \"2006-\\n01-05\", ...]\\n  Open float [77.76, 79.49, 78.41, 78.64, 78.5, \\n...]\\n  High float [79.35, 79.49, 78.65, 78.9, 79.83, \\n...]\\n  Low float [77.24, 78.25, 77.56, 77.64, 78.46, \\n...]\\n  Close float [79.11, 78.71, 77.99, 78.63, 79.02, \\n...]\\n  Volume integer [3117200, 2558000, 2529500, ...]\\n  Name string [\"MMM\", \"MMM\", \"MMM\", \"MMM\", \"MMM\", \\n...]\\n>\\nThe dataset consists of opening, low, high, and closing prices on each\\ntrading day for a handful of stock tickers. You’ll notice there’s no intraday', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 463}),\n",
       " Document(page_content='data and no auxiliary information aside from trading volume. Auxiliary\\ninformation, or side information, is information you have access to outside\\nof the target you’re trying to predict. Rather than use side information,\\nyou’re forced to make use only of past timesteps to predict future timesteps\\nin an autoregressive manner. Autoregression means you’re going to predict\\nfuture values from existing values.\\nFor this problem, you’ll pay attention to only a ticker’s closing prices, so\\nyou can filter out the other irrelevant information:\\ndf = Explorer.DataFrame.select(df, [ \"  Date\" ,  \"  \\nClose\" ,  \"  Name\" ])\\nAfter doing so, you’ll see the following output:\\n#Explorer.DataFrame<\\n  Polars[93612 x 3]\\n  Date string [\"2006-01-03\", \"2006-01-04\", \"2006-\\n01-05\", ...]\\n  Close float [79.11, 78.71, 77.99, 78.63, 79.02, \\n...]\\n  Name string [\"MMM\", \"MMM\", \"MMM\", \"MMM\", \"MMM\", \\n...]\\n>', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 464}),\n",
       " Document(page_content='Next, run the following code to get a visualization of the various stock\\ntickers using VegaLite:\\nVl.new( title:   \"  DJIA Stock Prices\" ,  width:  640,  \\nheight:  480)\\n|> \\nVl.data_from_values(Explorer.DataFrame.to_columns(d\\n|> Vl.mark( :line )\\n|> Vl.encode_field( :x ,  \"  Date\" ,  type:   :temporal )\\n|> Vl.encode_field( :y ,  \"  Close\" ,  type:   :quantitative\\n|> Vl.encode_field( :color ,  \"  Name\" ,  type:   :nominal )\\n|> Kino.VegaLite.new()\\nAnd you’ll see the rendered image.\\nThis image is, admittedly, a bit noisy. For this problem, you’re only\\nconcerned with the price of the AAPL stock, so you can filter your data\\naccordingly:\\naapl_df = Explorer.DataFrame.filter_with(df,  fn  df \\n->\\n  Explorer.Series.equal(df[ \"  Name\" ],  \"  AAPL\" )\\nend )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 465}),\n",
       " Document(page_content='After you do so, you’ll see the following output:\\n#Explorer.DataFrame<\\n  Polars[3019 x 3]\\n  Date string [\"2006-01-03\", \"2006-01-04\", \"2006-\\n01-05\", ...]\\n  Close float [10.68, 10.71, 10.63, 10.9, 10.86, \\n...]\\n  Name string [\"AAPL\", \"AAPL\", \"AAPL\", \"AAPL\", \\n\"AAPL\", ...]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 466}),\n",
       " Document(page_content='>\\nNow, regenerate your original plot with only AAPL data:\\nVl.new( title:   \"  AAPL Stock Price\" ,  width:  640,  heigh\\n|> \\nVl.data_from_values(Explorer.DataFrame.to_columns(a\\n|> Vl.mark( :line )\\n|> Vl.encode_field( :x ,  \"  Date\" ,  type:   :temporal )\\n|> Vl.encode_field( :y ,  \"  Close\" ,  type:   :quantitative\\n|> Kino.VegaLite.new()\\nAnd you’ll see the rendered image.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 467}),\n",
       " Document(page_content='Now it’s time to start preparing the data for model training.\\nFirst, notice your dataset consists of unnormalized stock prices. Remember\\nit’s important to normalize data before you feed it into your neural network\\nfor training. Run the following code to normalize your DataFrame of AAPL\\nstock prices:\\nnormalized_aapl_df = \\nExplorer.DataFrame.mutate_with(aapl_df,  fn  df ->', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 468}),\n",
       " Document(page_content=' var = Explorer.Series.variance(df[ \"  Close\" ])\\n  mean = Explorer.Series.mean(df[ \"  Close\" ])\\n  centered = Explorer.Series.subtract(df[ \"  Close\" ], \\nmean)\\n  norm = Explorer.Series.divide(centered, var)\\n  [ \"  Close\" : norm]\\nend )\\nYou can replot your graph to verify that the pattern remains the same\\ndespite the normalization:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 469}),\n",
       " Document(page_content='Next, you need to split your model between training and testing sets. In\\nprevious chapters, a clear delineation usually was made between input\\nfeatures and targets. That’s not necessarily the case in this problem. In time-\\nseries analysis, your goal is to predict either a single step or multiple steps\\nin the future. You can make this prediction based on a range of historical\\ninputs or based on the current timestep. In this example, you’ll perform\\nsingle-step predictions using a range of historical values.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 470}),\n",
       " Document(page_content='You’ll also need to divide your dataset into training and test sets. Start by\\ncreating the following window/3 function in a new module:\\ndefmodule  Data  do \\n   def  window(inputs, window_size, \\ntarget_window_size)  do \\n    inputs\\n    |> Stream.chunk_every(window_size + \\ntarget_window_size, 1,  :discard )\\n    |> Stream.map( fn  window ->\\n      features =\\n        window\\n        |> Enum.take(window_size)\\n        |> Nx.tensor()\\n        |> Nx.new_axis(1)\\n      targets =\\n        window\\n        |> Enum.drop(window_size)\\n        |> Nx.tensor()\\n        |> Nx.new_axis(1)\\n      {features, targets}\\n     end )\\n   end ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 471}),\n",
       " Document(page_content='end \\nThe window/3 method takes an Enumerable or stream, an input window size,\\nand a target window size. It then returns a new stream, where each element\\nis a tuple of tensors with features and targets.\\nThe features tensor is comprised of window_size prices from the last\\nwindow_size days. The targets tensor is comprised of target_window_size prices\\nfrom target_window_size days after the input window. Notice that for both\\nfeatures and targets you need to add a new axis. Recall from  Representing\\nthe World , time-series data is typically represented in three dimensions with\\nshape {batch, timesteps, features}. For this example, you only have a single\\nfeature—price—and you’re only predicting a single feature—price.\\nNext, add the following batch/2 function to your data module:\\ndef  batch(inputs, batch_size)  do \\n  inputs\\n  |> Stream.chunk_every(batch_size,  :discard )\\n  |> Stream.map( fn  windows ->\\n    {features, targets} = Enum.unzip(windows)\\n    {Nx.stack(features), Nx.stack(targets)}\\n   end )\\nend ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 472}),\n",
       " Document(page_content='The batch/2 method converts your input windows into batches of input\\nwindows by stacking features and targets on top of one another. You can\\nnow use these methods to create new training and testing sets. But how do\\nyou split up your data into training and test sets?\\nIn the past, you randomly split your dataset into some percentage of training\\nand testing data. You’re able to split your dataset somewhat naively because\\nyour data doesn’t have any temporal dependencies. With a time-series\\ndataset, you have a bunch of potentially overlapping dependencies, which\\nmeans you can leak information about your test set into the training process.\\nLeakage isn’t good, and it can result in overconfidence and the deployment\\nof bad models.\\nAn alternative approach is to split your dataset temporally. For this\\nexample, you have ten years’ worth of data, so you can take the first eight\\nyears as training data and the last two years as testing data. While this\\napproach is better, it’s important to understand that there are still\\ndrawbacks. Certain time-series analysis problems, like stock price\\nprediction, are often dependent on unseen macro windows. Your model may\\nappear quite good at predicting the price of a stock because the market was\\nin the middle of a very consistent bull run. However, if that fundamental\\nmacro trend changes for the test set, you’ll have a garbage model.\\nA challenge with time-series analysis is there are often confounding\\nvariables at play, and without access to that information, it can be difficult', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 473}),\n",
       " Document(page_content='to get an accurate model. You need to be extremely careful when evaluating\\ntime-series models to ensure you’ve eliminated most of the possible sources\\nof bias.\\nFor this example, after looking at the plot of AAPL stock over the last ten\\nyears, you can see the general trend: AAPL stock goes up. The macro trends\\nbetween the training and testing sets are relatively similar. However, that\\ndoesn’t necessarily mean you’ll end up with a model that’s good at\\npredicting stock prices for 2023 or 2024. One consideration when doing\\ntime-series analysis is that you’ll constantly need to retrain and update\\ntrends. The world is chaotic, and the future isn’t easy to predict.\\nTo split your data into training and test sets, run the following code:\\ntrain_df = \\nExplorer.DataFrame.filter_with(normalized_aapl_df, \\nfn  df ->\\n  Explorer.Series.less(df[ \"  Date\" ], Date.new!(2016, \\n1, 1))\\nend )\\ntest_df = \\nExplorer.DataFrame.filter_with(normalized_aapl_df, \\nfn  df ->', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 474}),\n",
       " Document(page_content=' Explorer.Series.greater_equal(df[ \"  Date\" ], \\nDate.new!(2016, 1, 1))\\nend )\\nAfter doing so, you’ll see the following output:\\n#Explorer.DataFrame< \\n  Polars[503 x 3]\\n  Date date [2016-01-04, 2016-01-05, 2016-01-06, \\n2016-01-07, ...]\\n  Close float [105.35, 102.71, 100.7, 96.45, \\n96.96, ...]\\n  Name string [ \"  AAPL\" ,  \"  AAPL\" ,  \"  AAPL\" ,  \"  AAPL\" ,  \"  \\nAAPL\" , ...]\\n>\\nNow, convert your DataFrames to batches of windowed tensors using your\\ndata module by running the following code:\\nwindow_size = 5\\nbatch_size = 32\\ntrain_prices = Explorer.Series.to_list(train_df[ \"  \\nClose\" ])', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 475}),\n",
       " Document(page_content='test_prices = Explorer.Series.to_list(test_df[ \"  \\nClose\" ])\\nsingle_step_train_data =\\n  prices\\n  |> Data.window(window_size, 1)\\n  |> Data.batch(batch_size)\\nsingle_step_test_data =\\n  prices\\n  |> Data.window(window_size, 1)\\n  |> Data.batch(batch_size)\\nYou can confirm you’ve correctly created your datasets by taking samples\\nfrom each. Run the following code in a new cell:\\nEnum.take(single_step_train_data, 1)\\nAnd you’ll see this:\\n[\\n  { #Nx.Tensor< \\n     f32[32][5][1]\\n     [\\n       [', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 476}),\n",
       " Document(page_content='        [-0.027216043323278427],\\n         [-0.027200918644666672],\\n         [-0.02724125050008297],\\n         [-0.02710512839257717],\\n         [-0.027125295251607895]\\n       ],\\n       ...\\n     ]\\n   >,\\n    #Nx.Tensor< \\n     f32[32][1][1]\\n     [\\n       [\\n         [-0.026777423918247223]\\n       ],\\n       [\\n         [-0.026555592194199562]\\n       ],\\n       ...\\n   >}\\n]\\nNow you have a dataset of training inputs and training outputs. Your inputs\\nare a sample of five days of closing prices, with the target being a single', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 477}),\n",
       " Document(page_content='closing price.\\nNow that you’ve built a dataset, it’s time to start training some neural\\nnetworks.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 478}),\n",
       " Document(page_content='Using CNNs for Single-Step Prediction\\nRecall from Chapter 7,  Learn to See , that the convolutional neural networks\\ntake advantage of locality and weight sharing to effectively learn patterns\\non image data. Images naturally have local structure—features that are\\nclose together spatially in an image are often related. Additionally,\\nconvolutional neural networks apply the same set of parameters across an\\nentire sliding window. That means convolutional neural networks learn to\\nextract the same set of features from multiple regions in the input.\\nThe properties that make convolutional neural networks useful for image\\ndata also make them useful for time-series data. With time-series data, local\\nrelationships matter because adjacent timesteps are likely related.\\nAdditionally, when you have situations with multiple features per time\\nwindow, convolutions are capable of extracting information across both\\nyour feature dimension and temporal dimension. That means you can use\\none-dimensional convolutions and treat the temporal or time axis as the\\nspatial dimension to extract useful representations of the input. Feed-\\nforward networks fall short here because they don’t have the same built-in\\nassumptions about input data as convolutions.\\nTo create a new convolutional neural network, run the following code in a\\nnew Livebook cell:\\ncnn_model =', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 479}),\n",
       " Document(page_content=' Axon.input( \"  stock_price\" )\\n  |> Axon.nx(&Nx.new_axis(&1, -1))\\n  |> Axon.conv(32,  kernel_size:  {window_size, 1},  \\nactivation:   :relu )\\n  |> Axon.dense(32,  activation:   :relu )\\n  |> Axon.dense(1)\\nThen, run the following code to inspect your network:\\ntemplate = Nx.template({32, 10, 1},  :f32 )\\nAxon.Display.as_graph(cnn_model, template)\\nAnd you’ll see the following output:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 480}),\n",
       " Document(page_content='Notice that the output shape is {32, 1, 1}. Specifically, you’ll notice that with\\nthis model, you’re predicting a single step. To achieve this effect, you need\\nto ensure your convolutional kernel size is the same size as your temporal\\nwindow size. The convolution will transform your temporal window into a\\nsingle timestep. You can now train your model on the single-step training\\ndata. But what metric and loss function should you use here?\\nMost commonly, time-series models use mean squared error or root mean\\nsquared error as the loss function. Both of these loss functions penalize\\nlarge errors, which is beneficial when you’re trying to closely predict the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 481}),\n",
       " Document(page_content='correct next values in a sequence. In addition to mean squared error, it’s\\ncommon to use mean absolute error because it’s more readily interpretable\\nthan mean squared error. You immediately know that a mean absolute error\\nof 10 means your prediction was $10 off the actual next day’s closing price\\nof AAPL stock on a given day.\\nThe training loop for time-series data is exactly the same as every other\\ntraining loop you’ve written so far. Run the following code to train your\\nconvolutional neural network:\\ncnn_trained_model_state =\\n  cnn_model\\n  |> Axon.Loop.trainer( :mean_squared_error ,  :adam )\\n  |> Axon.Loop.metric( :mean_absolute_error )\\n  |> Axon.Loop.run(single_step_train_data, %{},  \\nepochs:  10,  compiler:  EXLA)\\nAfter training, you’ll see the following output:\\nEpoch: 0, Batch: 50, loss: 16.0255032 \\nmean_absolute_error: 3.1325140\\nEpoch: 1, Batch: 72, loss: 14.3411617 \\nmean_absolute_error: 2.1435401\\nEpoch: 2, Batch: 44, loss: 11.9783182 \\nmean_absolute_error: 1.2704648', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 482}),\n",
       " Document(page_content='Epoch: 3, Batch: 66, loss: 12.3826933 \\nmean_absolute_error: 1.9970336\\nEpoch: 4, Batch: 38, loss: 11.7019396 \\nmean_absolute_error: 1.2344553\\nEpoch: 5, Batch: 60, loss: 11.8652802 \\nmean_absolute_error: 1.9007618\\nEpoch: 6, Batch: 32, loss: 11.5063419 \\nmean_absolute_error: 1.1604406\\nEpoch: 7, Batch: 54, loss: 11.4728088 \\nmean_absolute_error: 1.7184598\\nEpoch: 8, Batch: 76, loss: 11.6156797 \\nmean_absolute_error: 2.1458588\\nEpoch: 9, Batch: 48, loss: 11.1360197 \\nmean_absolute_error: 1.4754609\\nYou can now evaluate your model on the test set using the following code:\\ncnn_model\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :mean_absolute_error )\\n|> Axon.Loop.run(\\n  single_step_test_data,\\n  cnn_trained_model_state,\\n   compiler:  EXLA', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 483}),\n",
       " Document(page_content=')\\nAfter you do so, you’ll see the following output:\\nBatch: 13, mean_absolute_error: 0.0035810\\nYou can use the mean and variance of AAPL stock prices you computed\\nearlier to get a more accurate estimate of what this normalized error\\nrepresents:\\n0.0035810\\n|> Kernel.*(\\n   :math .sqrt(Explorer.Series.variance(aapl_df[ \"  \\nClose\" ]))\\n)\\n|> Kernel.+(\\n  Explorer.Series.mean(aapl_df[ \"  Close\" ])\\n)\\nAfter running the code, you’ll see the following output:\\n64.82237670703707\\nYou can interpret the result as meaning that over the course of two years,\\nyour model had an absolute error of $64 off the next day’s closing stock\\nprice across each batch. This seems pretty bad. You can put this more into', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 484}),\n",
       " Document(page_content='perspective by visualizing your single-step predictions against the AAPL\\nchart and seeing how well the curves line up. Start by creating the following\\nmodule:\\ndefmodule  Analysis  do \\nend \\nAdd the following function to the Analysis module:\\ndefmodule  Analysis  do \\n   def  visualize_predictions(\\n    model,\\n    model_state,\\n    prices,\\n    window_size,\\n    target_window_size,\\n    batch_size\\n  )  do \\n    {_, predict_fn} = Axon.build(model,  compiler:  \\nEXLA)\\n    windows =\\n      prices\\n      |> Data.window(window_size, \\ntarget_window_size)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 485}),\n",
       " Document(page_content='     |> Data.batch(batch_size)\\n      |> Stream.map(&elem(&1, 0))\\n    predicted = Enum.flat_map(windows,  fn  window -\\n>\\n      predict_fn.(model_state, window) |> \\nNx.to_flat_list()\\n     end )\\n    predicted = List.duplicate(nil, 10) ++ \\npredicted\\n    types =\\n      List.duplicate( \"  AAPL\" , length(prices))\\n      ++ List.duplicate( \"  Predicted\" , \\nlength(prices))\\n    days =\\n      Enum.to_list(0..length(prices) - 1)\\n      ++ Enum.to_list(0..length(prices) - 1)\\n    prices = prices ++ predicted\\n    plot(%{', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 486}),\n",
       " Document(page_content='      \"  day\"  => days,\\n       \"  prices\"  => prices,\\n       \"  types\"  => types\\n    },  \"  AAPL Stock Price vs. Predicted, CNN \\nSingle-Shot\" )\\n   end \\n   defp  plot(values, title)  do \\n    Vl.new( title:  title,  width:  640,  height:  480)\\n    |> Vl.data_from_values(values)\\n    |> Vl.mark( :line )\\n    |> Vl.encode_field( :x ,  \"  day\" ,  type:   :temporal )\\n    |> Vl.encode_field( :y ,  \"  prices\" ,  type:   \\n:quantitative )\\n    |> Vl.encode_field( :color ,  \"  types\" ,  type:   \\n:nominal )\\n    |> Kino.VegaLite.new()\\n   end \\nend \\nThe function visualize_predictions/6 will run through the entire AAPL dataset\\nand produce predictions for every valid window. It will then produce a plot\\nof the side-by-side predictions with the actual data—so you can see how far', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 487}),\n",
       " Document(page_content='off you actually are. With your module created, run the following code to\\nvisualize your CNN model’s predictions:\\nAnalysis.visualize_predictions(\\n  cnn_model,\\n  cnn_trained_model_state,\\n  Explorer.Series.to_list(aapl_df[ \"  Close\" ]),\\n  window_size,\\n  1,\\n  batch_size\\n)\\nAfter running the code, you’ll see the following plot:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 488}),\n",
       " Document(page_content='Your single-shot CNN model tends to follow the trend, but you’ll notice it\\nmisses some drastic drops and spikes in price, and thus it’s very off in many\\nplaces. Of course, this plot is a bit intentionally misleading. At first glance,\\nyou might think you have the key to riches on your hand as your CNN\\noutputs seem to track AAPL’s stock price pretty well. Realistically, you do\\nyour model a favor and self-correct it with the next day’s actual stock price,\\nso it can never be too far off. If you were to rerun this analysis and feed the\\nmodel its own inputs, it would diverge from AAPL’s actual stock price\\nrather quickly. Single-step predictions are generally easier than multi-step', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 489}),\n",
       " Document(page_content='predictions because you don’t drift too far from the initial conditions you’re\\nmodeling against.\\nIn a multi-step prediction, you create a feedback loop in the model so it\\nlearns to make multiple predictions from its own predictions. As your\\nmodel predicts more and more outputs in a multi-step prediction, it drifts\\nfurther from true values.\\nConvolutional neural networks are perfectly suitable for modeling time-\\nseries problems. However, they don’t have any temporal assumptions built\\ninto them like recurrent neural networks. In the next section, you’ll create\\nand train a recurrent neural network on the same problem, and you’ll see\\nhow it stacks up against the convolutional neural network.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 490}),\n",
       " Document(page_content='Using RNNs for Time-Series Prediction\\nRecall from Chapter 9,  Understand Text , the reason recurrent neural\\nnetworks are so powerful with text is they model temporal dependencies.\\nThe same assumptions built into RNNs that make them good for text also\\nmake them good for time-series data. In a time-series dataset, inputs at time\\nt - 1 obviously have an impact on inputs at time t. Recurrent neural networks\\nare quite good at capturing what the relationship is between timesteps.\\nYou should have some familiarity with implementing recurrent neural\\nnetworks in Axon from the previous chapter. You can start by creating a\\nnew recurrent model, like this:\\nrnn_model =\\n  Axon.input( \"  stock_prices\" )\\n  |> Axon.lstm(32)\\n  |> elem(0)\\n  |> Axon.nx(& &1[[0..-1//1, -1, 0..-1//1]])\\n  |> Axon.dense(1)\\nYou should be familiar with LSTMs from Chapter 9,  Understand Text . This\\nmodel takes a time series of stock prices as input, runs them through a\\nsingle LSTM layer, and extracts the final token from the LSTM. The model\\nthen passes the LSTM output to a dense layer with 1 output unit. Run the\\nfollowing code to visualize the LSTM as shown.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 491}),\n",
       " Document(page_content='template = Nx.template({32, 10, 1},  :f32 )\\nAxon.Display.as_graph(rnn_model, template)\\nNow, use the following code to run your single-step RNN:\\nrnn_trained_model_state =\\n  rnn_model\\n  |> Axon.Loop.trainer( :mean_squared_error ,  :adam )\\n  |> Axon.Loop.metric( :mean_absolute_error )\\n  |> Axon.Loop.run(single_step_train_data, %{},  \\nepochs:  50,  compiler:  EXLA)\\nAfter a while, you’ll see the following output:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 492}),\n",
       " Document(page_content='Epoch: 40, Batch: 70, loss: 0.0000170 \\nmean_absolute_error: 0.0032365\\nEpoch: 41, Batch: 43, loss: 0.0000170 \\nmean_absolute_error: 0.0035252\\nEpoch: 42, Batch: 66, loss: 0.0000169 \\nmean_absolute_error: 0.0027809\\nEpoch: 43, Batch: 39, loss: 0.0000168 \\nmean_absolute_error: 0.0030578\\nEpoch: 44, Batch: 62, loss: 0.0000166 \\nmean_absolute_error: 0.0023700\\nEpoch: 45, Batch: 35, loss: 0.0000165 \\nmean_absolute_error: 0.0026432\\nEpoch: 46, Batch: 58, loss: 0.0000162 \\nmean_absolute_error: 0.0021020\\nEpoch: 47, Batch: 31, loss: 0.0000161 \\nmean_absolute_error: 0.0023653\\nEpoch: 48, Batch: 54, loss: 0.0000158 \\nmean_absolute_error: 0.0019592\\nEpoch: 49, Batch: 27, loss: 0.0000157 \\nmean_absolute_error: 0.0023374\\nWith a trained model, you can use Axon’s evaluation API to evaluate it\\nagainst your test data:\\nrnn_model', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 493}),\n",
       " Document(page_content='|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :mean_absolute_error )\\n|> Axon.Loop.run(\\n  single_step_test_data,\\n  rnn_trained_model_state,\\n   compiler:  EXLA\\n)\\nAfter running this code, you’ll see the following output:\\nBatch: 13, mean_absolute_error: 0.0032470\\nAgain, you can use the mean and variance you computed previously to get a\\nmore interpretable version of this metric:\\n0.0032470\\n|> Kernel.*(\\n   :math .sqrt(Explorer.Series.variance(aapl_df[ \"  \\nClose\" ]))\\n)\\n|> Kernel.+(\\n  Explorer.Series.mean(aapl_df[ \"  Close\" ])\\n)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 494}),\n",
       " Document(page_content='So your single-shot RNN performed slightly better than the CNN. But the\\ndifference between model errors is near negligible, so it’s difficult to say\\nwhich of these models between the CNN and RNN is better. If you were\\nmaking a decision between deploying the RNN or CNN, you would likely\\nneed to do more tests or use other criteria such as storage and compute cost\\nof each model, to determine which to use.\\nYou can repeat the analysis you ran in  Using CNNs for Single-Step\\nPrediction , to get a better visualization of how your model fits the data\\noverall:\\nAnalysis.visualize_predictions(\\n  rnn_model,\\n  rnn_trained_model_state,\\n  Explorer.Series.to_list(aapl_df[ \"  Close\" ]),\\n  window_size,\\n  1,\\n  batch_size\\n)\\nAfter running the code, you’ll see the image as shown.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 495}),\n",
       " Document(page_content='And again, you’ll notice there are negligible differences between how well\\nyour RNN and CNN fit to the data. Both an RNN and CNN may be\\nappropriate for conducting time-series analysis on this particular dataset\\nwhen making inferences over a single timestep.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 496}),\n",
       " Document(page_content='Tempering Expectations\\nAfter running these small experiments on stock prices, you might feel\\nthere’s some promise in using neural networks to predict the future. After\\nall, both of the neural networks you’ve trained do a pretty good job of\\nfollowing the trend line of AAPL’s stock price over the course of ten years.\\nIn reality, forecasting the future is really challenging.\\nNeural networks are powerful interpolators. Interpolation is the process of\\nfilling in unknown data points within a fixed domain. Interpolation is\\nexplicitly different from extrapolation because in extrapolation we attempt\\nto fill in unknown data points from outside a fixed domain. What does this\\nhave to do with time-series analysis?\\nWell, you may have noticed that your neural networks were decent at\\npredicting AAPL’s stock prices for the time-period you had access to. You\\nhave two neural networks that are decent models of the past, but they have\\nno information about the future. You can attempt to extrapolate information\\nabout the future from past patterns, but the patterns may change drastically.\\nWhen doing time-series analysis, you’re always at risk of drawing incorrect\\nconclusions from incomplete information from the past. That’s not to say\\nyou cannot exploit certain patterns in the past to use in the future, but you\\nshould always be aware of potential catastrophic failures in relying too\\nmuch on old patterns.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 497}),\n",
       " Document(page_content='[32]\\n[33]\\n[34]\\nWrapping Up\\nIn this chapter, you implemented two types of neural networks to predict the\\nprices of AAPL stock in the future. You created and trained both a\\nconvolutional and recurrent neural network to perform single-step time-\\nseries prediction on AAPL stock prices. You learned a bit about the\\nchallenges of modeling time-series data. You used Explorer for normalizing\\nyour input data and VegaLite for visualizing your input data.\\nIn the previous two chapters, you spent a lot of time working with recurrent\\nneural networks. Recurrent neural networks are powerful, but in the past\\nfive years, they’ve been completely blown away by an even more powerful\\nclass of models.\\nIn the next chapter, you’ll come face-to-face with the transformer, the\\narchitecture which powers some of the most powerful models in use today\\nincluding ChatGPT and Whisper.\\nFOOTNOTES\\nhttps://www.kaggle.com/datasets/szrlee/stock-time-series-20050101-to-20171231\\nhttps://openai.com/blog/chatgpt/\\nhttps://openai.com/blog/whisper/\\nCopyright © 2024, The Pragmatic Bookshelf.\\n[33] [34]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 498}),\n",
       " Document(page_content='Chapter 11\\nModel Everything with\\nTransformers\\n\\xa0\\nAt this point in the book, you’ve worked with many different types of data,\\nsuch as structured data, visual data, text data, and time-series data. For each\\nof these specific applications of machine learning, you used a new approach\\nor architecture to solve the problem. With each of these new models or\\narchitectures, you saw firsthand how building a model with some simple\\nassumptions and slight tweaks can drastically impact its performance.\\nIn Chapter 7,  Learn to See , you used convolutions with learned kernels to\\ndrastically increase the ability of your model to extract features from image\\ninputs. Convolutional neural networks work well because they assume your\\ninputs have a grid-like structure.\\nIn Chapter 8,  Stop Reinventing the Wheel , you improved performance on\\nlow-data problems with pre-trained models. You assume your chosen pre-\\ntrained model acts as a superior feature extractor to anything you can train\\nfrom scratch.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 499}),\n",
       " Document(page_content='In Chapter 9,  Understand Text , you applied recurrent architectures to\\nsequential problems. Recurrent neural networks work well because they\\nassume your inputs have temporal dependencies.\\nWhen making assumptions in machine learning, you also make trade-offs.\\nLinear models are simple, but they can’t capture nonlinear relationships.\\nConvolutional neural networks work well on images but are generally not\\nas strong as recurrent neural networks on sequences.\\nIn an ideal world, you wouldn’t need to construct a distinct architecture for\\neach input modality. Up until recently, the promise of a universally good or\\npreferred model architecture was somewhat of a fantasy. Remember from\\nChapter 1,  Make Machines That Learn , that this fantasy stems from the no-\\nfree lunch theorem. However, in recent years, transformers have proven\\nextremely generalizable on a wide range of input modalities.\\nTransformers are a class of deep learning models that were originally\\ndesigned for natural language processing. Over time, transformer models\\nhave emerged as not only the most powerful models in natural language\\nprocessing, but also as some of the most powerful models in audio\\nprocessing, computer vision, and more. There’s seemingly no limit to the\\npower of a carefully crafted transformer model and a lot of input data.\\nTransformers aren’t necessarily the best model for modeling everything, but\\nthey do pretty well at modeling a lot of things. For example, when dealing', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 500}),\n",
       " Document(page_content='with text, transformers are a good choice. When dealing with certain kinds\\nof image problems, such as image-to-image search, transformers are still a\\ngood choice. When working with multiple modalities, and one of those\\nmodalities is text, you guessed it, use transformers. Are there exceptions?\\nOf course. But if there was a competition for the most universally good\\nmodel, transformer models would probably be at the top of the list.\\nTransformers are revolutionizing nearly every domain of artificial\\nintelligence and machine learning. The last few years have seen the\\nemergence of a new transformer ecosystem in which libraries,\\ninfrastructure, hardware, and even companies are built entirely on the\\npromises of transformers. Understanding transformers is vital to being\\nsuccessful as a machine learning engineer in today’s world. In this chapter,\\nyou’ll apply transformers on three different problems using Nx, Axon, and\\nBumblebee. Bumblebee is an Elixir library that consists of pre-trained\\nAxon models as well as out-of-the-box pipelines for solving machine\\nlearning problems. You’ll use Bumblebee to simplify the process of\\ncreating, training, and using transformer models—and you’ll see how\\neffective transformers can be at modeling everything.\\n[35]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 501}),\n",
       " Document(page_content='Paying Attention\\nTo understand transformers, you first need to understand attention. The key\\nfeature of transformers is in how they apply attention. But what is attention,\\nand where does attention come from?\\nFirst, consider you’re trying to train a model to translate text between\\nEnglish and German. You have a large corpus of text pairs with a source\\nsentence in English and a target sentence in German. This is a classic\\nexample of machine translation, and more specifically in the context of\\ndeep learning, neural machine translation.\\nIn neural machine translation, the goal is to map a source sequence to a\\ntarget sequence. From what you know so far, this sounds like a perfect\\napplication of recurrent neural networks. Remember, recurrent neural\\nnetworks are capable of taking a source sequence and transforming it into\\nsome output sequence. Intuitively, it seems reasonable to believe that you\\ncan transform the input sequence with one or many LSTM or GRU blocks.\\nAfter all, they are designed for processing sequences and mapping\\nsequences to convenient sequence representations.\\nHowever, this problem isn’t that easy. There’s no guarantee the source\\nsequence and target sequence have the same length. Additionally, syntax\\nbetween languages differs drastically—you can’t just map source tokens to\\nthe translated variants and expect the translation to be correct. As it turns', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 502}),\n",
       " Document(page_content='out, this task is pretty difficult for a single RNN. To address this problem,\\nresearchers introduced sequence-to-sequence models.\\nSequence-to-sequence models are designed specifically for problems in\\nwhich the objective is to map a source sequence to a target sequence.\\nSequence-to-sequence models consist of an encoder and a decoder. The\\nencoder is responsible for summarizing the information present in the\\nsource sequence in a context vector. The decoder is initialized with the\\ncontext vector and produces the target sequence. The context vector is a\\nconvenient representation of the input sequence. If you consider what a\\ncontext vector is in the context of machine translation, you can think of it as\\nthe meaning of the source sentence. The goal of a translation is to capture\\nthe meaning of the sentence in the source language in the target language.\\nSequence-to-sequence models break the process of mapping source to target\\nsequences into two steps. First, the encoder extracts meaning from the\\nsource sequence, and then the decoder uses this meaning to generate a\\ntarget sequence. In Axon, the implementation of a sequence-to-sequence\\nmodel looks like this:\\nsource = Axon.input( \"  source\" )\\ncur_target = Axon.input( \"  target\" )\\n{_, context_vector} = Axon.lstm(source, 64)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 503}),\n",
       " Document(page_content='{out_target, _} = Axon.lstm(cur_target, \\ncontext_vector, 64)\\nAxon.dense(out_target, vocab_size)\\nNotice that the context vector is the final hidden state produced by the\\nLSTM layer. You use this context vector as the initial hidden state of the\\ndecoder to produce a transformed representation of the sequence. Notice\\nthat you don’t transform source but rather cur_target, which is the current\\ntarget sequence. In a sequence-to-sequence model, rather than try to output\\nan entire sequence at once, you generally predict one token at a time. You\\ncan almost think of it as a game of filling in the blanks. You’re given a\\nsource sentence, such as this:\\nI love cats\\nThe source comes with a partial sequence, like this:\\nIch liebe\\nThe goal is to predict the next token that produces the next translation,\\nwhich, in this case, is Katzen.\\nSequence-to-sequence models work really well, and in theory, can learn to\\nmap any sequence to any other sequence. In practice, they have some\\nlimitations. Most notably, sequence-to-sequence models struggle to', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 504}),\n",
       " Document(page_content='remember information from long sequences. Remember, the state in an\\nRNN is accumulated from the beginning of the sequence to the end. With\\nlong sequences, it’s common for the latter portions of the sequence to be\\noverrepresented in the final state, while earlier portions are forgotten.\\nTo overcome the problem of forgetting, you can introduce a shortcut that\\ncomputes the alignment between source and target sentences. This is the\\nattention mechanism. Attention was originally designed as a shortcut\\nbetween the context vector and the source input—it provides a map that\\nindicates how much each hidden state should be weighed into each output\\ntoken. You might see this called an alignment score, the attention matrix, or\\nthe attention weights. The attention matrix essentially tells you how\\nimportant each word in the source sequence is to the target sequence. For a\\ntranslation task, you can think of this as mapping which words in the source\\nsequence best relate to words in the target sequence. For example, the\\nalignment score between cats and Ich should be relatively low whereas the\\nalignment between cats and Katzen should be relatively high.\\nMuch like you can visualize intermediate activations in convolutional\\nneural networks, you can also visualize attention weights to provide a bit of\\ninsight into the relationships your model is learning. For example, you can\\nsee the visualized attention matrix for an English-to-French translation.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 505}),\n",
       " Document(page_content='Notice how words whose meanings are more closely aligned are more\\nactive in the attention map. Attention enables a model to learn to selectively\\npay attention to certain tokens in the input. This small addition to recurrent\\nmodels turns out to be pretty powerful. But it still depends on a recurrent\\nneural network. So how do you go from a recurrent neural network with\\nattention to an attention-only transformer?', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 506}),\n",
       " Document(page_content='Going from RNNs to Transformers\\nAttention provides a shortcut between source and target sequences. This\\nhelps a recurrent model learn longer sequences by highlighting which\\nportions of the context vector map to portions of the target sequence. The\\nlearned attention matrix encodes information about the relationships\\nbetween tokens in source and target sequences. But you still need to rely on\\na recurrent network for extracting context from the source sequence.\\nRecall from Chapter 9,  Understand Text , that it’s common practice to\\nextract the last token from a sequence transformed by an RNN such as an\\nLSTM. This is because this token contains a representation that encodes\\ninformation about the entire sequence. The sequential nature of RNNs\\nrestricts their ability to make connections between nonadjacent tokens in a\\nsequence. The recurrent process works on a single timestep in the forward\\nor reverse direction, so it’s difficult to capture relationships between\\nnonadjacent tokens. Fortunately, you already know how to address this\\nproblem using attention.\\nRather than weight the relationships between a source and target sequence,\\nyou can use attention to weight the relationships between a source sequence\\nand itself. Using the same process in the original attention mechanism, you\\ninitialize and learn an attention matrix, which highlights the relationship\\nbetween words in a single sentence. This is known as self-attention, and it\\nhighlights relationships between words in a sequence. This allows you to', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 507}),\n",
       " Document(page_content='learn which words most relate to one another and which words provide the\\nmost context to the input sequence. At this point, you might ask yourself,\\n“Isn’t this what a recurrent neural network is supposed to do? Isn’t the point\\nof a recurrent neural network to extract temporal relationships between\\ntokens in sequences?” Yes. But in this case, you’ve managed to model these\\nrelationships without the need for a recurrent neural network. In fact,\\nAttention is All You Need. [VSPU17]\\nThe typical transformer architecture makes use of an encoder-decoder\\narchitecture on a masked language modeling task. The masked language-\\nmodeling task is best described as an elementary school vocabulary\\nassignment. In other words, transformers learn to model language by\\npredicting the next word most likely to appear in a sequence.\\nThe encoder in a transformer typically consists of a collection of multiple\\nencoder layers and operates on a representation of the source sequence.\\nEach encoder layer is identical and consists of some form of self-attention\\nand some series of output projections. The actual variant of self-attention in\\nuse in a transformer is typically multi-headed self-attention, which is\\nidentical to self-attention with one slight exception.\\nIn single-headed self-attention, the model computes one attention matrix,\\nwhich means it learns to weight the relationships between tokens in one\\nway. In multi-headed self-attention, the model computes multiple (typically\\n12) different attention matrices (heads). This means the model learns an', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 508}),\n",
       " Document(page_content='ensemble of relationships between tokens in the input. Encoders pass the\\nencoded representation on to subsequent encoder layers.\\nThe decoder in a transformer model typically consists of a collection of\\nmultiple decoder layers and operates on a representation of the target\\nsequence—usually, the source sequence shifted to the right by one token—\\nand some information about the source sequence from the encoder. Each\\ndecoder layer also consists of some variant of self-attention, but the decoder\\nalso makes use of cross-attention. The decoder layer computes multi-headed\\nself-attention on the target sequence. The decoder layer also computes\\nmulti-headed cross-attention between information provided by the encoder\\nand the target sequence. This is similar to attention in the traditional sense\\nyou learned about earlier. Cross-attention provides a shortcut in context\\nbetween the encoder and the decoder. You can see a high-level visual\\noverview of cross-attention in the diagram.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 509}),\n",
       " Document(page_content='One issue with using attention to model temporal data is that transformers\\nhave no way of representing temporal dependencies. To overcome this\\nlimitation, most transformers introduce a positional embedding or positional\\nencoding which injects information about positional relationships into the\\nmodel.\\nMost transformers consist of hierarchies of encoder layers and decoder\\nlayers stacked on top of one another. While there are many variants of the\\ntraditional transformer architecture, essentially all of them follow this same\\npattern. If you look at the transformer implementations in Bumblebee,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 510}),\n",
       " Document(page_content='you’ll find that many of them are nearly identical. Most models are\\ndifferentiated only by slight tweaks in specific parts such as the attention\\nimplementation, the positional encoding used, or the training process.\\nThe typical transformer architecture produces a high-dimensional\\nrepresentation of sequences, which are useful on downstream tasks.\\nTransformers, more or less, eliminate the need for recurrent neural networks\\nin the context of natural language processing. In fact, in almost every case,\\ntransformers prove significantly more powerful than their recurrent\\ncounterparts. Additionally, transformers are readily parallelizable and are\\nsignificantly more scalable than an equivalent recurrent neural network.\\nCombined with massive amounts of data, transformers have proven to be\\none of the most important innovations in deep learning.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 511}),\n",
       " Document(page_content='Using Transformers with Bumblebee\\nTo really appreciate the power of transformers, you’ll put a few into\\npractice using the Bumblebee library. Bumblebee is a library that contains\\nimplementations of popular deep learning architectures in Axon.\\nBumblebee interacts directly with the HuggingFace hub and is even capable\\nof loading parameters directly from pre-trained PyTorch checkpoints.\\nBumblebee allows you to rapidly use some of the most powerful open-\\nsource models in existence with a few lines of Elixir. To demonstrate the\\npower of transformers and the simplicity of Bumblebee, you’ll use\\nBumblebee to solve three different kinds of problems.\\nInstalling Dependencies\\nTo get started, fire up a Livebook and add the following dependencies:\\nMix.install([\\n  { :bumblebee ,  \"  ~> 0.5\" },\\n  { :axon ,  \"  ~> 0.6\" },\\n  { :nx ,  \"  ~> 0.7\" },\\n  { :kino ,  \"  ~> 0.8\" },\\n  { :kino_bumblebee ,  \"  >= 0.0.0\" },\\n  { :exla ,  \"  >= 0.0.0\" }\\n])', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 512}),\n",
       " Document(page_content='The only new dependencies here are Bumblebee and KinoBumblebee.\\nKinoBumblebee provides some conveniences for using Bumblebee in\\nLivebook. You can actually implement a number of the tasks demonstrated\\nin this chapter without having to write any code using the smart cells\\nprovided by KinoBumblebee.\\nYou’ll also need Axon for performing inference and fine-tuning on pre-\\ntrained models, Nx for basic tensor manipulations, and EXLA for hardware\\nacceleration. Before continuing, set the default backend to EXLA to ensure\\nall of your computations are accelerated:\\nNx.global_default_backend(EXLA.Backend)\\nZero-Shot Classification with BART\\nTo get your toes wet with Bumblebee, you’ll start by implementing some\\nbasic zero-shot classification tasks using Bumblebee and Axon. Zero-shot\\nclassification is a technique that associates text with labels without the need\\nto specialize a model on specific input domains. Some models are trained\\non general enough data that they’re able to classify closely related natural\\nlanguage labels to portions of text without needing to see any examples of\\ninput-target pairs. With zero-shot classification, you can take advantage of a\\nmodel without needing to gather input data.\\nAs a practical example, imagine you’re tasked with automating the process\\nof filing customer support tickets for an airline. In this example, users enter', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 513}),\n",
       " Document(page_content='their inquiries in natural language, and your model is responsible for filing\\nthe requests under the appropriate tags: “New Booking,” “Update\\nBooking,” “Cancel Booking,” or “Refund.” Up until this point in the book,\\nyou wouldn’t have had any way to solve this problem without access to a\\nsufficient amount of training data. Fortunately, with zero-shot classification,\\nyou don’t necessarily need training data.\\nFirst, copy and run the following code to import a pre-trained model and\\ntokenizer with Bumblebee:\\n{ :ok , model} = Bumblebee.load_model(\\n  { :hf ,  \"  facebook/bart-large-mnli\" }\\n)\\n{ :ok , tokenizer} = Bumblebee.load_tokenizer(\\n  { :hf ,  \"  facebook/bart-large-mnli\" }\\n)\\nThis code uses the Bumblebee module to load a pre-trained model and\\ntokenizer directly from the HuggingFace hub. Bumblebee is designed to\\nsupport multiple model hubs, which is why you need to specify the tuple\\n{:hf, path} in both load_model/1 and load_tokenizer/1. load_model/1 returns the\\npre-trained model and parameters, as well as the model’s configuration. The\\nconfiguration will tell you exactly what model architecture you’ve loaded\\nfrom the given path. In this case, bart-large-mnli is an instantiation of the\\nBART transformer architecture from Facebook. BART is a large language', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 514}),\n",
       " Document(page_content='model pre-trained by Facebook for text generation, translation, and\\ncomprehension. The MNLI variant is trained on natural language inference\\ntasks, which attempt to predict the probability a given hypothesis is true\\nbased on some input text.\\nload_tokenizer/1 serves the same purpose as load_model/1, but with tokenizers.\\nBumblebee depends on a low-level tokenizers library that binds\\nHuggingFace’s Rust tokenizer implementations. Recall from Chapter 9,  \\nUnderstand Text , that some sophisticated tokenization strategies rely on\\nprobabilistic methods to learn vocabularies with fixed sizes. Most of the\\ntransformer models in Bumblebee employ these kinds of tokenization\\nstrategies and come with a pre-trained tokenizer. It’s important that you use\\na model’s corresponding pre-trained tokenizer because it will encode text in\\nthe same manner the model was exposed to during training.\\nmodel is actually a map with :model, :spec, and :params keys. You can inspect\\neach key in the map to see what it represents, but for now, inspect the :model\\nkey in model:\\nIO.inspect model.model\\nYou’ll see this:\\n#Axon<\\n  inputs: %{\\n    \"attention_head_mask\" => {12, 16},', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 515}),\n",
       " Document(page_content='   \"attention_mask\" => {nil, nil},\\n    \"cache\" => nil,\\n    \"cross_attention_head_mask\" => {12, 16},\\n    \"decoder_attention_head_mask\" => {12, 16},\\n    \"decoder_attention_mask\" => {nil, nil},\\n    \"decoder_input_embeddings\" => {nil, nil, \\n1024},\\n    \"decoder_input_ids\" => {nil, nil},\\n    \"decoder_position_ids\" => {nil, nil},\\n    \"encoder_hidden_state\" => {nil, nil, 1024},\\n    \"input_embeddings\" => {nil, nil, 1024},\\n    \"input_ids\" => {nil, nil},\\n    \"position_ids\" => {nil, nil}\\n  }\\n  outputs: \"container_64\"\\n  nodes: 1956\\n>\\nNotice how model is a regular Axon model. Under the hood, Axon pulls the\\nspecified model’s configuration from the HuggingFace hub and delegates to\\na module that builds the Axon implementation with the given configuration.\\nThis means you can use model like you would any other Axon model.\\nBumblebee provides conveniences; it doesn’t change how you interact with\\nAxon models.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 516}),\n",
       " Document(page_content='Next, run the following Livebook cell:\\nlabels = [ \"  New booking\" ,  \"  Update booking\" ,  \"  Cancel \\nbooking\" ,  \"  Refund\" ]\\nzero_shot_serving =\\n  Bumblebee.Text.zero_shot_classification(\\n    model,\\n    tokenizer,\\n    labels\\n  )\\nRunning this cell creates an %Nx.Serving{} struct which runs a zero-shot\\nclassification task end-to-end. Nx.Serving is a high-level serving API that\\nencapsulates preprocessing, inference, and postprocessing. Bumblebee\\nworks directly with the serving API, offering serving factories that return\\nend-to-end pipelines for tasks such as image and text classification.\\nThis code makes use of a natural language inferencing (NLI) model for\\nzero-shot classification. NLI models are commonly used for understanding\\nwhether two sentences are entailments, contradictions, or neutral. In other\\nwords, you can propose a premise and a hypothesis, and the model will tell\\nyou if the hypothesis contradicts or goes along with the premise. With these\\nkinds of models, you can build the hypothesis in such a way that it’s useful\\nfor zero-shot classification. In this case, you always build your hypothesis', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 517}),\n",
       " Document(page_content='as This example is #{label}, which fills in the hypothesis with the given label.\\nThe model spits out a probability for each label that the premise (input\\nsequence) goes along with the hypothesis. At the end, you have a list of\\nlabels and their corresponding probabilities, which you can extract the\\nmaximum value from.\\nIn addition to a model and tokenizer, Bumblebee’s zero-shot classification\\nfunction accepts labels to assign to zero-shot queries. That’s all you need to\\nstart making zero-shot classifications. To try out your zero-shot\\nclassification pipeline, you can use Nx.Serving.run/2. Try running the\\nfollowing code in a new cell and observe the output:\\ninput =  \"  I need to book a new flight\" \\nNx.Serving.run(zero_shot_serving, input)\\nYour model will output the following:\\n%{\\n   predictions:  [\\n    %{ label:   \"  New booking\" ,  score:  \\n0.5991652011871338},\\n    %{ label:   \"  Update booking\" ,  score:  \\n0.3455488979816437},', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 518}),\n",
       " Document(page_content='   %{ label:   \"  Refund\" ,  score:  \\n0.028283976018428802},\\n    %{ label:   \"  Cancel booking\" ,  score:  \\n0.027001921087503433}\\n  ]\\n}\\nYou can try a few more examples to get a feel for how powerful this zero-\\nshot classification task is:\\ninputs = [\\n   \"  I want to change my existing flight\" ,\\n   \"  I want to cancel my current flight\" ,\\n   \"  I demand my money back\" \\n]\\nNx.Serving.run(zero_shot_serving, input)\\nYour model will output the following:\\n[\\n  %{\\n     predictions:  [\\n      %{ label:   \"  New booking\" ,  score:  \\n0.43927058577537537},', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 519}),\n",
       " Document(page_content='     %{ label:   \"  Update booking\" ,  score:  \\n0.4268641471862793},\\n      %{ label:   \"  Cancel booking\" ,  score:  \\n0.10792690515518188},\\n      %{ label:   \"  Refund\" ,  score:  \\n0.02593844011425972}\\n    ]\\n  },\\n  %{\\n     predictions:  [\\n      %{ label:   \"  Cancel booking\" ,  score:  \\n0.5605528950691223},\\n      %{ label:   \"  Refund\" ,  score:  \\n0.3020733594894409},\\n      %{ label:   \"  Update booking\" ,  score:  \\n0.09756755083799362},\\n      %{ label:   \"  New booking\" ,  score:  \\n0.03980622440576553}\\n    ]\\n  },\\n  %{\\n     predictions:  [\\n      %{ label:   \"  Refund\" ,  score:  \\n0.913806140422821},', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 520}),\n",
       " Document(page_content='     %{ label:   \"  Cancel booking\" ,  score:  \\n0.04736287519335747},\\n      %{ label:   \"  Update booking\" ,  score:  \\n0.02491646446287632},\\n      %{ label:   \"  New booking\" ,  score:  \\n0.013914537616074085}\\n    ]\\n  }\\n]\\nNotice you can pass batches of inputs to the Nx.Serving API, which means\\nyou can easily batch-process inputs.\\nWithout any training data, and in under five lines of Elixir, you have a\\nmodel that’s capable of filing customer service requests with appropriate\\ntags. You can also augment or extend this pipeline to allow for multiple\\nlabels for each input. For example, you might add tags related to the\\nurgency of the request. Thanks to the power of transformers, you’re limited\\nmostly by your imagination and needs.\\nMaking Conversation\\nMore than likely, you have been exposed to the incredible coherence of AI-\\ngenerated text. Models such as GPT-3, OPT-3, and BLOOM are capable of\\nincredible feats. These large language models are adept at sentence', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 521}),\n",
       " Document(page_content='completion tasks. You feed a large language model a prompt, and it\\ncompletes the prompt with what it thinks is the most reasonable sequence.\\nIf you engineer the prompt in a clever way, you can get some pretty\\nimpressive behavior. One application of a completion or generation task\\nsuch as this one is a chat bot. With some prompt engineering, you can have\\na transformer model that maintains coherent conversations in a few lines of\\ncode.\\nTo start, you’ll need to import a new model and tokenizer:\\nrepo = { :hf ,  \"  google/gemma-2b-it\" }\\n{ :ok , model} = Bumblebee.load_model(repo)\\n{ :ok , tokenizer} = Bumblebee.load_tokenizer(repo)\\n{ :ok , generation_config} = \\nBumblebee.load_generation_config(repo)\\ngeneration_config =\\n  Bumblebee.configure(generation_config,\\n     max_new_tokens:  256,\\n     strategy:  %{ type:   :multinomial_sampling ,  \\ntop_p:  0.6}\\n  )\\nNext, create a new generation serving using Bumblebee.Text.generation/4:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 522}),\n",
       " Document(page_content='serving =\\n  Bumblebee.Text.generation(\\n    model_info,\\n    tokenizer,\\n    generation_config,\\n     compile:  [ batch_size:  1,  sequence_length:  \\n1028],\\n     stream:  true,\\n     defn_options:  [ compiler:  EXLA]\\n  )\\nKino.start_child({Nx.Serving,  name:  Gemma,  \\nserving:  serving})\\nThis serving encapsulates all of the logic necessary for using large-language\\nmodels to generate text. Next, you’ll want to implement an interface for\\nquerying the large language model.\\nAdd the following code to a new Livebook cell:\\nuser_input = Kino.Input.textarea( \"  User prompt\" ,  \\ndefault:   \"  Who are you?\" )\\nuser = Kino.Input.read(user_input)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 523}),\n",
       " Document(page_content='prompt =  \"\"\" \\n<start_of_turn>user \\n#{user} \\n<start_of_turn>model \\n\"\"\" \\nNx.Serving.batched_run(Gemma, prompt)\\n|> Enum.each(&IO.write/1)\\nThis code renders an input, which prompts the user to initiate the\\nconversation. It then formats a prompt in the particular style required by the\\nGemma LLM. Then, it uses Nx.Serving.batched_run/2 to generate text, and it\\nstreams the generated text out using IO.write/1.\\nCertain models, such as the Gemma model used here, support generation\\nout of the box in Bumblebee by implementing a generation behavior. By\\ndefault, Bumblebee uses a greedy text generation strategy to generate\\ntokens from a sequence. The greedy strategy generates one token of the\\nsequence at a time by predicting the most probable next token. Bumblebee\\nalso supports more complex generation strategies out of the box, but they’re\\noften more compute-intensive. You’ll find when working with large\\nlanguage models that generation strategies and hyperparameters often have\\na significant impact on the quality of generations.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 524}),\n",
       " Document(page_content='After generating the bot’s response given the chat history, the chat bot\\noutputs the text to the screen using Kino and waits for the next input.\\nAfter running this cell, you’ll see a Kino input form with a large submit\\nbutton. You can type in any text you want to converse with the large\\nlanguage model. With only a few lines of Elixir code, you’re able to interact\\nwith some extremely powerful language models.\\nClassifying Images\\nAs the title of this chapter implies, transformers are also not just limited to\\ntext data. You can also apply them effectively to the visual domain. To\\ndemonstrate the seemingly universal dominance of transformers, you’ll\\nimplement a simple image classifier using a pre-trained transformer.\\nStart by creating a new cell and running the following code:\\n{ :ok , model_info} =\\n  Bumblebee.load_model({ :hf ,  \"  google/vit-base-\\npatch16-224\" })\\n{ :ok , featurizer} = Bumblebee.load_featurizer(\\n  { :hf ,  \"  google/vit-base-patch16-224\" }\\n)\\nserving =', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 525}),\n",
       " Document(page_content=' \\nBumblebee.Vision.image_classification(model_info, \\nfeaturizer,\\n     top_k:  1,\\n     compile:  [ batch_size:  1],\\n     defn_options:  [ compiler:  EXLA]\\n  )\\nYou should be pretty familiar with this workflow by now. The only\\ndifference in this example is that you’ve changed load_tokenizer to\\nload_featurizer. Much like tokenizers are used to extract features from text,\\nfeaturizers are used to extract features from images. In this example, you’re\\nusing the Vision Transformer (ViT). The ViT is a pre-trained transformer\\narchitecture that achieves a state-of-the-art performance on ImageNet.\\nWith your model loaded, you can load an image file, such as this one:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 526}),\n",
       " Document(page_content='You’ll use the following code:\\nimage_input = Kino.Input.image( \"  Image\" ,  size:  \\n{224, 224})\\nform = Kino.Control.form([ image:  image_input],  \\nsubmit:   \"  Run\" )\\nframe = Kino.Frame.new()\\nform\\n|> Kino.Control.stream()\\n|> Stream.filter(& &1.data.image)\\n|> Kino.listen( fn  %{ data:  %{ image:  image}} ->\\n  Kino.Frame.render(frame, Kino.Markdown.new( \"  \\nRunning...\" ))', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 527}),\n",
       " Document(page_content=' image =\\n    image.file_ref\\n    |> Kino.Input.file_path()\\n    |> File.read!()\\n    |> Nx.from_binary( :u8 )\\n    |> Nx.reshape({image.height, image.width, 3})\\n  output = Nx.Serving.run(serving, image)\\n  output.predictions\\n  |> Enum.map(&{&1.label, &1.score})\\n  |> Kino.Bumblebee.ScoredList.new()\\n  |> then(&Kino.Frame.render(frame, &1))\\nend )\\nKino.Layout.grid([form, frame],  boxed:  true,  gap:  \\n16)\\nThis cell will make use of Kino form controls again. The code creates an\\nimage upload component and listens for input. Once you upload an image,\\nthe code parses the image data into an Nx tensor and passes it through the\\nserving you declared in the previous cell. The serving returns scores for the\\ntop k labels. You then use Kino to render these scores and labels into a\\nscorecard. After the cell runs, you’ll see this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 528}),\n",
       " Document(page_content='\\xa0\\n\\xa0\\nAgain, with only a few lines of code you’re able to make use of a powerful\\nimage classification model.\\nFine-Tuning Pre-trained Models', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 529}),\n",
       " Document(page_content='All of these examples demonstrate the practicality of Bumblebee for\\ninference tasks, which are well-suited to using pre-trained models without\\nany need for additional training. But what happens if you have a use case\\nwhich requires additional training? Fortunately, as you learned in Chapter 8,\\n Stop Reinventing the Wheel , it’s possible to fine-tune pre-trained models in\\nAxon to your specific use case. Models imported from Bumblebee are no\\nexception.\\nAs with pre-trained image models, it’s common to make use of pre-trained\\nlanguage models as a base representation for more specific language tasks.\\nIn this example, you’ll fine-tune a pre-trained Transformer to categorize\\nYelp Reviews. Each review consists of the actual review text and the\\nrating associated with the review. This should feel similar to the problem\\nyou solved in Chapter 9,  Understand Text —classifying movie reviews. In\\nthis case, however, each rating belongs to one of five possible categories.\\nStart by downloading the dataset from here.\\nNext, import a pre-trained transformer with the following code:\\n{ :ok , spec} = Bumblebee.load_spec({ :hf ,  \"  \\ndistilbert-base-cased\" },\\n   module:  Bumblebee.Text.Distilbert,\\n   architecture:   :for_sequence_classification \\n)\\nspec = Bumblebee.configure(spec,  num_labels:  5)\\n[36]\\n[37]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 530}),\n",
       " Document(page_content='{ :ok , %{ model:  model,  params:  params}} = \\nBumblebee.load_model(\\n  { :hf ,  \"  distilbert-base-cased\" },\\n   spec:  spec\\n)\\n{ :ok , tokenizer} = Bumblebee.load_tokenizer({ :hf ,  \\n\"  distilbert-base-cased\" })\\nIn this example, you’re using the DistilBERT  model. DistilBERT is a\\nsmaller, faster version of BERT, which trades off compute and storage for a\\nsmall dip in accuracy. Bidirectional encoder representations of text (BERT)\\nis one of the most famous transformer implementations originally\\ndeveloped at Google. When it was originally integrated in 2017, BERT\\nproved so powerful in practice that it caused one of the most significant\\nchanges to the Google search algorithms in years. While you can adjust this\\nexample to make use of a more powerful model such as BERT, you should\\nnote that larger models are more expensive and slower to train.\\nBefore actually loading the model, you use load_spec/2, which loads the\\nmodel configuration before loading the actual model. You can then edit the\\nconfiguration using Bumblebee.conﬁgure/2. In this example, you configure\\nthe spec to accept num_labels: 5, and in load_model/2, you specify the\\n[38]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 531}),\n",
       " Document(page_content='architecture as :for_sequence_classiﬁcation. Additionally, you pass the\\nreconfigured spec with the :spec option to tell Bumblebee to create a\\nsequence classification model with five output classes.\\nNext, you’ll need to create an input pipeline. If you extract the dataset into a\\nlocal directory, you’ll notice it consists of both train and test CSVs. You can\\nstream a CSV line by line and parse the ratings and review into batches of\\nTensors using this code:\\nbatch_size = 32\\nmax_length = 128\\ntrain_data =\\n  File.stream!( \"  yelp_review_full_csv/train.csv\" )\\n  |> Stream.chunk_every(batch_size)\\n  |> Stream.map( fn  inputs ->\\n    {labels, reviews} =\\n      inputs\\n      |> Enum.map( fn  line ->\\n        [label, review] = String.split(line,  \"  \\\\\" ,\\\\ \\n\"  \" )\\n        {String.trim(label,  \"  \\\\\"  \"  ), \\nString.trim(review, \" \\\\ \"  \" )}\\n       end )\\n      |> Enum.unzip()', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 532}),\n",
       " Document(page_content='   labels = labels |> \\nEnum.map(&String.to_integer/1) |> Nx.tensor()\\n    tokens = Bumblebee.apply_tokenizer(tokenizer, \\nreviews,  length:  max_length)\\n    {tokens, labels}\\n   end )\\nThis code will stream the file line by line in chunks of 32. Each chunk gets\\nsplit into a label and review. Each label is parsed to an integer, and each\\nreview is tokenized using Bumblebee. Reviews are padded or truncated to a\\nmax sequence length of 128 characters. You can take the first sample from\\nyour training data and inspect it like this:\\nEnum.take(train_data, 1)\\nYou’ll see the following:\\n[\\n  {#Nx.Tensor<\\n     s64[32]\\n     [5, 2, 4, 4, 1, 5, 5, 1, 2, 3, 1, 1, 4, 2, \\n...]\\n   >,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 533}),\n",
       " Document(page_content='  %{\\n     \"attention_mask\" => #Nx.Tensor<\\n       s64[32][128]\\n       [\\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \\n1, 1, 1, ...],\\n         ...\\n       ]\\n     >,\\n     \"input_ids\" => #Nx.Tensor<\\n       s64[32][128]\\n       [\\n         [101, 173, 1197, 119, 2284, 2953, 3272, \\n1917, ...],\\n         ...\\n       ]\\n     >,\\n     \"token_type_ids\" => #Nx.Tensor<\\n       s64[32][128]\\n       [\\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\n0, 0, ...],\\n         ...', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 534}),\n",
       " Document(page_content='      ]\\n     >\\n   }}\\n]\\nNow, you need to tweak your model a little bit. The models returned from\\nBumblebee return outputs in addition to those you care about for training.\\nIn this instance, the model output is a map consisting of :attentions,\\n:hidden_states, and :logits. You can verify this by running the following:\\nAxon.get_output_shape(model, %{ \"  input_ids\"  => \\nNx.template({32, 128},  :s64 )})\\nAnd you’ll see this output:\\n%{\\n  attentions: #Axon.None<...>,\\n  hidden_states: #Axon.None<...>,\\n  logits: {32, 5}}\\nYou only care about :logits in this case. Before passing the model to a\\ntraining loop, you’ll need to extract logits specifically. You can do so by\\nadding the following layer to your model:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 535}),\n",
       " Document(page_content='model = Axon.nx(model,  fn  %{ logits:  logits} -> \\nlogits  end )\\nThis will create a new model that only returns :logits.\\nNow all you need to do is create a training loop. This problem is a multi-\\nclass classification problem, so you should be pretty familiar with how to\\nsolve it. You can implement your loop with the following code:\\noptimizer = Axon.Optimizers.adamw(5.0e-5)\\nloss = &Axon.Losses.categorical_cross_entropy(&1, \\n&2,\\n   from_logits:  true,\\n   sparse:  true,\\n   reduction:   :mean \\n)\\ntrained_model_state =\\n  model\\n  |> Axon.Loop.trainer(loss, optimizer,  log:  1)\\n  |> Axon.Loop.metric( :accuracy )\\n  |> Axon.Loop.run(train_data, params,  epochs:  3,  \\ncompiler:  EXLA)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 536}),\n",
       " Document(page_content='And that’s all you need. Note that transformers are often significantly larger\\nand more expensive to run than the other types of models you’ve\\nimplemented in this book. You may end up waiting for a while for results.\\nYou can validate your training process by only training on a small portion\\nof the original dataset. After running this code, you’ll see this:\\nEpoch: 0, Batch: 249, accuracy: 0.3605000 loss: \\n1.1831361\\nEpoch: 1, Batch: 249, accuracy: 0.5363751 loss: \\n1.0118284\\nEpoch: 2, Batch: 249, accuracy: 0.6470000 loss: \\n0.8826395\\nAfter your model finishes training, you can create a test pipeline with the\\ngiven test data and evaluate your trained model with the following code:\\nmodel\\n|> Axon.Loop.evaluator()\\n|> Axon.Loop.metric( :accuracy )\\n|> Axon.Loop.run(test_data, params,  compiler:  \\nEXLA)\\nAnd you’ll see this:\\nBatch: 49, accuracy: 0.3675000', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 537}),\n",
       " Document(page_content='Note that the 36% accuracy shown here demonstrates the accuracy of the\\nmodel after training on a small subset of the original data. For a more\\npowerful classifier, you can continue training for a longer duration. With a\\nfew lines of code, you fine-tuned a powerful pre-trained model into a\\npowerful classifier capable of categorizing Yelp reviews.\\nOnly the Beginning\\nWhile this section specifically highlights four different use cases of\\nBumblebee, the actual number of practical use cases far exceeds that. As\\nyou continue your ML journey, you’ll encounter transformers in use\\neverywhere for search applications, image generation, document\\nclassification, text summarization, machine translation, and more. The\\nchances are also good that you’ll be able to make use of the same models\\nyou find others using simply by importing a pre-trained implementation\\nwith Bumblebee.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 538}),\n",
       " Document(page_content='[35]\\nWrapping Up\\nIn this chapter, you harnessed the power of transformers on a variety of\\nmachine learning tasks. You learned about what transformers are and where\\nthe intuition behind transformers comes from. You learned about attention\\nand how slight modifications to the original attention mechanism lead to the\\npower of transformers. You used Bumblebee to make use of powerful pre-\\ntrained transformers to solve a variety of different tasks. You put\\nBumblebee in action on inference tasks, and you fine-tuned a pre-trained\\nmodel with Bumblebee and Axon.\\nAt this point, you’ve implemented the majority of the most important types\\nof deep learning models in use. You know how to solve almost any type of\\nproblem on any type of data using Axon. Of course, almost every one of the\\nproblems you’ve solved so far assumes access to a large amount of labeled\\ndata. But what happens if you don’t have any input data?\\nIn the next chapter, you’ll explore some approaches to deep unsupervised\\nlearning and generative modeling with Axon. This will wrap up your\\njourney into deep learning with Axon, and you’ll be ready to move on to\\nputting machine learning into practice with Nx, Axon, and Elixir.\\nFOOTNOTES\\nhttps://github.com/elixir-nx/bumblebee', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 539}),\n",
       " Document(page_content='[36]\\n[37]\\n[38]\\nhttps://huggingface.co/datasets/yelp_review_full/tree/main\\nhttps://s3.amazonaws.com/fast-ai-nlp/yelp_review_full_csv.tgz\\nhttps://huggingface.co/docs/transformers/model_doc/distilbert\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 540}),\n",
       " Document(page_content='Chapter 12\\nLearn Without Supervision\\n\\xa0\\nThroughout this book, you’ve primarily been focused on problems that fall\\nin the domain of supervised learning. Remember, a supervised learning\\nproblem is one where you have access to labeled data at training time.\\nAlgorithms for supervised learning problems are great in theory because\\nthey typically have well-established solutions and are often easier to\\ninterpret and understand.\\nUnfortunately, the reality is that a majority of the data you encounter is\\nunlabeled. Equally problematic is that labeling data is an expensive,\\ntedious, and time-consuming task. While most machine learning solutions\\nwould benefit from some level of supervision, this isn’t possible to achieve\\nin all cases. In these cases, it’s necessary to take advantage of unsupervised\\nlearning.\\nRecall from Chapter 1,  Make Machines That Learn , that unsupervised\\nlearning is learning to capture relationships in data without supervision\\nsignals. Essentially, you learn from data without labels. In this chapter,\\nyou’ll apply deep unsupervised learning, which is a subset of unsupervised\\nlearning that depends on the power of neural networks, to solve', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 541}),\n",
       " Document(page_content='compression and generative modeling problems. Specifically, you’ll\\nimplement three types of unsupervised neural networks: autoencoders,\\nvariational autoencoders, and generative adversarial networks. You’ll use\\nthese networks to generate images of handwritten digits.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 542}),\n",
       " Document(page_content='Compressing Data with Autoencoders\\nIf you’re familiar with compression, you already know a bit about\\nunsupervised learning. A somewhat close relationship exists between the\\nobjectives of compression and the objectives of unsupervised learning. In a\\ncompression algorithm, your goal is to reduce the size of the original data—\\ntypically by taking advantage of patterns and structures of the input data. In\\nan unsupervised learning problem, your goal is to capture or model the\\npatterns and structures of the input data.\\nIf you consider a compression algorithm as an Elixir behavior, the contract\\nlook something like this:\\n@callback compress(data) :: compressed_data\\n@callback decompress(compressed_data) :: data\\nRecall from Chapter 6,  Go Deep with Axon , that neural networks are\\nfunction approximators. Because neural networks simply approximate\\nfunctions, you can use them to approximate functions for compression and\\ndecompression. Your neural network learns to compress data, and then you\\ncan train an additional neural network to decompress the compressed form.\\nThis type of neural network is known as an autoencoder.\\nAn autoencoder is a neural network that consists of an encoder and a\\ndecoder. The encoder learns a latent representation of input data. A latent', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 543}),\n",
       " Document(page_content='representation is a compressed representation of input data in which similar\\nitems are close together in space. For example, if you had a latent\\nrepresentation of animals, the latent representations of a tiger and a cat\\nwould be more similar than the latent representations of an alligator and a\\nhuman. The decoder learns to reconstruct input data from the latent\\nrepresentation. The goal of an autoencoder is to map an input to a latent\\nrepresentation and back with minimal information loss. The output of the\\ndecoder should resemble the input as much as possible.\\nTo see how autoencoders work in practice, fire up a Livebook and install\\nthe following dependencies:\\nMix.install([\\n  { :scidata ,  \"  ~> 0.1\" },\\n  { :axon ,  \"  ~> 0.5\" },\\n  { :exla ,  \"  ~> 0.5\" },\\n  { :nx ,  \"  ~> 0.5\" },\\n  { :kino ,  \"  ~> 0.8\" }\\n])\\nNothing new here. With your dependencies installed, you can get started\\nworking on an input pipeline. To simplify this example, you’ll use the\\nMNIST dataset. You’ve worked with MNIST a few times throughout this\\nbook, so you should be pretty familiar with it by now. MNIST requires', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 544}),\n",
       " Document(page_content='minimal preprocessing. You just need to convert the massive binary to a\\nstream consumable by your training loop:\\nbatch_size = 64\\n{{data, type, shape}, _} = \\nScidata.MNIST.download()\\ntrain_data =\\n  data\\n  |> Nx.from_binary(type)\\n  |> Nx.reshape({ :auto , 28, 28, 1})\\n  |> Nx.divide(255)\\n  |> Nx.to_batched(batch_size)\\nThis code downloads MNIST data and metadata using Scidata and then\\nprocesses the data into batches of 64 images. MNIST images are 28x28\\ngrayscale images, so each input has one color channel with a height and\\nwidth of 28 pixels.\\nYou can now move forward with implementing your model. First, run the\\nfollowing code:\\ndefmodule  Autoencoder  do \\n   def  encoder(input)  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 545}),\n",
       " Document(page_content='   input\\n    |> Axon.flatten()\\n    |> Axon.dense(256,  activation:   :relu ,  name:   \"  \\nencoder_dense_0\" )\\n    |> Axon.dense(128,  activation:   :relu ,  name:   \"  \\nencoder_dense_1\" )\\n   end \\n   def  decoder(input)  do \\n    input\\n    |> Axon.dense(256,  activation:   :relu ,  name:   \"  \\ndecoder_dense_0\" )\\n    |> Axon.dense(784,  activation:   :sigmoid ,  name:  \\n\"  decoder_dense_1\" )\\n    |> Axon.reshape({ :batch , 28, 28, 1})\\n   end \\nend \\nThis code implements an autoencoder module with two functions: encoder/1\\nand decoder/1. The encoder flattens the input for consumption by two dense\\nlayers. Each dense layer is followed by a ReLU activation function. It’s\\nimportant to note that the final encoder shape will be {batch_size, 128}. You\\nshould notice that the 128-length vector is smaller than the original\\ndimensionality of each input. Remember, the point is to compress the input.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 546}),\n",
       " Document(page_content='The output of the encoder is the latent representation of the input or the\\nlatent space. You might find it useful to mess around with different latent\\nconfigurations to see how they affect the overall model.\\nThe decoder takes an input and transforms it with two dense layers\\nfollowed by a final reshape to match the original shape of the input. The\\nfinal dense layer consists of a sigmoid activation. The final activation is\\nimportant because your input pipeline scales input pixels to be between 0\\nand 1, so you need your output to also have pixel values between 0 and 1.\\nNotice that the output dimensionality matches the input dimensionality.\\nNow, you can use your helpers to declare your joint autoencoder model:\\nmodel =\\n  Axon.input( \"  image\" )\\n  |> Autoencoder.encoder()\\n  |> Autoencoder.decoder()\\nmodel\\nThe model will look like this:\\n#Axon<\\n  inputs: %{\"image\" => nil}\\n  outputs: \"reshape_0\"\\n  nodes: 11', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 547}),\n",
       " Document(page_content='>\\nYour model implementation is relatively small, but it’s the first time you’ve\\nused a module and helper functions to implement a model. While in some\\ninstances it makes sense to declare a model as a sequence of Axon function\\ncalls, in other cases it makes sense to extract reusable blocks into helper\\nfunctions. In this instance, your model has two clear sub-architectures that\\nmight be useful on their own, so it’s best to declare them separately for later\\nuse. The important thing to understand is that Axon is still just a regular\\nElixir library. You should embrace Elixir patterns when using Axon.\\nNow you can focus on implementing your training loop. The goal of your\\nautoencoder is to produce a network that encodes the original input into a\\nlower-dimensional representation and decodes the representation into an\\noutput that’s identical to the input. Following this logic, you can see that\\nyou can kind of turn this into a supervised learning problem by treating\\nyour original input as both your input data and output labels. You have\\nplenty of experience implementing supervised training loops. But what\\nshould your loss function be?\\nYou want a loss function that, when minimized, minimizes the distance\\nbetween your input and autoencoder output. If you treat the distance\\nbetween input and output as the pairwise distance between pixels, it follows\\nthat you should use :mean_squared_error:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 548}),\n",
       " Document(page_content='trained_model_state =\\n  model\\n  |> Axon.Loop.trainer( :mean_squared_error , \\nAxon.Optimizers.adam(1.0e-3))\\n  |> Axon.Loop.run(\\n    Stream.zip(train_data, train_data),\\n    %{},\\n     epochs:  5,\\n     compiler:  EXLA\\n  )\\nNotice that the training loop is similar to many of the training loops you’ve\\nimplemented in this book. By treating the input as both the input and label,\\nyou’ve turned this unsupervised learning problem into a supervised learning\\nproblem. Now, you could run this loop and watch the loss decrease, but it’s\\ndifficult to interpret how this loss corresponds to how well your model is\\nreconstructing compressed inputs. A much more interpretable measure of\\nprogress is a periodic visualization of your model’s outputs on some\\nexample data. You can accomplish this with Axon event handlers.\\nThe Axon.Loop reduction fires deterministic events at different points in the\\nloop cycle. For example, at the beginning and end of each epoch, the\\n:epoch_started and :epoch_completed events fire. You can register handlers\\nwith each of these events to trigger custom functions to run inside the loop.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 549}),\n",
       " Document(page_content='This gives you more fine-grained control over what happens during the\\nloop.\\nHandlers are arity-1 functions that take a loop state and return a control-\\nflow/state tuple: {:continue | :halt_epoch | :halt_loop, state}. Internally, Axon.Loop\\naggregates a struct Axon.Loop.State which contains loop metadata, metrics,\\nand step state. When a handler is triggered after an event fires, the handler\\nreceives the current Axon.Loop.State struct. A handler can then use this state\\nto perform side-effecting operations such as saving loop or model\\ncheckpoints, inspecting model metrics, or, in this case, visualizing a\\nmodel’s predictions. Handlers can also choose to terminate a loop based on\\nsome criteria, by passing the control-flow term :halt in the control-flow/state\\ntuple.\\nYou already have experience with some event handlers that Axon offers out\\nof the box including checkpoints and early stopping. These handlers are\\nimplemented on top of the more general and readily-customizable event-\\nhandler API. Your custom event handler needs to visualize your model’s\\npredictions—which you can do using Kino’s built-in image and rendering\\nfunctionality. Copy the following code to implement your custom event\\nhandler:\\ntest_batch = Enum.at(train_data, 0)\\ntest_image = test_batch[0] |> Nx.new_axis(0)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 550}),\n",
       " Document(page_content='visualize_test_image =  fn \\n  %Axon.Loop.State{ step_state:  step_state} = state \\n->\\n    out_image = Axon.predict(\\n      model,\\n      step_state[ :model_state ],\\n      test_image,\\n       compiler:  EXLA\\n    )\\n    out_image =\\n      out_image\\n      |> Nx.multiply(255)\\n      |> Nx.as_type( :u8 )\\n      |> Nx.reshape({28, 28, 1})\\n    Kino.Image.new(out_image) |> Kino.render()\\n    { :continue , state}\\nend \\nYour visualization handler needs to make a prediction on your test image\\nand then convert the floating point result to an unsigned 8-bit integer tensor.\\nThat’s because Kino.Image expects image tensors to be created from u8\\ntensors. To cast from float to u8, you can multiply the image by 255 and use', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 551}),\n",
       " Document(page_content='Nx.as_type/2. Note that this approach is lossy as you’re going from a 32-bit\\nto an 8-bit tensor; but for this example, that won’t matter much.\\nTo get the predictions from your model, you need the model, its parameters,\\nand an input. To get the most up-to-date model parameters, you need to\\naccess the :model_state within the loop state’s step state. That’s a bit of a\\nmouthful, but it’s easy to understand with a little understanding of how the\\nAxon.Loop API works.\\nThe Axon.Loop API is an abstraction for building the kind of reductions\\ncommonly encountered in machine learning applications. These reductions\\niterate over datasets and aggregate state—whether it be metrics or actual\\nmodel state. At the core, this reduction looks something like this:\\ndef  run(data)  do \\n  state = init_state()\\n  Enum.reduce(data, state,  fn  batch, state ->\\n    step(batch, state)\\n   end )\\nend \\nWhat you care about at the end of this loop is the final value of state. For a\\nsupervised training loop, this state contains the model state, but it also\\ncontains some additional metadata such as internal optimizer state,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 552}),\n",
       " Document(page_content='aggregate loss, and more. The Axon.Loop API aggregates this state for you.\\nBut to generalize better to other types of loops, it needs to track some\\nadditional metadata. This metadata is baked into the Axon.Loop.State struct.\\nThe meat and potatoes of this struct is the step_state which maps directly to\\nstate in the run/1 implementation shown previously. In most custom\\nhandlers, you’ll only ever touch the step_state portion of the Axon.Loop.State\\nstruct. But the additional metadata is there for you—if you need it.\\nWith your handler implemented, you need to make some slight adjustments\\nto your training loop to register your event handler:\\ntrained_model_state =\\n  model\\n  |> Axon.Loop.trainer( :mean_squared_error , \\nAxon.Optimizers.adam(1.0e-3))\\n  |> Axon.Loop.handle_event( :epoch_completed , \\nvisualize_test_image)\\n  |> Axon.Loop.run(\\n    Stream.zip(train_data, train_data),\\n    %{},\\n     epochs:  5,\\n     compiler:  EXLA\\n  )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 553}),\n",
       " Document(page_content='Axon.Loop.handle/3 registers an event handler to run on the given event. In\\nthis example, you register visualize_test_image to run on each :epoch_completed\\nevent. This means that after each epoch, you’ll get a nice visualization of an\\nexample model output—giving you a good idea of how well your model is\\nable to deconstruct and then reconstruct an input example.\\nAfter running this code, you’ll see the following output:\\n\\xa0\\n\\xa0', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 554}),\n",
       " Document(page_content='At this point, you might be thinking that, while your model is able to\\ncompress and decompress images pretty well, training a neural network to\\ndo this seems like overkill when there are much more efficient ways to\\nsolve the compression problem. And you’re absolutely right—so what’s the\\npoint?\\nWell, if you think about it, you’ve accidentally trained a deep generative\\nmodel. A deep generative model is a deep learning model designed to\\ngenerate data from some distribution. In this example, your decoder is\\nactually kind of a generative model—it takes a latent representation and\\nproduces an output. Hypothetically, you can skip the encoder altogether and\\ngive the decoder some random latent representation, and it should give you\\nan output that resembles a handwritten digit:\\ndecoder_only =\\n  Axon.input( \"  noise\" )\\n  |> Autoencoder.decoder()\\nkey = Nx.Random.key(42)\\n{noise, _key} = Nx.Random.normal(key,  shape:  {1, \\n128})\\nout_image = Axon.predict(decoder_only, \\ntrained_model_state, noise)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 555}),\n",
       " Document(page_content='upsampled = Axon.Layers.resize(out_image,  size:  \\n{512, 512})\\nout_image =\\n  upsampled\\n  |> Nx.reshape({512, 512, 1})\\n  |> Nx.multiply(255)\\n  |> Nx.as_type( :u8 )\\nKino.Image.new(out_image)\\nThis code is pretty much identical to your visualization handler; however,\\nyou add a function to upsample the input using Axon’s resize layer. This\\nwill resize the input image to be much larger than the original 28x28. After\\nrunning this code, you’ll see the following output:\\nThat’s definitely not a handwritten digit. So what’s going on here?', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 556}),\n",
       " Document(page_content='In spirit, your decoder would make a great generative model with access to\\nthe right latent representations. Unfortunately, you didn’t force your model\\nto have a latent space with structure. The structure of your encoded\\nrepresentations is at the mercy of a neural network and gradient descent.\\nYou can’t pass random uniform or normal noise to your decoder and expect\\ncoherent output because your decoder only knows how to handle latent\\nrepresentations produced by the encoder. But what if there was a way to\\nforce your encoder to learn a structured representation, which you can\\neasily query later on? Fortunately, there is. Enter the variational\\nautoencoder.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 557}),\n",
       " Document(page_content='Learning a Structured Latent\\nWhile trying to implement a basic compression algorithm, you’ve managed\\nto stumble upon a strategy for generating arbitrary input data. But to make\\nit work, you need to tame the latent space. Remember, a latent space is a\\nrepresentation of data where similar inputs lie closely together in the space.\\nFor example, word embeddings and sentence embeddings from Chapter 9,  \\nUnderstand Text , and Chapter 11,  Model Everything with Transformers , are\\nexamples of a learned latent space for text.\\nUnfortunately, the emphasis here is that your latent space is learned. That is\\nto say that it’s difficult to query the latent space without access to\\ninformation from the encoder. To generate images, you need access to a\\ntarget image—which defeats the purpose of the generative model. You need\\na way to force your model to learn a structured latent space. This means\\nyou force your model to follow some input distribution, such as a normal\\ndistribution. Variational autoencoders are designed to do just that.\\nVariational autoencoders are the same as autoencoders—with a slight twist.\\nRather than project inputs down to some lower-dimensional vector\\nrepresentation, variational autoencoders learn to project inputs down to the\\nparameters of a distribution. You can easily sample inputs from this\\ndistribution and use these inputs to generate examples. With some small\\nmodifications, you can turn a basic autoencoder into a generative model.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 558}),\n",
       " Document(page_content='In a new cell, create a new module named VAE:\\ndefmodule  VAE  do \\n   import  Nx.Defn\\nend \\nNext, add the following function to implement the encoder:\\ndef  encoder(input)  do \\n  encoded =\\n    input\\n    |> Axon.conv(32,\\n       kernel_size:  3,\\n       activation:   :relu ,\\n       strides:  2,\\n       padding:   :same \\n    )\\n    |> Axon.conv(32,\\n       kernel_size:  3,\\n       activation:   :relu ,\\n       strides:  2,\\n       padding:   :same \\n    )\\n    |> Axon.flatten()\\n    |> Axon.dense(16,  activation:   :relu )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 559}),\n",
       " Document(page_content=' z_mean = Axon.dense(encoded, 2)\\n  z_log_var = Axon.dense(encoded, 2)\\n  z = Axon.layer(&sample/3, [z_mean, z_log_var],  \\nop_name:   :sample )\\n  Axon.container({z_mean, z_log_var, z})\\nend \\nThis encoder implementation is somewhat different than your first encoder\\nimplementation. Rather than using dense layers for feature extraction, this\\nimplementation uses some convolutional layers. Remember, the goal of\\nyour encoder is to extract a useful representation from the input so your\\ndecoder can use it to reconstruct the output. As you learned in Chapter 7,  \\nLearn to See , convolutional layers are powerful feature extractors for\\nimages. It follows that adding convolutional layers to encode the input will\\nlikely result in a more useful latent representation.\\nIn addition to some slight architectural modifications, you slightly modified\\nthe encoder output. Axon.container/1 is an Axon construct that allows you to\\nwrap layers into Elixir collections such as tuples, maps, and structs.\\nContainers are most commonly used as output layers—like in this example.\\nYour encoder will output a tuple of three variables: z_mean, z_log_var, and z.\\nRather than encode the input into a single dense vector, your new encoder', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 560}),\n",
       " Document(page_content='returns the parameters of a distribution. These parameters are the result of\\nlearned transformations of the input. From the parameters z_mean and\\nz_log_var, your model produces a latent z using a custom Axon layer.\\nCustom layers in Axon are defined with the Axon.layer/3 function. You can\\nremember the form of Axon.layer/3 by keeping in mind that it’s nearly\\nidentical to Elixir’s apply. You pass a function, typically defined with defn,\\nand a list of inputs. Axon.layer/3 also accepts some additional metadata\\noptions for defining the layer’s name and what operation it performs. You\\nmight be wondering, why is sample/3 an arity-3 function if you’re only\\ngiving it two inputs. Each Axon layer takes an additional opts argument.\\nEvery layer must accept at least the :mode option—which changes the layer\\nbehavior between training and inference.\\nNow, add the following sample/3 implementation to your module:\\ndefnp sample(z_mean, z_log_var, _opts \\\\\\\\ [])  do \\n  noise_shape = Nx.shape(z_mean)\\n  epsilon = Nx.random_normal(noise_shape)\\n  z_mean + Nx.exp(0.5 * z_log_var) * epsilon\\nend \\nThis method implements what is known as the reparameterization trick.\\nRemember, your goal is to learn a structured latent. In this instance, you’ve\\nestimated two parameters of a normally distributed latent space: its mean', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 561}),\n",
       " Document(page_content='and log-variance. Ideally, you could just use these parameters to sample\\nfrom a normal distribution with Nx.random_normal/3. But you cannot take the\\ngradient of a random node. If you tried to sample just from\\nNx.random_normal/3 and then attempted to train this model with gradient\\ndescent, there would be no feedback signal to z_mean and z_log_var. The\\nreparameterization trick makes the sampling a function of z_mean and\\nz_log_var in such a way that your model still produces a normally distributed\\nlatent, but z_mean and z_log_var can learn from signals during gradient\\ndescent.\\nOne thing to note here is that the pattern of pairing custom layers with defn\\nimplementations is the best way to add custom functionality to your model.\\nThis is actually how Axon behaves under the hood. Every Axon layer has a\\nbackend implementation in the Axon.Layers module, with a frontend interface\\nin the Axon module.\\nNext, add the following function to your module to implement the decoder:\\ndef  decoder(input)  do \\n  input\\n  |> Axon.dense(7 * 7 * 64,  activation:   :relu )\\n  |> Axon.reshape({ :batch , 7, 7, 64})\\n  |> Axon.conv_transpose(64,\\n     kernel_size:  {3, 3},\\n     activation:   :relu ,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 562}),\n",
       " Document(page_content='    strides:  [2, 2],\\n     padding:   :same \\n  )\\n  |> Axon.conv_transpose(32,\\n     kernel_size:  {3, 3},\\n     activation:   :relu ,\\n     strides:  [2, 2],\\n     padding:   :same \\n  )\\n  |> Axon.conv_transpose(1,\\n     kernel_size:  {3, 3},\\n     activation:   :sigmoid ,\\n     padding:   :same \\n  )\\nend \\nThis decoder implementation is also slightly different from your previous\\none. Both decoder implementations use convolutional layers instead of\\ndense layers. The difference is that this decoder implementation uses\\ntransposed convolutions instead of plain convolutions. Transposed\\nconvolutions behave somewhat similarly to traditional convolutions, but\\nthey can be used as a strategy for upsampling rather than downsampling. If\\nyou want to inspect the layer-by-layer shape of your decoder, you use this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 563}),\n",
       " Document(page_content='template = Nx.template({1, 128},  :f32 )\\nAxon.Display.as_graph(VAE.decoder(Axon.input( \"  \\nlatent\" )), template)\\nAnd you’ll see the output as shown in the image.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 564}),\n",
       " Document(page_content='', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 565}),\n",
       " Document(page_content='The size of each spatial dimension increases by a factor of 2 after each\\nsubsequent call to Axon.conv_transpose/3. You’ll often see\\nAxon.conv_transpose/3 used in generative architectures such as this one.\\nAt this point, you’ve successfully implemented a variational autoencoder.\\nNow, you need to train it.\\nIn the previous section, you were able to turn your unsupervised learning\\nproblem into a supervised learning problem by using the inputs as labels.\\nThis meant you could make use of Axon’s out-of-the-box APIs for\\nsupervised training loops. Unfortunately, that won’t work for training your\\nvariational autoencoder. Variational autoencoders require some specific\\ntweaks for the training process to work. You’ll need to write your first\\ncustom training loop.\\nWriting a custom training loop in Axon is easier than you think and is a\\ngood exercise in understanding what Axon does for you under the hood. To\\nstart, look at what Axon.Loop.trainer/3 returns for you on a dummy model by\\nentering this:\\nmodel = Axon.input( \"  foo\" )\\nloop  = Axon.Loop.trainer(model,  \\n:binary_cross_entropy ,  :sgd )\\nIO.inspect  loop ,  structs:  false', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 566}),\n",
       " Document(page_content='You’ll see the following output:\\n%{\\n  __struct__: Axon.Loop,\\n  attached_state: nil,\\n  handlers: %{\\n    completed: [],\\n    epoch_completed: [\\n      {#Function<24.105974664/1 in \\nAxon.Loop.log/5>,\\n       #Function<5.105974664/1 in \\nAxon.Loop.build_filter_fn/1>}\\n    ],\\n    epoch_halted: [],\\n    epoch_started: [],\\n    halted: [],\\n    iteration_completed: [\\n      {#Function<24.105974664/1 in \\nAxon.Loop.log/5>,\\n       #Function<3.105974664/1 in \\nAxon.Loop.build_filter_fn/1>}\\n    ],\\n    iteration_started: [],\\n    started: []', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 567}),\n",
       " Document(page_content=' },\\n  init: #Function<136.105575625/2 in \\nNx.Defn.wrap_arity/2>,\\n  metrics: %{\\n    \"loss\" => {#Function<12.116147200/3 in \\nAxon.Metrics.running_average/1>,\\n     #Function<6.105974664/2 in \\nAxon.Loop.build_loss_fn/1>}\\n  },\\n  output_transform: #Function<37.105974664/1 in \\nAxon.Loop.trainer/4>,\\n  step: #Function<136.105575625/2 in \\nNx.Defn.wrap_arity/2>\\n}\\n\\xa0\\nMost of this is internal information Axon uses to control the execution of\\nthe loop. The most important fields here are :init and :step. Every single\\nAxon loop needs an initialization function for initializing the step state and\\na step function for updating the step state from the current input and current\\nstep state. Axon.Loop.trainer is a factory that generates these for you. In\\nreality, Axon.Loop.trainer makes use of the step factory Axon.Loop.train_step\\nwhich implements initialization and step functions for a supervised training', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 568}),\n",
       " Document(page_content='loop. The big takeaway here is that all you need to do to implement a\\ncustom training loop is implement an initialization and step function.\\nIt’s easiest to start by implementing a single train step. This function takes\\nthe current batch and the current step state and returns an updated step state.\\nYou can think of it as the inner function in an Enum.reduce/3. Your train_step\\nneeds to do the following:\\n1. Determine the gradient of the model with respect to some objective\\nfunction\\n2. Scale gradients using an optimizer and a current state\\n3. Apply scaled gradients to obtain a new model state\\n4. Return the updated state\\nFor tracking purposes, you’ll also want to keep a running average of the\\nloss. With all of this in mind, you can take a shot at implementing your\\ncustom train step within the VAE module:\\ndefn train_step(encoder_fn, decoder_fn, \\noptimizer_fn, batch, state)  do \\n  {batch_loss, joint_param_grads} = \\nvalue_and_grad(\\n    state[ :model_state ],\\n    &joint_objective(encoder_fn, decoder_fn, \\nbatch, &1)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 569}),\n",
       " Document(page_content=' )\\n  {scaled_updates, new_optimizer_state} = \\noptimizer_fn.(\\n    joint_param_grads,\\n    state[ :optimizer_state ],\\n    state[ :model_state ]\\n  )\\n  new_model_state = Axon.Updates.apply_updates(\\n    state[ :model_state ], scaled_updates\\n  )\\n  new_loss =\\n    state[ :loss ]\\n    |> Nx.multiply(state[ :i ])\\n    |> Nx.add(batch_loss)\\n    |> Nx.divide(Nx.add(state[ :i ], 1))\\n  %{\\n    state\\n    |  i:  Nx.add(state[ :i ], 1),\\n       loss:  new_loss,\\n       model_state:  new_model_state,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 570}),\n",
       " Document(page_content='      optimizer_state:  new_optimizer_state\\n  }\\nend \\nWhile it seems like a lot is going on here, it actually follows perfectly from\\nthe steps you just outlined. First, you take the gradient of some\\njoint_objective/4 on batch with respect to :model_state within state. Notice you\\nneed to parameterize joint_objective/4 on encoder_fn and decoder_fn. The\\nfunctions represent the predict functions for the encoder and decoder,\\nrespectively.\\nNext, you use the given optimizer_fn to scale the joint_param_grads with the\\ngiven :optimizer_state and :model_state in state. optimizer_fn is an update\\nfunction that scales the gradients according to some algorithm such as\\nAdam before you use them to update your model state.\\nThe next line actually applies the scaled updates to :model_state using\\nAxon.Updates.apply_updates. The rest of the implementation is for tracking\\npurposes—you need to update the loss according to the given batch loss and\\nthen return an entirely new state.\\nNotice that train_step/5 is a defn. You want to try as much as possible to\\nimplement loop initialization and step functions as defn in order to take\\nadvantage of hardware acceleration and JIT compilation during the training\\nprocess.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 571}),\n",
       " Document(page_content='Now, you need to implement the joint_objective/4 defined in train_step/5. Add\\nthe following function to your VAE module:\\ndefnp joint_objective(encoder_fn, decoder_fn, \\nbatch, joint_params)  do \\n  %{ prediction:  preds} = encoder_fn.(joint_params[ \\n\"  encoder\" ], batch)\\n  {z_mean, z_log_var, z} = preds\\n  %{ prediction:  reconstruction} = decoder_fn.\\n(joint_params[ \"  decoder\" ], z)\\n  recon_loss = Axon.Losses.binary_cross_entropy(\\n    batch, reconstruction,  reduction:   :mean \\n  )\\n  kl_loss = -0.5 * (1 + z_log_var - \\nNx.power(z_mean, 2) - Nx.exp(z_log_var))\\n  kl_loss = Nx.mean(Nx.sum(kl_loss,  axes:  [1]))\\n  recon_loss + kl_loss\\nend \\nThis code implements a joint-objective function using both the encoder and\\ndecoder. First, you get an encoded representation from the encoder by\\ncalling encoder_fn/2 on the \"encoder\" parameters in joint_params and the', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 572}),\n",
       " Document(page_content='current batch. This returns the tuple {z_mean, z_log_var, z}. Next, you\\nreconstruct the input from the encoded latent output z by calling decoder_fn.\\nBoth encoder_fn and decoder_fn return a map—this is the case for all Axon\\nmodels which are called in training mode. The training mode output is a\\nmap with a key :prediction representing the output and a key :state for the\\nupdated internal layer state for stateful layers such as Axon.batch_norm/3.\\nYour model doesn’t have any stateful layers, so you can safely ignore layer\\nstate.\\nThe joint-objective function uses all of the model’s outputs to form a joint-\\nloss. The first part of the loss is a reconstruction loss which measures how\\nwell your decoder reconstructs the original batch of images from the\\nencoded representation. The second part of the loss is a regularization\\nwhich penalizes the model from drifting too far away from a normal\\ndistribution. It basically measures the difference of the distribution defined\\nby the given parameters z_mean and z_log_var from a normal distribution\\nwith mean 1 and variance 0. This term helps coerce your encoder into\\nlearning to encode your data as a normal distribution.\\nNow that you’ve implemented your training step, you can implement your\\ninitialization function. From your training step, it follows that you need to\\ntrack the current iteration i, the current loss :loss, the current model state\\n:model_state, and the current optimizer state :optimizer_state:\\ndefn init_step(', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 573}),\n",
       " Document(page_content=' encoder_init_fn,\\n  decoder_init_fn,\\n  optimizer_init_fn,\\n  batch,\\n  init_state\\n)  do \\n  encoder_params = encoder_init_fn.(batch, \\ninit_state)\\n  decoder_params = decoder_init_fn.\\n(Nx.random_uniform({64, 2}), init_state)\\n  joint_params = %{\\n     \"  encoder\"  => encoder_params,\\n     \"  decoder\"  => decoder_params\\n  }\\n  optimizer_state = optimizer_init_fn.\\n(joint_params)\\n  %{\\n     i:  Nx.tensor(0),\\n     loss:  Nx.tensor(0.0),\\n     model_state:  joint_params,\\n     optimizer_state:  optimizer_state,\\n  }\\nend ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 574}),\n",
       " Document(page_content='This function uses the initialization functions for both the encoder and\\ndecoder. The function initializes separate encoder and decoder parameters\\ninside a joint parameter map. Then, you combine the encoder and decoder\\nparameters into a single map, joint_params. It’s necessary to join the\\nparameters because you want to use a single optimizer for both models, so\\nboth sets of parameters must be within a single structure. Next, you call\\noptimizer_init_fn on your joint parameters to initialize the optimizer state\\nbefore finally returning the initial state.\\nYou now have everything you need to define a training loop for your VAE.\\nTo see how all of this comes together, start by defining your encoder and\\ndecoder:\\nencoder = Axon.input( \"  image\" ) |> VAE.encoder()\\ndecoder = Axon.input( \"  latent\" ) |> VAE.decoder()\\nCurrently, both your encoder and decoder are Axon structs. To make use of\\nthem within defn, you need to build them into their {init_fn, predict_fn} tuples\\nusing Axon.build/2:\\n{encoder_init_fn, encoder_fn} = \\nAxon.build(encoder,  mode:   :train )\\n{decoder_init_fn, decoder_fn} = \\nAxon.build(decoder,  mode:   :train )\\nNext, you can define your optimizer:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 575}),\n",
       " Document(page_content='{optimizer_init_fn, optimizer_fn} = \\nAxon.Optimizers.adam(1.0e-3)\\nYou’ve actually constructed optimizers in this way before. But you’ve never\\ndecomposed them into distinct parts. Every optimizer is a tuple of {init_fn,\\nupdate_fn}. With all of your components, you can build parameterized\\ninitialization and train steps:\\ninit_fn = &VAE.init_step(\\n  encoder_init_fn,\\n  decoder_init_fn,\\n  optimizer_init_fn,\\n  &1,\\n  &2\\n)\\nstep_fn = &VAE.train_step(\\n  encoder_fn,\\n  decoder_fn,\\n  optimizer_fn,\\n  &1,\\n  &2\\n)\\nWith an initialization and step function, you can create a loop using the\\nAxon.Loop.loop/2 factory, and then you can work with and execute this loop', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 576}),\n",
       " Document(page_content='in the ways you’re already familiar with. For this example, you’ll also want\\nto add some functionality for displaying generated samples after each\\nepoch. You already know how to do this with an event handler.\\nAdditionally, you’ll want to add some basic logging. You can take care of\\nthis with Axon.Loop.log/4:\\nstep_fn\\n|> Axon.Loop. loop (init_fn)\\n|> Axon.Loop.handle( :epoch_completed , \\n&VAE.display_sample(&1, decoder_fn))\\n|> Axon.Loop.log( :iteration_completed ,  fn \\n  %Axon.Loop.State{ epoch:  epoch,  iteration:  iter,  \\nstep_state:  state} ->\\n   \"  \\\\rEpoch:   #{ epoch }  , batch:   #{ iter }  , loss:   #{ \\nNx.to_number(state[ :loss ]) }  \" \\nend ,  :stdio )\\n|> Axon.Loop.run(train_data, %{},  compiler:  EXLA,  \\nepochs:  10)\\nNotice you need to implement the event handler display_sample/2.\\nAdditionally, you use Axon.Loop.log/4 to attach a log event to the loop. This\\nlogs the string returned from the given function on the given event to the\\ngiven device. In this instance, you simply write some loop metadata such as\\nthe current epoch, iteration, and loss. Before executing this loop, add the\\nfollowing code to your VAE module:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 577}),\n",
       " Document(page_content='def  display_sample(\\n  %Axon.Loop.State{ step_state:  state} = out_state,\\n  decoder_fn\\n)  do \\n  latent = Nx.tensor([[0.0, 0.0], [0.5, 0.5], \\n[1.0, 1.0]])\\n  %{ prediction:  out} = decoder_fn.(state[ \\n:model_state ][ \"  decoder\" ], latent)\\n  out_image = Nx.multiply(out, 255) |> Nx.as_type( \\n:u8 )\\n  upsample = Axon.Layers.resize(\\n    out_image,\\n     size:  {512, 512},\\n     channels:   :first \\n  )\\n  for i <- 0..2  do \\n    Kino.Image.new(Nx.reshape(upsample[i], {512, \\n512, 1})) |> Kino.render()\\n   end \\n  { :continue , out_state}\\nend \\nThis event handler implementation will sample three outputs from the\\ndecoder and convert them to a heatmap before displaying the output. Now,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 578}),\n",
       " Document(page_content='you can execute the loop. After a while you’ll see this:\\nEpoch: 9, batch: 937, loss: 0.5638180375099182\\nYou’ll also see periodic images that look like this:\\nWhoa. Those actually look like handwritten digits. The quality isn’t great,\\nbut it’s apparent the generated output is similar to what your inputs look\\nlike. Now the question is, can you do better? Of course, you can.\\nYour VAE, while impressive, is nowhere near the quality of modern\\ngenerative models. You need only look at DALL-E 2 to understand that\\ndeep generative models are capable of amazing things.[39] DALL-E 2 is an\\nexample of a diffusion model that internally makes use of a VAE and a few\\nother models to generate incredibly realistic images. Before DALL-E and\\ndiffusion models, VAEs were mostly an afterthought. Up until recently, a\\ndifferent kind of model ruled the generative AI game.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 579}),\n",
       " Document(page_content='Generating with GANs\\nVariational autoencoders are somewhat impressive generative models for\\nhow easy they are to train; but up until recently, they weren’t seen as all that\\nuseful for generative AI. Before diffusion models, the most popular and\\nmost powerful generative models were variants of generative adversarial\\nnetworks or GANs.\\nDiffusion Models\\nRecent developments in generative modeling using diffusion\\nhave lead to incredible breakthroughs. Stable Diffusion, a\\nspecific type of diffusion model, has quickly become one of\\nthe most popular deep learning models on the internet. For\\nmany image generation tasks, you’ll find a pre-trained\\ndiffusion model like stable diffusion is much more powerful\\nthan a pre-trained GAN.\\nGenerative adversarial networks are deep generative models that make use\\nof a dueling network architecture with a generator and discriminator. The\\ngenerator is responsible for transforming a latent representation into\\nsomething that resembles inputs from the training data. The discriminator\\nattempts to differentiate real inputs from generated inputs, and in turn, it\\nprovides feedback to the generator to improve its generations.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 580}),\n",
       " Document(page_content='The classic analogy for the relationship between a generator and a\\ndiscriminator in a generative adversarial network is the relationship\\nbetween an art counterfeiter and a critic. The generator is akin to an art\\ncounterfeiter who attempts to create forgeries of famous paintings. The\\ndiscriminator is the critic who attempts to determine if a painting is\\ncounterfeit. The critic provides feedback signals to the counterfeiter which\\nimprove the counterfeiter’s next work until the counterfeiter generates art\\nthat the critic cannot differentiate from the real thing.\\nGenerative adversarial networks have some interesting theoretical\\nguarantees, and in practice, they’ve proven capable of generating incredible\\nimages. But they can be difficult to train when compared to VAEs. More\\nrecent evolutions of generative adversarial networks attempt to alleviate\\nsome of these training difficulties. But the reality is that training a\\ngenerative adversarial network successfully can involve more art than\\nscience. Even so, with enough data and the right training recipe, GAN\\noutputs can be truly incredible.\\nTo get a better sense of how GANs work in practice, you’ll implement a\\nGAN that generates handwritten digits—just like your VAE.\\nYou should already have a Livebook open with all of the dependencies you\\nneed. Start by defining a new module in a new cell:\\ndefmodule  GAN  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 581}),\n",
       " Document(page_content='  import  Nx.Defn\\nend \\nNext, copy the following code into the module to implement your\\ndiscriminator:\\ndef  discriminator(input)  do \\n  input\\n  |> Axon.conv(32,\\n     activation:   :mish ,\\n     kernel_size:  3,\\n     strides:  2,\\n     padding:   :same \\n  )\\n  |> Axon.layer_norm()\\n  |> Axon.conv(64,\\n     activation:   :mish ,\\n     kernel_size:  3,\\n     strides:  2,\\n     padding:   :same \\n  )\\n  |> Axon.layer_norm()\\n  |> Axon.flatten()\\n  |> Axon.dropout( rate:  0.5)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 582}),\n",
       " Document(page_content=' |> Axon.dense(1,  activation:   :sigmoid )\\nend \\nRemember the goal of your discriminator is to determine if an input image\\nis from the real training data or the generator. This should sound a lot like a\\nstraightforward image classification task—so your discriminator only needs\\nto be a powerful enough image classification model. This example makes\\nuse of some convolutional layers followed by layer normalization layers to\\nextract features from the input image before passing them to a dense\\nclassifier.\\nNow you need to implement your generator. You already have some\\nexperience implementing a model that generates image outputs from a\\nlatent representation from the last example. The principles for creating a\\ngenerator for a GAN aren’t that different from creating a decoder for a\\nVAE. You can copy the following code into your GAN module to\\nimplement the GAN generator:\\ndef  generator(input)  do \\n  input\\n  |> Axon.dense(128 * 7 * 7,  activation:   :mish )\\n  |> Axon.reshape({ :batch , 7, 7, 128})\\n  |> Axon.resize({14, 14})\\n  |> Axon.conv(128,  kernel_size:  3,  padding:   :same \\n)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 583}),\n",
       " Document(page_content=' |> Axon.layer_norm()\\n  |> Axon.relu()\\n  |> Axon.resize({28, 28})\\n  |> Axon.conv(64,  kernel_size:  3, padding:   :same )\\n  |> Axon.layer_norm()\\n  |> Axon.relu()\\n  |> Axon.conv(1,  activation:   :tanh ,  kernel_size:  \\n3,  padding:   :same )\\nend \\nThis implementation is slightly different from your decoder implementation\\nin the VAE example. Your generator makes use of a single dense layer to\\ntransform the input latent representation into a representation that matches\\nthe size of your input image. The generator then uses strided convolutional\\nblocks to downsample the input representation, and it upsamples the\\nrepresentation using a combination of convolutional layers and resize\\nlayers. Resize layers in Axon do exactly what they say they do—resize\\ninputs. Specifically, Axon’s resize layers are for upsampling or\\ndownsampling the spatial dimensions of image data. In this instance, you\\nuse them to resize the downsampled spatial dimensions to the correct output\\nsize.\\nNow, run the following to create two new models with your functions:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 584}),\n",
       " Document(page_content='discriminator = GAN.discriminator(Axon.input( \"  \\nimage\" ))\\ngenerator = GAN.generator(Axon.input( \"  latent\" ))\\n\\xa0\\nWith your models implemented, you need to write the training loop. You’ll\\nneed to implement a custom training loop—just as in the VAE example.\\nFortunately, most of the same concepts you learned while writing your VAE\\ntraining loop also apply here. You need to make them work with the GAN\\ntraining framework. Remember, for a custom training loop you need to\\nimplement an initialization function and a step function. It’s easiest to start\\nwith the step function. When training a GAN, the basic workflow is:\\n1. Compute the gradient of the discriminator on its objective\\n2. Scale the gradients by the discriminator’s optimizer\\n3. Update the discriminator\\n4. Compute the gradient of the generator on its objective\\n5. Scale the gradients by the generator’s optimizer\\n6. Update the generator\\nAgain, you’ll also want some basic loss monitoring so you can keep track\\nof training progress. You can implement your train step inside the GAN\\nmodule like this:\\ndefn train_step(', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 585}),\n",
       " Document(page_content='      discriminator_fn,\\n       generator_fn,\\n       discriminator_optimizer,\\n       generator_optimizer,\\n       batch,\\n       state\\n     )  do \\n  d_params = state[ :model_state ][ \"  discriminator\" ]\\n  g_params = state[ :model_state ][ \"  generator\" ]\\nd_optimizer_state = state[ :optimizer_state ][ \"  \\ndiscriminator\" ]\\ng_optimizer_state = state[ :optimizer_state ][ \"  \\ngenerator\" ]\\n# Update discriminator \\n{d_loss, d_grads} = value_and_grad(d_params,  fn  \\nd_params ->\\n  d_objective(\\n    d_params, g_params, discriminator_fn, \\ngenerator_fn, batch\\n  )\\nend )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 586}),\n",
       " Document(page_content='{d_updates, new_d_optimizer_state} = \\ndiscriminator_optimizer.(\\n  d_grads, d_optimizer_state, d_params\\n)\\nnew_d_params = \\nAxon.Updates.apply_updates(d_params, d_updates)\\n# Update generator \\n{g_loss, g_grads} = value_and_grad(g_params,  fn  \\ng_params ->\\n  g_objective(\\n    d_params, g_params, discriminator_fn, \\ngenerator_fn, batch\\n  )\\nend )\\n{g_updates, new_g_optimizer_state} = \\ngenerator_optimizer.(\\n  g_grads, g_optimizer_state, g_params\\n)\\nnew_g_params = \\nAxon.Updates.apply_updates(g_params, g_updates)\\n# Update Losses ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 587}),\n",
       " Document(page_content='new_d_loss =\\n  state[ :loss ][ \"  discriminator\" ]\\n  |> Nx.multiply(state[ :i ])\\n  |> Nx.add(d_loss)\\n  |> Nx.divide(Nx.add(state[ :i ], 1))\\nnew_g_loss =\\n  state[ :loss ][ \"  generator\" ]\\n  |> Nx.multiply(state[ :i ])\\n  |> Nx.add(g_loss)\\n  |> Nx.divide(Nx.add(state[ :i ], 1))\\nnew_loss = %{\\n   \"  discriminator\"  => new_d_loss,\\n   \"  generator\"  => new_g_loss\\n}\\nnew_model_state = %{\\n   \"  discriminator\"  => new_d_params,\\n   \"  generator\"  => new_g_params\\n}\\nnew_optimizer_state = %{\\n   \"  discriminator\"  => new_d_optimizer_state,\\n   \"  generator\"  => new_g_optimizer_state\\n}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 588}),\n",
       " Document(page_content=' %{\\n     model_state:  new_model_state,\\n     optimizer_state:  new_optimizer_state,\\n     loss:  new_loss,\\n     i:  Nx.add(state[ :i ], 1)\\n  }\\nend \\nThis implementation might seem like a lot, but it’s relatively\\nstraightforward once you break it down. First, for convenience, you access\\nthe individual model and optimizer state for the discriminator and generator\\nwith the following code:\\nd_params = state[ :model_state ][ \"  discriminator\" ]\\ng_params = state[ :model_state ][ \"  generator\" ]\\nd_optimizer_state = state[ :optimizer_state ][ \"  \\ndiscriminator\" ]\\ng_optimizer_state = state[ :optimizer_state ][ \"  \\ngenerator\" ]\\nNext, you update the discriminator by computing the gradient of its\\nobjective function with respect to its parameters before scaling the gradients\\nwith its optimizer and applying them to obtain new parameters with the\\nfollowing code:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 589}),\n",
       " Document(page_content='{d_loss, d_grads} = value_and_grad(d_params,  fn  \\nd_params ->\\n  d_objective(\\n    d_params, g_params, discriminator_fn, \\ngenerator_fn, batch\\n  )\\nend )\\n{d_updates, new_d_optimizer_state} = \\ndiscriminator_optimizer.(\\n  d_grads, d_optimizer_state, d_params\\n)\\nnew_d_params = \\nAxon.Updates.apply_updates(d_params, d_updates)\\nYou repeat the same process to update the generator with its own objective\\nfunction and optimizer:\\n{g_loss, g_grads} = value_and_grad(g_params,  fn  \\ng_params ->\\n  g_objective(\\n    d_params, g_params, discriminator_fn, \\ngenerator_fn, batch\\n  )\\nend )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 590}),\n",
       " Document(page_content='{g_updates, new_g_optimizer_state} = \\ngenerator_optimizer.(\\n  g_grads, g_optimizer_state, g_params\\n)\\nnew_g_params = \\nAxon.Updates.apply_updates(g_params, g_updates)\\nNext, you update a running average loss for both the generator and\\ndiscriminator:\\nnew_d_loss =\\n  state[ :loss ][ \"  discriminator\" ]\\n  |> Nx.multiply(state[ :i ])\\n  |> Nx.add(d_loss)\\n  |> Nx.divide(Nx.add(state[ :i ], 1))\\nnew_g_loss =\\n  state[ :loss ][ \"  generator\" ]\\n  |> Nx.multiply(state[ :i ])\\n  |> Nx.add(g_loss)\\n  |> Nx.divide(Nx.add(state[ :i ], 1))\\nFinally, you construct an updated return state:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 591}),\n",
       " Document(page_content='new_loss = %{\\n   \"  discriminator\"  => new_d_loss,\\n   \"  generator\"  => new_g_loss\\n}\\nnew_model_state = %{\\n   \"  discriminator\"  => new_d_params,\\n   \"  generator\"  => new_g_params\\n}\\nnew_optimizer_state = %{\\n   \"  discriminator\"  => new_d_optimizer_state,\\n   \"  generator\"  => new_g_optimizer_state\\n}\\n%{\\n   model_state:  new_model_state,\\n   optimizer_state:  new_optimizer_state,\\n   loss:  new_loss,\\n   i:  Nx.add(state[ :i ], 1)\\n}\\nIf you compare this training step with your VAE training step, you’ll notice\\nthe common structure. In fact, if you routinely write custom training steps,\\nyou’ll notice they all have similar structures and implementations, with\\nsome minor tweaks. In general, the process of writing a custom training', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 592}),\n",
       " Document(page_content='loop is only as difficult as writing a custom objective function. Objective\\nfunctions are the meat and potatoes of the training process because they\\nimplicitly control the performance of your model.\\nThat being said, you still need to implement your objective functions for\\nboth the discriminator and generator. You can start by copying the following\\ncode to implement the discriminator’s objective:\\ndefn d_objective(\\n  d_params,\\n  g_params,\\n  discriminator_fn,\\n  generator_fn,\\n  real_batch\\n)  do \\n  batch_size = Nx.axis_size(real_batch, 0)\\n  real_targets = Nx.broadcast(1, {batch_size, 1})\\n  fake_targets = Nx.broadcast(0, {batch_size, 1})\\n  latent = Nx.random_normal({batch_size, 128})\\n  %{ prediction:  fake_batch} = generator_fn.\\n(g_params, latent)\\n  %{ prediction:  real_labels} = discriminator_fn.\\n(d_params, real_batch)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 593}),\n",
       " Document(page_content=' %{ prediction:  fake_labels} = discriminator_fn.\\n(d_params, fake_batch)\\n  real_loss = Axon.Losses.binary_cross_entropy(\\n    real_targets, real_labels,  reduction:   :mean \\n  )\\n  fake_loss = Axon.Losses.binary_cross_entropy(\\n    fake_targets, fake_labels,  reduction:   :mean \\n  )\\n  0.5 * real_loss + 0.5 * fake_loss\\nend \\nThere isn’t any magic going on here either. You generate fake images by\\nobtaining predictions from your generator on a random latent\\nrepresentation. You finally compute labels for both real training images and\\nfake images using your discriminator. Then you compute the\\ndiscriminator’s loss on both real and fake targets. Because this is a binary\\nclassification problem, you use the binary cross-entropy loss function.\\nAdditionally, you average the discriminator’s loss on both real and fake\\nimages.\\nThe most important takeaway here is that the discriminator’s objective is to\\nclassify real images as real (positive labels) and fake images as fake', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 594}),\n",
       " Document(page_content='(negative labels).\\nNow, you can implement your generator’s objective function by adding the\\nfollowing code to your module:\\ndefn g_objective(\\n  d_params,\\n  g_params,\\n  discriminator_fn,\\n  generator_fn,\\n  real_batch\\n)  do \\n  batch_size = Nx.axis_size(real_batch, 0)\\n  real_targets = Nx.broadcast(1, {batch_size, 1})\\n  latent = Nx.random_normal({batch_size, 128})\\n  %{ prediction:  fake_batch} = generator_fn.\\n(g_params, latent)\\n  %{ prediction:  fake_labels} = discriminator_fn.\\n(d_params, fake_batch)\\n  Axon.Losses.binary_cross_entropy(\\n    real_targets, fake_labels,  reduction:   :mean \\n  )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 595}),\n",
       " Document(page_content='end \\nThis implementation is similar to the objective function for your\\ndiscriminator. You generate a fake batch using the generator and then\\ngenerate a set of labels for that batch using the discriminator. But you\\nshould notice the subtle difference in the loss computation. Rather than\\ntrying to assign fake labels to fake images, the objective is to trick the\\ndiscriminator into applying real labels to fake images. The generator\\nimplicitly learns to generate data similar to the training data by learning to\\nfool the discriminator.\\nNow you need to implement an initialization function for your training\\nloop. Given your training step, your state needs to contain :model_state,\\n:optimizer_state, and :loss for both \"generator\" and \"discriminator\", as well as a\\ncounter to track the current iteration:\\ndefn init_state(\\n       discriminator_init_fn,\\n       generator_init_fn,\\n       discriminator_optimizer_init,\\n       generator_optimizer_init,\\n       batch,\\n       init_state\\n     )  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 596}),\n",
       " Document(page_content=' d_params = discriminator_init_fn.(batch, \\ninit_state)\\n  g_params = generator_init_fn.\\n(Nx.random_normal({64, 128}), init_state)\\n  d_optimizer_state = \\ndiscriminator_optimizer_init.(d_params)\\n  g_optimizer_state = generator_optimizer_init.\\n(g_params)\\n  model_state = %{\\n     \"  discriminator\"  => d_params,\\n     \"  generator\"  => g_params\\n  }\\n  optimizer_state = %{\\n     \"  discriminator\"  => d_optimizer_state,\\n     \"  generator\"  => g_optimizer_state\\n  }\\n  loss = %{\\n     \"  discriminator\"  => Nx.tensor(0.0),\\n     \"  generator\"  => Nx.tensor(0.0)\\n  }\\n  %{\\n     model_state:  model_state,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 597}),\n",
       " Document(page_content='    optimizer_state:  optimizer_state,\\n     loss:  loss,\\n     i:  Nx.tensor(0)\\n  }\\nend \\nThere’s nothing new here. You initialize both of your models and their\\nrespective optimizers and then construct maps for each item you want to\\ntrack.\\nAs with the VAE, you’ll probably want to also track how well your\\ngenerator learns to generate images between epochs. You can achieve this\\nby converting your display_sample/2 function from the previous section to\\nthis example here:\\ndef  display_sample(\\n  %Axon.Loop.State{ step_state:  state} = out_state,\\n  generator_fn\\n)  do \\n  latent = Nx.random_normal({3, 128})\\n  %{ prediction:  out} = generator_fn.(state[ \\n:model_state ][ \"  decoder\" ], latent)\\n  out_image = Nx.multiply(out, 255) |> Nx.as_type( \\n:u8 )\\n  upsample = Axon.Layers.resize(', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 598}),\n",
       " Document(page_content='   out_image,\\n     size:  {512, 512},\\n     channels:   :first \\n  )\\n  for i <- 0..2  do \\n    Kino.Image.new(Nx.reshape(upsample[i], {512, \\n512, 1})) |> Kino.render()\\n   end \\n  { :continue , out_state}\\nend \\nThe final step is to implement your custom training loop. Overall, the\\nprocess is similar to your VAE example. Start by building both of your\\nmodels’ initialization and prediction functions with the following code:\\n{discriminator_init_fn, discriminator_fn} =\\n  Axon.build(discriminator,  mode:   :train )\\n{generator_init_fn, generator_fn} =\\n  Axon.build(generator,  mode:   :train )\\nNext, you need to build optimizers for each model:\\n{d_optimizer_init, d_optimizer} = \\nAxon.Optimizers.adam(1.0e-4)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 599}),\n",
       " Document(page_content='{g_optimizer_init, g_optimizer} = \\nAxon.Optimizers.adam(1.0e-3)\\nNotice that you’re using slightly different optimizers for each model. To\\nprevent the discriminator from dominating the generator before it can get its\\nbearings during training, you need to lower the learning rate of the\\ndiscriminator. GAN training is really fickle—feel free to mess with\\ndifferent hyperparameters and optimizers to see if you can find a better\\nconfiguration.\\nNow you can implement parameterized forms of your initialization and step\\nfunctions:\\ninit_fn = &GAN.init_state(\\n  discriminator_init_fn,\\n  generator_init_fn,\\n  d_optimizer_init,\\n  g_optimizer_init,\\n  &1,\\n  &2\\n)\\nstep_fn = &GAN.train_step(\\n  discriminator_fn,\\n  generator_fn,\\n  d_optimizer,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 600}),\n",
       " Document(page_content=' g_optimizer,\\n  &1,\\n  &2\\n)\\nFinally, you’re ready to run your training loop:\\nstep_fn\\n|> Axon.Loop. loop (init_fn)\\n|> Axon.Loop.handle(\\n   :epoch_completed ,\\n  &GAN.display_sample(&1, generator_fn)\\n)\\n|> Axon.Loop.log( fn \\n  %Axon.Loop.State{ epoch:  epoch,  iteration:  iter,  \\nstep_state:  state} ->\\n    d_loss = state[ :loss ][ \"  discriminator\" ]\\n    g_loss = state[ :loss ][ \"  generator\" ]\\n     \"  \\\\rEpoch:   #{ epoch }  , batch:   #{ iter }  ,\" \\n      <>  \"   d_loss:   #{ Nx.to_number(d_loss) }  ,\" \\n      <>  \"   g_loss:   #{ Nx.to_number(g_loss) }  \" \\nend ,  event:   :iteration_completed ,  device:   :stdio )\\n|> Axon.Loop.run(train_data, %{},  compiler:  EXLA,  \\nepochs:  10)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 601}),\n",
       " Document(page_content='During training, you’ll see this:\\nEpoch: 2, batch: 827, d_loss: 0.722309589, g_loss: \\n0.9743837\\nYou’ll also see some images that look like this:\\nIt’s difficult to demonstrate the power of GANs on low-resolution\\nhandwritten digits, but you might find these generations are a bit better than\\nyour VAE. You might also find that your model collapses during training\\nand starts to converge to bad generations. Training GANs is fun but can be\\ndifficult in practice. You should explore more complex GAN\\nimplementations and training techniques to learn how to take your low-\\nresolution digit generations and turn them into high-resolution, photo-\\nrealistic imagery.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 602}),\n",
       " Document(page_content='Learning Without Supervision in Practice\\nSo far in this chapter, you’ve learned a bit about deep unsupervised learning\\nmostly from the perspective of deep generative modeling. You progressed\\nfrom compressing data with autoencoders to generating data with VAEs and\\nfinally to generating data with GANs. You were able to train some models\\nto generate decent-looking, low-resolution, handwritten digits—which you\\nmight think is exciting. But at the same time, you might still be wondering\\nabout the secrets of power models like DALL-E and why learning about\\nGANs, VAEs, and Autoencoders is even useful. In this section, you’ll dive a\\nbit deeper into modern unsupervised learning in practice.\\nUnsupervised Learning in Theory\\nYann Lecun, one of the godfathers of deep learning, once called\\nunsupervised learning—specifically self-supervised learning—the “cake”\\nwhen compared to supervised learning and reinforcement learning. In other\\nwords, Lecun believes unsupervised learning is the most important research\\narea in machine learning. More recently, Lecun wrote about self-supervised\\nlearning being the Dark Matter of Intelligence.\\nSelf-supervised learning can be considered a subset of unsupervised\\nlearning in the sense that no labels are provided. The most common self-\\nsupervised models you see in practice are transformers. Transformer models\\nlearn to model language from raw text without labels—the tokens in the\\n[40]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 603}),\n",
       " Document(page_content='text act as labels during the pre-training process. This eliminates the need to\\ncarefully hand-label datasets and exponentially increases the size of\\navailable training data.\\nIn a philosophical sense, generative modeling can be considered the purest\\nform of unsupervised learning and might even possibly be at the core of\\nintelligence. Learning to generate data that matches some distribution is far\\nmore powerful than learning to model some specific relationship between\\ninput data and labels. This is part of the reason pre-trained, self-supervised\\nmodels like transformers are so powerful in practice. They learn to capture\\nrelationships in semantics and syntax in language—not whether the\\npresence of certain words maps to certain labels.\\nWhat Is the State of the Art?\\nAfter learning a bit about generative modeling, you might be left wondering\\nwhat other types of generative models are out there and which ones you\\nshould use.\\nPerhaps the most famous generative model today is stable diffusion. Stable\\ndiffusion is a text-to-image model which makes use of latent diffusion.\\nDiffusion is the process of progressively denoising images starting from\\nrandom noise over a fixed number of timesteps.\\nThe forward diffusion process progressively adds noise to an input image,\\nuntil the image essentially becomes random noise. The reverse diffusion', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 604}),\n",
       " Document(page_content='process learns to map the random noise back to the input image.\\nDiffusion models have picked up in popularity in recent years thanks to the\\nsuccess of generative models like stable diffusion. Before diffusion models,\\nGANs were the preferred architecture for generative modeling. In many\\nways, GANs are still the most popular generative model used in practice.\\nApplications of Autoencoders, VAEs, and GANs\\n\\xa0\\nSo what practical applications do unsupervised models like autoencoders,\\nVAEs, and GANs actually have? Actually, quite a few, but in this section,\\nyou’ll learn about only a handful. I highly recommend you research\\nadditional applications on your own. You’ll find that these and other\\nunsupervised models are incredibly useful in practice.\\nImage Denoising\\nOne practical application of autoencoders is in Image Denoising [Gon16].\\nImage denoising is the process of taking a noisy image and mapping it to a\\nclean image. By applying a variety of noise to input images, you can train\\nan autoencoder to defeat intentional and unintentional defects in imagery.\\nSuper-resolution', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 605}),\n",
       " Document(page_content='Super-resolution is a class of image techniques to enhance the resolution of\\nan input image. GANs have been applied with great success to Super-\\nresolution. [LTHC17] Super-resolution itself has a number of applications\\nin medical imaging, microscopic imaging, and more.\\nAnomaly Detection\\nOne unique application of generative models such as VAEs and GANs is in\\nanomaly detection. Anomaly detection is the process of detecting\\nanomalous data points. One such example of GANs applied to anomaly\\ndetection uses GANs to identify anomalies in images of the retina\\n[SSWS17]. Because generative models like GANs are capable of modeling\\nwhat real data is supposed to look like, they’re also capable of modeling\\nwhat anomalous data looks like.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 606}),\n",
       " Document(page_content='[39]\\n[40]\\nWrapping Up\\nIn this chapter, you implemented three types of deep unsupervised learning\\nand deep generative modeling algorithms in Axon: an autoencoder, a\\nvariational autoencoder, and a generative adversarial network. You\\nimplemented custom event handlers in Axon and wrote your first custom\\ntraining loop. You also used some new Axon layers such as\\nAxon.conv_transpose/3 and Axon.resize/3.\\nFinally, you learned a bit about unsupervised learning and generative\\nmodeling in theory and practice. You also learned about the state of the art\\nin generative modeling and about some applications of generative models in\\nthe real world.\\nAt this point, you’ve learned essentially everything you need to know about\\ndeep learning and Axon. You’re more than ready to learn how to put\\nmachine learning into practice at any scale with Elixir. In the next chapter,\\nyou’ll dive into a practical machine learning problem and put a solution into\\nproduction with Nx, Axon, and Phoenix.\\nFOOTNOTES\\nhttps://openai.com/dall-e-2/\\nhttps://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 607}),\n",
       " Document(page_content='Copyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 608}),\n",
       " Document(page_content='', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 609}),\n",
       " Document(page_content='Part 3\\nMachine Learning in Practice', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 610}),\n",
       " Document(page_content='Chapter 13\\nPut Machine Learning into\\nPractice\\n\\xa0\\nAt this point in the book, you have experience applying machine learning to\\nmany different input modalities. You’ve worked with structured data, text,\\nimages, time-series data, and more. You’ve created and applied both\\nsupervised and unsupervised machine learning algorithms. You know how\\nto use Nx, Axon, and Livebook to train and validate models. You’re well on\\nyour way to becoming a proficient Elixir machine learning engineer.\\nThere’s one more question you need to answer: how do I go from Livebook\\nto running machine learning in production?\\nIn this chapter, you’ll learn how to operationalize your machine learning\\nmodels with Nx.Serving and Elixir’s web framework Phoenix. You’ll\\ncreate a machine-learning-powered web application and see firsthand some\\nof the considerations you must have in a production deployment. This\\nchapter is designed to serve as an introduction to machine learning\\noperations (MLOps). Machine learning operations is a blanket term for the\\nset of practices used to deploy and monitor machine learning models in\\nproduction. This chapter covers some of the basics, but if you’re looking for\\n[41]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 611}),\n",
       " Document(page_content='a more comprehensive book, I suggest looking at Designing Machine\\nLearning Systems [Huy22] by Chip Huyen.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 612}),\n",
       " Document(page_content='Deciding to Use Machine Learning\\nImagine you’ve been tasked with improving the book search experience for\\nlocal readers. Rather than requiring readers to search based on specific\\nauthors or genres they like, or memorize the Dewey decimal system, the\\nlibrary wants readers to have the power to search with natural language.\\nBefore attacking this problem with machine learning, you need to decide if\\nit’s actually worth using machine learning over a more traditional approach.\\nMachine learning is a powerful tool, but if you apply it to every problem\\nwithout thought, you risk building applications that use hammers for screws\\nand screwdrivers for nails.\\nMachine learning comes with a number of costs that might not be worth it\\nwhen compared to the benefits of a naive solution. When making the\\ndecision of whether or not to use machine learning, there are no one-size-\\nfits-all metrics or decision criteria to use. You need to make the decision in\\nthe context of your specific application and business goals.\\nTo add to the challenge of deciding between machine learning or naive\\nsolutions, requirements are often vague. If you break down the problem\\nstatement above, you end up with a single requirement: “improve the book\\nsearch experience for local readers.” That’s not a good requirement. What\\ncounts as an improvement?', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 613}),\n",
       " Document(page_content='Without concrete metrics, it’s difficult to make a sound decision. At this\\ntime, you’d probably want to go back to the drawing board and hash out\\nwhat success looks like, with concrete metrics and milestones. Part of\\nhashing out requirements for machine learning applications is knowing\\nwhat questions to ask. You should consider some common things for every\\napplication you create, such as the following:\\nHow do machine learning and naive solutions compare in terms of the\\napplication’s KPIs?\\nWhat are the compute and storage constraints of the application?\\nWhat are the development and maintenance requirements/constraints of\\nthe application?\\nIs it ethical to solve this problem with machine learning?\\nTo understand the decision-making process, you’ll consider each of these\\nquestions in the context of the book search service. But, before you can do\\nthat, you need to understand the possible approaches you can use to solve\\nthis problem with and without machine learning: full-text search and\\nsemantic search.\\nFull-Text Search vs. Semantic Search\\nIf you’re familiar with search applications, your first inclination might be to\\nreach for full-text search capabilities, which are built into Postgres and\\nElasticsearch.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 614}),\n",
       " Document(page_content='Full-text search is a search technique for retrieving documents that contain\\nsome text from a given keyword query. While full-text search works well\\nfor some applications, it pushes some additional burden on users to know\\nthe correct keywords to search for to find what they’re looking for. Full-text\\nsearch doesn’t capture the nuance or meaning of a document, and thus it can\\nfall short when trying to make good recommendations to users attempting\\nto describe what they want.\\nAn alternative approach is to use a semantic search. Semantic search is a\\nsearch technique for retrieving documents based on semantic meaning.\\nRather than literal keyword matching, semantic search retrieves results that\\nare most similar to the meaning conveyed in a user’s query. Semantic\\nsearch is a powerful alternative to full-text search because it allows users to\\nconvey what they’re looking for with natural language, rather than\\ndepending on keyword hacking. Semantic search depends on having access\\nto a model that can accurately represent the meaning and semantics of\\ndocuments and queries. In practice, semantic search often depends on a\\ncombination of machine learning and vector databases.\\nVector databases are optimized for storing, retrieving, and searching dense\\nvector representations. But why are vector databases applicable to semantic\\nsearch? Where are the vectors coming from?\\nRecall from  Tokenizing and Vectorizing in Elixir , that word embeddings\\ngive mathematical meaning to words in a sentence. To perform machine', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 615}),\n",
       " Document(page_content='learning on text, you need a way to turn text into a vector representation.\\nWord embeddings do this transformation at the word level. More powerful\\nmodels based on transformers are capable of doing this transformation at\\nthe sentence level. That means you can create embeddings that capture the\\nmeaning of entire sentences, rather than the meaning of individual words in\\na sentence. Semantic search makes it possible to express what you want\\nmore naturally than keyword search. For example, rather than searching for\\ncarefully selected keywords like “horror,” “murder mystery,” and\\n“historical,” you could express your query as a sentence: “A horror story\\nbased on a murder in a colonial town.”\\nThis is where vector databases come into play. Your vector database stores\\nvectors that represent the meaning of books captured by some machine\\nlearning model. You can query your database with another vector, and a\\nvector database will return the books closest to your query vector. In a\\nvector search, the word “closest” can take on a variety of meanings as you\\ncan measure the distance between two vectors in a number of different\\nways. To simplify this concept, consider that your vector database contains\\nvectors for each book in your dataset in a two-dimensional space. If you\\nplot this two-dimensional space, it might look something like this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 616}),\n",
       " Document(page_content='Notice that in the two-dimensional space, horror books are closer to\\nmystery books, biographies are closer to PragProg technical books, and so\\non. Books that are generally more similar will be grouped together visually\\nin space. Now, say you make a query for the “best book on genetic\\nalgorithms in elixir” and run that through the same model you used to\\nembed the meaning of each of your books. If you were to plot the query in\\nthe same embedding space, it might look something like this:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 617}),\n",
       " Document(page_content='The query falls very close to PragProg books because the PragProg\\ncollection contains Genetic Algorithms in Elixir [Mor21]. If you were to\\nmeasure the straight-line distance between every book in the collection in\\nyour query, you could then return the books that most closely match the\\nquery in semantic meaning by returning the ones that are closest in the\\nembedding space. This is essentially what vector databases do, but with\\nfancy tricks to allow them to scale to quickly search through billions of\\nembeddings at once.\\nHow Do Machine Learning and Naive Solutions Compare?', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 618}),\n",
       " Document(page_content='Now that you understand your two options for designing the library search\\napplication, you need to know what questions to ask in order to fairly\\ncompare solutions. First, you need to look at key performance indicators\\n(KPIs). They are metrics that evaluate the success of an application.\\nDetermining KPIs in the context of applications is often a difficult task. For\\nexample, in your book search service, how do you measure user\\nsatisfaction?\\nOften, you need to choose auxiliary metrics that align with your overall\\ngoal. Note that it’s important to think carefully about these auxiliary metrics\\n—you don’t want to end up optimizing for the wrong thing. For example,\\nimagine you designed a fraud detection application. You might think it’s\\nbest to maximize for number of fraudulent transactions declined, but you’d\\nquickly be surprised at the angry complaints from legitimate customers\\ngetting their transactions declined. In reality, to provide a good user\\nexperience, you need to balance the ratio of legitimate transactions accepted\\nto fraudulent transactions rejected.\\nChoosing a metric for recommender systems like your book search service\\nis notoriously difficult. Some examples of metrics you might consider:\\nNumber of queries to find a book\\nAggregate time browsing to find a book\\nRate of top N results being taken', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 619}),\n",
       " Document(page_content='None of these are necessarily better than the others. All of them have\\npotentially problematic confounding variables. For now, imagine the library\\nhas chosen the number of queries needed to find a book as it’s top metric.\\nWhat Are the Compute and Storage Constraints of the\\nApplication?\\nCertain approaches to machine learning come with significant compute and\\nstorage requirements. Semantic search depends on transformers—the deep\\nlearning models you used in Chapter 11,  Model Everything with\\nTransformers .\\nTransformer models are often computationally expensive. Depending on the\\nstrength of the model, you may need to rely on GPU compute to provide a\\ngood enough experience in production. Otherwise, your application will end\\nup being relatively high-latency due to the inference overhead of a large\\nmodel like a transformer.\\nYou should always consider the compute and storage constraints of your\\napplication, especially with respect to latency. If you’re not able to scale up\\nthe resources necessary to provide low-latency inference in production, then\\nyour users will have a bad experience even with the most powerful machine\\nlearning model in the world.\\nAdditionally, for certain applications of machine learning like semantic\\nsearch, you need to consider the storage constraints of your application.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 620}),\n",
       " Document(page_content='Remember, you need to store a dense vector for each book in the library,\\nwhich will come with a storage cost. But when compared to a full-text\\nsearch, this storage cost might actually be better. Vector search tools often\\nsupport compressed indices which allow them to scale efficiently in size.\\nAdditionally, the vector representing a book is itself a compressed\\nrepresentation of the text in the book. With a semantic search, you get some\\nstorage benefit when compared to storing full documents in a full-text\\nsearch.\\nFor this example, you can assume that no significant compute and storage\\nconstraints exist that would prevent you from using either semantic or full-\\ntext search.\\nWhat Are the Development and Maintenance Constraints of\\nthe Application?\\nWhen compared to alternative solutions, machine-learning-based solutions\\noften introduce significant developer and maintenance costs. Machine\\nlearning in production is still a relatively new concept, and the tools and\\nbest practices are still evolving. Unlike traditional software, machine\\nlearning models atrophy in production—a phenomenon known as model\\ndrift.\\nIf you leave a model in production without retraining on new data, it will\\nslowly deteriorate overtime and deliver a worse experience for your users.\\nModel drift often occurs because the distribution of data the model is', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 621}),\n",
       " Document(page_content='exposed to in production slowly drifts from the distribution of data the\\nmodel was trained on.\\nAdditionally, while with traditional software you can write tests to help\\nensure the correctness of your software, the same doesn’t go for machine\\nlearning. Models in production require constant monitoring to verify\\npredictions remain good. Even with constant monitoring, it can be difficult\\nto tell when models are starting to deteriorate.\\nFinally, the unique combination of data, code, and models involved in\\nbuilding machine learning systems makes developing them extremely\\ndifficult. It’s common for machine learning projects to devolve into\\nnightmares of unmaintainable code, unversioned data, and barely working\\nmodels. When deciding to use machine learning in production, it’s\\nimportant to remain disciplined, start with a simple MLOps solution, and\\nscale in complexity as needed.\\nIn the context of the book-search application, some of the development and\\nmaintenance costs for the machine learning approach are dissipated because\\nyou can take advantage of pre-trained models. That being said, there’s still\\nsome development overhead in deploying any machine learning in\\nproduction—despite how easy Elixir may make it.\\nChoosing a Solution', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 622}),\n",
       " Document(page_content='Given the requirement to build a book search tool capable of helping users\\nfind books in fewer search queries, you need to decide between a machine-\\nlearning-based solution (semantic search) or a naive solution (full-text\\nsearch). In terms of compute and storage costs, machine learning will likely\\nhave additional computational overhead. Both solutions will have\\ncomparable storage overhead. In terms of development and maintenance\\ncosts, the machine learning solution will have more overhead. However,\\nthat’s mitigated with the use of a pre-trained model. The decision mainly\\ncomes down to your KPI—the number of queries to find a book.\\nIn the context of your KPI, semantic search typically provides better results,\\nas it doesn’t depend on users knowing exactly what keywords or partial\\nkeywords to search for. Semantic search allows users to express what they\\nwant naturally and finds the closest representation. Semantic search also\\nopens up the possibility of iteration in the future by training better models\\nbased on user preferences in the library.\\nSo, it seems you’ll need machine learning after all. By now, you should be\\npretty familiar with all of the tools you’ll need for this application. You\\nhave Nx for general data manipulation, Axon and Bumblebee for neural\\nnetwork tasks, and EXLA for hardware acceleration. But what about vector\\nsearch?\\nFortunately, there’s a Postgres extension for performing vector search called\\npgvector,  which allows you to build vector search tools directly on top of[42]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 623}),\n",
       " Document(page_content='your existing database. This makes it easy to keep your vector store in sync\\nwith your database, as the data lies right next to one another. Best of all,\\nthere’s an Ecto implementation of pgvector. That means you can embed\\nvector search directly into your existing Ecto queries.\\nWith a plan of attack, it’s time to create your application.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 624}),\n",
       " Document(page_content='Setting Up the Application\\nTo start your book search application, create a new Phoenix project with mix\\nphx.new:\\nmix phx.new book_search\\nNow, run the following command to change to the root directory of your\\nproject and list its contents:\\ncd book_search && ls\\nThe output of this command will look like this:\\nassets  _build  config  deps  lib  mix.exs  \\nmix.lock  priv  README.md  test\\nNext, you’ll need to install pgvector. Start by adding pgvector as a\\ndependency in your mix.exs:\\n{ :pgvector ,  \"  ~> 0.2.0\" }\\nThen run mix deps.get. Next, create a new file lib/postgrex_types.ex and add\\nthe following code:\\nPostgrex.Types.define(\\n        BookSearch.PostgrexTypes,', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 625}),\n",
       " Document(page_content='       [Pgvector.Extensions.Vector] ++ \\nEcto.Adapters.Postgres.extensions(),\\n        []\\n)\\nNext, add the following to conﬁg/conﬁg.exs:\\nconfig  :book_search , BookSearch.Repo,  types:  \\nBookSearch.PostgrexTypes\\nThen, create a migration with mix ecto.gen.migration create_vector_extension\\nand add the following:\\ndefmodule  \\nBookSearch.Repo.Migrations.CreateVectorExtension  \\ndo \\n   use  Ecto.Migration\\n   def  up  do \\n    execute  \"  CREATE EXTENSION IF NOT EXISTS \\nvector\" \\n   end \\n   def  down  do \\n    execute  \"  DROP EXTENSION vector\" ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 626}),\n",
       " Document(page_content='  end \\nend \\nFinally, you can run the migration with mix ecto.migrate. If it ran without a\\nhitch, pgvector should be installed correctly. Note that you may need to also\\nfollow the pgvector installation notes if you run into any issues during\\nthis installation process.\\nWith pgvector installed, it’s time to start creating your application\\nresources. To simplify things, your book search application will revolve\\naround a single resource: books. You can generate everything you need for\\nthe book resource by running the following code:\\nmix phx.gen.html Library Book books  \\\\ \\n  author:string title:string description:text \\nembedding:binary\\nmix phx.gen.html is a generator that creates a new HTML resource with\\nviews, controllers, and more for your book resource. The generator also\\ncreates a Library context, a Book schema, and a books database table. The\\nbook schema consists of an author, a title, a description of the book, and a\\nbinary embedding. The embedding is a byte array that represents the dense\\nvector representation of a book. You’ll use the embedding to perform a\\nvector search in your application. After running this command, you’ll see\\nthe following output:\\n[43]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 627}),\n",
       " Document(page_content='* creating lib/book_search_web/controllers/book_con\\n* creating \\nlib/book_search_web/controllers/book_html/edit.html\\n* creating \\nlib/book_search_web/controllers/book_html/index.htm\\n* creating \\nlib/book_search_web/controllers/book_html/new.html.\\n* creating \\nlib/book_search_web/controllers/book_html/show.html\\n* creating \\nlib/book_search_web/controllers/book_html/book_form\\n* creating lib/book_search_web/controllers/book_htm\\n* creating \\ntest/book_search_web/controllers/book_controller_te\\n* creating lib/book_search/library/book.ex\\n* creating \\npriv/repo/migrations/20230517143726_create_books.ex\\n* creating lib/book_search/library.ex\\n* injecting lib/book_search/library.ex\\n* creating test/book_search/library_test.exs\\n* injecting test/book_search/library_test.exs\\n* creating test/support/fixtures/library_fixtures.e\\n* injecting test/support/fixtures/library_fixtures.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 628}),\n",
       " Document(page_content='Add the resource to your browser scope in \\nlib/book_search_web/router.ex:\\n    resources \"/books\", BookController\\nRemember to update your repository by running migra\\n    $ mix ecto.migrate\\nNow, you want to edit the embedding type in the migration such that it uses\\nthe pgvector vector type. Edit the generated migration so it looks like this:\\ndefmodule  BookSearch.Repo.Migrations.CreateBooks  \\ndo \\n   use  Ecto.Migration\\n   def  change  do \\n    create table( :books )  do \\n      add  :author ,  :string \\n      add  :title ,  :string \\n      add  :description ,  :text \\n      add  :embedding ,  :vector ,  size:  384', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 629}),\n",
       " Document(page_content='     timestamps()\\n     end \\n   end \\nend \\nNotice that we specify :vector with size: 384 in place of :binary. This tells\\nPostgres that we’re creating a vector column which we expect to have a\\ndimensionality of 384. You’ll also want to edit the book schema to reflect\\nthe usage of the vector type:\\nschema  \"  books\"   do \\n  field  :author ,  :string \\n  field  :description ,  :string \\n  field  :embedding , Pgvector.Ecto.Vector\\n  field  :title ,  :string \\n  timestamps()\\nend \\nNext, you need to set up your database. You can combine database creation,\\nmigrations, and seeding into a single step using mix ecto.setup, like this:\\nmix ecto.setup', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 630}),\n",
       " Document(page_content='This code creates your database, runs your migrations (there should only be\\none), and runs the seed file in priv/repo/seeds.exs to seed your database.\\nAfter setting up your database, add a new resource route to\\nbook_search_web/router.ex, like this:\\nget  \"  /\" , PageController,  :index \\nresources  \"  /books\" , BookController\\nThis code creates CRUD routes for your book resource. You can inspect the\\nroutes with mix phx.routes:\\nmix phx.routes\\nAnd you’ll see the following output:\\nGET     /                              \\nBookSearchWeb.PageController :home\\nGET     /books                         \\nBookSearchWeb.BookController :index\\nGET     /books/:id/edit                \\nBookSearchWeb.BookController :edit\\nGET     /books/new                     \\nBookSearchWeb.BookController :new', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 631}),\n",
       " Document(page_content='GET     /books/:id                     \\nBookSearchWeb.BookController :show\\nPOST    /books                         \\nBookSearchWeb.BookController :create\\nPATCH   /books/:id                     \\nBookSearchWeb.BookController :update\\nPUT     /books/:id                     \\nBookSearchWeb.BookController :update\\nDELETE  /books/:id                     \\nBookSearchWeb.BookController :delete\\nGET     /dev/dashboard                 \\nPhoenix.LiveDashboard.PageLive :home\\nGET     /dev/dashboard/:page           \\nPhoenix.LiveDashboard.PageLive :page\\nGET     /dev/dashboard/:node/:page     \\nPhoenix.LiveDashboard.PageLive :page\\n*       /dev/mailbox                   \\nPlug.Swoosh.MailboxPreview []\\nWS      /live/websocket                \\nPhoenix.LiveView.Socket\\nGET     /live/longpoll                 \\nPhoenix.LiveView.Socket\\nPOST    /live/longpoll                 \\nPhoenix.LiveView.Socket', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 632}),\n",
       " Document(page_content='You can now start your application by running mix phx.server:\\nmix phx.server\\nYou can navigate to http://localhost:4000 in the browser to verify your server\\nstarted successfully. If all went well, you’ll see the following welcome\\npage.\\nSuccess! With your Phoenix application set up, you’re ready to add a model\\nto the mix.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 633}),\n",
       " Document(page_content='Integrating Nx with Phoenix\\nNow that you have a running Phoenix application, you can start working\\ntoward integrating your semantic search machine learning model.\\nFirst, add the following dependencies to your mix.exs file:\\n{ :bumblebee ,  \"  ~> 0.5\" },\\n{ :nx ,  \"  ~> 0.7\" },\\n{ :exla ,  \"  >= 0.0.0\" },\\n{ :explorer ,  \"  ~> 0.5\" }\\nFor your application, you’ll need Bumblebee for loading pre-trained\\nmodels, Nx for manipulating tensors and model serving, and EXLA for\\nproviding accelerated inference. You’ll also use Explorer for working with\\nsome CSV data to seed your database.\\nNext, run mix deps.get:\\nmix deps.get\\nThen, create a new file book_search/model.ex. This file will contain the logic\\nfor loading, serving, and performing inference with your semantic search\\nmodel. While the Nx ecosystem is still developing best practices around\\nmanaging and deploying models, here are some emerging best practices:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 634}),\n",
       " Document(page_content='1. Serialize model state as data, for example, using Nx.serialize/2 or another\\nserialization format, and serialize model code as code. This means if you\\nimplement your own model using Axon’s model creation API, you\\nshould keep the model creation code around, and not rely on serializing\\nthe entire model data structure to use the model later on.\\n2. Keep your creation, loading, serving, and inference logic for a single\\nmodel in one place. The relationships between model creation, loading,\\nserving, and inference are tightly coupled. If any of these processes\\nchange, the others are likely to change as well.\\nLike all best practices, these rules aren’t one-size-fits-all. The best best\\npractice is to do whatever works best for your application.\\nIn book_search/model.ex, create a module named BookSearch.Model:\\ndefmodule  BookSearch.Model  do \\n  @moduledoc  \"\"\" \\n  Manages the BookSearch model for similarity \\nsearch. \\n  \"\"\" \\n  @hf_model_repo  \"  intfloat/e5-small-v2\" \\nend \\nNext, create a function load/0, like so:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 635}),\n",
       " Document(page_content='defp  load()  do \\n  { :ok , model_info} = Bumblebee.load_model({ :hf , \\n@hf_model_repo})\\n  { :ok , tokenizer} = Bumblebee.load_tokenizer({ :hf \\n, @hf_model_repo})\\n  {model_info, tokenizer}\\nend \\nThis function uses Elixir’s Bumblebee library to load a model from\\nHuggingFace. You should be familiar with Bumblebee from Chapter 11,  \\nModel Everything with Transformers .\\nBumblebee is a library for using pre-trained models and tasks for common\\napplications like image classification, text generation, and more. It’s also\\ncapable of importing pre-trained models designed for similarity search. For\\nexample, the model you’re using here is from HuggingFace and is\\ndesigned specifically for sentence similarity. That means it was trained\\nspecifically with tasks like searching and clustering in mind.\\nThe load/0 function will return a tuple of model and tokenizer. The model\\nconsists of a model tuple with {model, params}, and the tokenizer is just a\\nBumblebee tokenizer struct. You’ll use both of these for creating an\\ninference pipeline later on.\\n[44]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 636}),\n",
       " Document(page_content='Next, create a function named serving/1:\\ndef  serving(opts \\\\\\\\ [])  do \\n  opts = Keyword.validate!(opts, [\\n     :defn_options ,  sequence_length:  64,  batch_size:\\n  ])\\n  {model_info, tokenizer} = load()\\n  \\nBumblebee.Text.TextEmbedding.text_embedding(model_i\\ntokenizer, [\\n     defn_options:  opts[ :defn_options ],\\n     compile:  [\\n         sequence_length:  opts[ :sequence_length ],\\n         batch_size:  opts[ :batch_size ]\\n    ]\\n  ])\\nend \\nThis function uses the pre-built serving pipeline provided by Bumblebee to\\nperform text embedding.\\nYou’ll hear the word “serving” in the context of machine learning quite a\\nbit. In Nx, a serving is a simple abstraction for building inference pipelines.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 637}),\n",
       " Document(page_content='It consists of preprocessing, inference, and postprocessing. If you were\\nwriting this serving from scratch, it would look something like this:\\ndef  serving(opts \\\\\\\\ [])  do \\n         opts = Keyword.validate!(opts, [\\n     :defn_options ,  sequence_length:  64,  \\nbatch_size:  16\\n  ])\\n  {%{ model:  model,  params:  params}, tokenizer} = \\nload()\\n  {_, predict_fn} = Axon.build(model)\\n        Nx.Serving.jit( fn  input ->\\n                pad_size = opts[ :sequence_length ] \\n- Nx.axis_size(input, 1)\\n                predict_fn.(params, \\nNx.Batch.pad(input, pad_size))\\n         end )\\n        |> Nx.Serving.client_preprocessing( fn  text \\n->\\n                tokenized = \\nBumblebee.apply_tokenizer(tokenizer, text)\\n                {Nx.Batch.stack(tokenized),  :meta }\\n         end )', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 638}),\n",
       " Document(page_content='       |> Nx.Serving.client_postprocessing(& \\nelem(&1, 0))\\nend \\nThis function is known as serving/1 because it returns your Nx.Serving\\npipeline. The Nx.Serving module contains functions for building servings,\\nlike the ones you can see here, such as Nx.Serving.jit/1 and\\nNx.Serving.client_preprocessing/1. But why do you need a special-purpose\\nserving pipeline? Why can’t you just call a model’s predict_fn?\\nThere are a few good reasons, but the most important reason is that servings\\nallow you to maximize research usage by taking advantage of dynamic\\nbatching. Deep learning models are expensive. However, they’re also\\ndesigned to take advantage of batch inference. Neural network inference\\nlatency doesn’t scale linearly with batch size for accelerators. In fact, some\\naccelerators, such as TPUs, require inputs to have a batch size of at least\\n128 and will even pad inputs to that batch size before executing inference.\\nEvery inference you don’t execute with the maximum possible batch size is\\na waste of resources.\\nDynamic batching is a way to take advantage of the resources available to\\nyou. When you start an Nx.Serving process, behind the scenes it spins up a\\nqueue. As you make calls to the serving process to perform inference,\\nNx.Serving will automatically batch overlapping requests for you behind the\\nscenes and then dispatch them back out to the calling process. That means if', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 639}),\n",
       " Document(page_content='you make an inference request at the same time as another person, both of\\nyour inference requests will be executed within the same batch, and then\\nreturned to you as if executed standalone. Dynamic batching is critical to\\neffectively scaling machine learning inference in production.\\nAs you can see in serving/1, every Nx.Serving starts with a call to\\nNx.Serving.jit/1 or Nx.Serving.new/1. Nx.Serving.jit/1 accepts an arity-1 creation\\nfunction that returns a just-in-time compiled inference inference function.\\nIn this example, your Nx.Serving.jit/1 function creates a compiled encoding\\nfunction. Your actual inference function accepts serving inputs and returns\\nthe encoded outputs. Before calling encoding_fun/2, you need to call\\nNx.Batch.pad/2. Nx.Batch.pad/2 is a helper function that pads your inputs to the\\nrequired batch size specified by your compiled encoding function. If your\\ndynamic batch queue doesn’t feel up to the required batch size,\\nNx.Batch.pad/2 will pad the inputs to the required size.\\nIn addition to an inference function specified in Nx.Serving.jit/1, you can\\nspecify client preprocessing and postprocessing functions using Nx.Serving\\nfunctions. In this simple example, you can just call out to\\nBumblebee.apply_tokenizer/2.\\nPreprocessing is any work you want done before an input reaches the\\ninference step. Typically, it converts raw data into a cleaned-up tensor.\\nPreprocessing, like inference, is also performed in batches under the hood.\\nWhile it’s common to perform preprocessing on the server, for some', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 640}),\n",
       " Document(page_content='applications it makes sense to perform it on the client to avoid saturating the\\nserver with work. For example, you can use the Javascript canvas API to\\nresize images to save resources on the server.\\nEvery client preprocessing function accepts raw inputs and returns a tuple\\nof {processed, meta}. The additional metadata is forwarded to postprocessing,\\nas it’s common in some tasks to need access to the raw inputs in order to\\nperform postprocessing.\\nBumblebee implements a number of common machine learning pipelines\\nout of the box for you, so it saves you the complexity of worrying about\\nwhat’s going on under the hood. For certain applications, you may want or\\nneed to implement your own custom-serving pipeline, so it’s important to\\nunderstand how it works.\\nWith your serving implemented, you can add the following code to your\\napplication’s supervision tree:\\n{Nx.Serving,\\n serving:  BookSearch.Model.serving( defn_options:  [ \\ncompiler:  EXLA]),\\n batch_size:  16,\\n batch_timeout:  100,\\n name:  BookSearchModel}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 641}),\n",
       " Document(page_content='This code will start a new Nx.Serving process using the serving you defined\\nin BookSearch.Model.serving/1. The serving process will be named\\nBookSearchModel, so you can reference by name when performing inference\\nlater on. The other two options, batch_size and batch_timeout, control the\\nbatch queue used in Nx.Serving.\\nbatch_size refers to the maximum batch size the queue will accept before\\ndispatching to predict. The batch_timeout refers to the amount of time the\\nqueue will wait to fill up to the maximum batch size before dispatching to\\npredict. In this example, your model will run inference either when it\\nreaches a batch size of 16 or after it has waited 100ms since it received the\\nfirst inference request, whichever comes first. Depending on your\\napplication, model, and resources, you might need to play with these\\nconfiguration options to maximize your performance.\\nNow that you have an Nx.Serving process running in your application, you\\ncan use it to conduct inference. To do so, add the following predict/1 method\\nto your BookSearch.Model module:\\ndef  predict(text)  do \\n  Nx.Serving.batched_run(BookSearchModel, text)\\nend \\nThis function just wraps the Nx.Serving.batched_run/2 method.\\nNx.Serving.batched_run/2 performs dynamic batch inference with the given', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 642}),\n",
       " Document(page_content='process and inputs. In this case, calls to BookSearch.Model.predict/1 will return\\nan embedded representation of the input text provided in text. To verify\\nyour model is up and running, restart your Phoenix server:\\niex -S mix phx.server\\nYou may notice a slight delay on application start-up—that’s EXLA\\nactually compiling and loading your model under the hood.\\nNext, run the following code in IEx:\\niex(1)> BookSearch.Model.predict( \"  a good book on \\nmachine learning\" )\\nAlmost instantly you’ll see the following output:\\n%{embedding: #Nx.Tensor<\\n  f32[1][384]\\n  EXLA.Backend<host:0, \\n0.2486389031.1855586325.245579>\\n  [\\n    [-0.01250604260712862, 0.027123818174004555, \\n...]\\n  ]\\n>}', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 643}),\n",
       " Document(page_content='Success! You have a working machine learning model running inside a\\nPhoenix application. You can now embed documents and queries, but you\\nstill can’t perform any search. Before you can search though, you need\\nsome seed data.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 644}),\n",
       " Document(page_content='Seeding Your Databases\\nWith your model up and running, it’s time to seed your database with the\\nlibrary’s books. Before that happens, you need to make some slight\\nmodifications to the generated book resource code.\\nFirst, modify BookSearch.Book.changeset/2 to match the following:\\n@doc false\\ndef  changeset(book, attrs)  do \\n  book\\n  |> cast(attrs, [ :author ,  :title ,  :description ,  \\n:embedding ])\\n  |> validate_required([ :author ,  :title ,  \\n:description ])\\nend \\nNotice that you removed :embedding as a required argument in attrs. Because\\nyou compute the embedding from book attributes, you don’t want to have to\\npass it up front.\\nNext, add the following put_embedding/1 function to BookSearch.Book:\\n@doc false\\ndef  put_embedding(%{ changes:  %{ description:  desc}} \\n= book_changeset)  do ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 645}),\n",
       " Document(page_content=' %{ embedding:  embedding} = \\nBookSearch.Model.predict(desc)\\n  put_change(book_changeset,  :embedding , \\nembedding)\\nend \\nThis code embeds the given book based on its description and adds the\\nembedding to the book changeset. Notice that you can use an Nx tensor\\ndirectly as a type within the changeset. The Elixir pgvector extension\\nsupports marshaling directly from Nx tensors to the pgvector postgres type.\\nNext, in BookSearch.Library, modify the create_book/1 method to match the\\nfollowing:\\ndef  create_book(attrs \\\\\\\\ %{})  do \\n  book =\\n    %Book{}\\n    |> Book.changeset(attrs)\\n    |> Book.put_embedding()\\n  Repo.insert(book)\\nend \\nThis method creates a changeset from the given book attributes, embeds the\\nbook, and then inserts the book into your database.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 646}),\n",
       " Document(page_content='As a final housekeeping measure before seeding your library, you’ll want to\\nupdate your HTML views to remove references to embeddings. Because\\nembeddings are internally represented as raw binaries, they don’t render\\nwell on a webpage. Additionally, they aren’t intended to be edited by the\\nuser, so it doesn’t make sense to include them in a form. Remove the\\nreferences to the following code in each of the following files:\\n# book_form.html.heex \\n<.input field={{f,  :embedding }} type= \"  text\"  label= \\n\"  Embedding\"  />\\n# show.html.heex \\n< :item  title= \"  Embedding\"  > <%= @book.embedding % > </ \\n:item >\\n# index.html.heex \\n< :col   :let= {book} label= \"  Embedding\"  > <%= \\nbook.embedding % > </ :col >\\nYou’re ready to work with real data. The dataset you’ll use to represent\\nyour library comes from a book summaries database from David Bamman.\\n Run the following commands to download and extract the database:\\ncd priv/repo\\n[45]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 647}),\n",
       " Document(page_content='wget \\nhttps://www.cs.cmu.edu/~dbamman/data/booksummaries.\\ntar -xvf booksummaries.tar.gz\\nrm booksummaries.tar.gz\\nThe database consists of a file of book information, where each book is on a\\ntab-delimited line. To parse this information into something you can use to\\nseed your database, you can use Explorer. Particularly, you want to use\\nExplorer to parse the input file, parse out the author, title, and description,\\nand then add each book to your database. To do this, add the following code\\nto your priv/repo/seeds.exs file:\\npath =  \"  priv/repo/booksummaries/booksummaries.txt\" \\ndf = Explorer.DataFrame.from_csv!(path,  delimiter:  \\n\"  \\\\t\" ,  header:  false)\\ndf = df[[ \"  column_3\" ,  \"  column_4\" ,  \"  column_7\" ]]\\ndf = Explorer.DataFrame.rename(df, [ \"  author\" ,  \"  \\ntitle\" ,  \"  description\" ])\\ndf\\n|> Explorer.DataFrame.to_rows()\\n|> Enum.each(&BookSearch.Library.create_book/1)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 648}),\n",
       " Document(page_content='This script parses the book summary file and extracts the author, title, and\\ndescription from each book. It then converts the DataFrame to a list of maps\\nand uses BookSearch.Library.create_book/1 to create an entry for each book.\\nNow you can run the following:\\nmix run priv/repo/seeds.exs\\nAfter a while, your script will finish running, and you’ve successfully\\nseeded your database with a library of books. Now you need to implement\\nthe search functionality.\\nFor a production application, this process of seeding your database is\\nsomething you might have to run periodically as new books are added. One\\nway to handle this might be to use Oban in conjunction with FLAME\\nto periodically execute expensive machine learning pipelines on a separate\\nmachine. With FLAME, you can spin up a GPU-enabled machine to run a\\nlong-running machine learning job such as this one without impacting your\\nproduction servers.\\n[46] [47]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 649}),\n",
       " Document(page_content='Building the Search LiveView\\nAt this point, you have a seeded database and a machine learning model\\nready to embed books. The final requirement is to implement the front-end\\nsearch application.\\nIn this example, you’ll implement the search functionality as a LiveView.\\nStart by creating a new file book_search_web/live/search_live/index.ex and add\\nthe following code:\\ndefmodule  BookSearchWeb.SearchLive.Index  do \\n   use  BookSearchWeb,  :live_view \\n  alias BookSearch.Library\\n  @impl true\\n   def  mount(_params, _session, socket)  do \\n    { :ok , socket}\\n   end \\n  @impl true\\n   def  render(assigns)  do \\n     ~ H \"\"\" \\n    <h1>Welcome to search</h1> \\n    \"\"\" ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 650}),\n",
       " Document(page_content='  end \\nend \\nThis code creates the scaffold of a basic LiveView that renders the welcome\\nheader and does nothing else.\\nNext, add a live route to your LiveView in your router:\\nlive  \"  /search\" , SearchLive.Index,  :index \\nNext, start your Phoenix application:\\niex -S mix phx.server\\nIf everything succeeded, when you navigate to https://localhost:4000/search,\\nyou’ll see the default Phoenix welcome page.\\nYour search application consists of a simple search form and search results.\\nEdit render/1 to match the following:\\n@impl true\\ndef  render(assigns)  do \\n   ~ H \"\"\" \\n  <div class=\"w-full flex flex-col space-y-2\"> \\n    <.search_form query={@query} /> \\n    <div :if={@query}> ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 651}),\n",
       " Document(page_content='     <h2 class=\"text-md\"> \\n        <span class=\"font-semibold\">Searching For:\\n</span> \\n        <span class=\"italic\"><%= @query %></span> \\n      </h2> \\n    </div> \\n    <div> \\n      <.search_results results={@results} /> \\n    </div> \\n  </div> \\n  \"\"\" \\nend \\nThis code renders the components search_form and search_results.\\nsearch_results requires an assign @results, which represents book search\\nresults. You need to edit the mount/3 method to assign results to an empty\\nlist up front so that everything renders properly:\\n@impl true\\ndef  mount(_params, _session, socket)  do \\n  { :ok , assign(socket,  :results , [])}\\nend \\nNext, you need to implement components for the search form and results.\\nFor now, you can implement them as function components:', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 652}),\n",
       " Document(page_content='defp  search_form(assigns)  do \\n   ~ H \"\"\" \\n  <div class=\"w-full\"> \\n    <form \\n      id=\"search\" \\n      phx-change=\"validate_search\" \\n      phx-submit=\"search_for_books\" \\n      class=\"w-full flex space-x-2\" \\n    > \\n      <input \\n        placeholder=\"search for a book\" \\n        type=\"text\" \\n        name=\"search\" \\n        value={@query} \\n        id=\"search\" \\n        class={[\"block w-full rounded-md border-\\ngray-300 pr-12\", \\n        \"shadow-sm focus:border-indigo-500 \\nfocus:ring-indigo-500\", \\n        \"sm:text-sm\"]} /> \\n      <button \\n        type=\"submit\" \\n        class={[\"inline-flex items-center rounded-\\nmd border\", ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 653}),\n",
       " Document(page_content='       \"border-transparent shadow-sm text-white\", \\n        \"bg-indigo-600 px-3 py-2 text-sm font-\\nmedium leading-4\", \\n        \"hover:bg-indigo-700 focus:outline-none \\nfocus:ring-2\", \\n        \"focus:ring-offset-2\"]}> \\n          Search \\n      </button> \\n    </form> \\n  </div> \\n  \"\"\" \\nend \\ndefp  search_results(assigns)  do \\n   ~ H \"\"\" \\n  <div class=\"w-full\"> \\n    <ul role=\"list\" class=\"-my-5 divide-y divide-\\ngray-200\"> \\n      <%= for result <- @results do %> \\n        <li class=\"py-5\"> \\n          <div \\n              class=\"relative focus-within:ring-2 \\n                focus-within:ring-indigo-500\" \\n            > ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 654}),\n",
       " Document(page_content='           <h3 class=\"text-sm font-semibold text-\\ngray-800\"> \\n              <a \\n                href={~p\"/books/#{result.id}\"} \\n                class=\"hover:underline \\n                focus:outline-none\" \\n              > \\n                <span class=\"absolute inset-0\" \\naria-hidden=\"true\"></span> \\n                <%= result.title %> \\n              </a> \\n            </h3> \\n            <p class=\"mt-1 text-sm text-gray-600 \\nline-clamp-2\"> \\n              <%= result.author %> \\n            </p> \\n          </div> \\n        </li> \\n      <% end %> \\n    </ul> \\n  </div> \\n  \"\"\" \\nend ', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 655}),\n",
       " Document(page_content='search_form is a relatively basic form with a single input and submit button.\\nOn submission, it fires the search_for_books event. The search_for_books event\\nwill push a patch to the URL, which you can handle in handle_params to\\nactually search for books based on the input query. search_results is a list that\\nrenders list entries for each of the elements in the results assign. As you can\\nimagine search_for_books will update the results assign, and search_results will\\nrerender with new results on every query.\\nNext, add the following event handlers to your LiveView to handle your\\nform events:\\n@impl true\\ndef  handle_event( \"  validate_search\" , %{ \"  search\"  => \\n_query}, socket)  do \\n  { :noreply , socket}\\nend \\ndef  handle_event( \"  search_for_books\" , %{ \"  search\"  => \\nquery}, socket)  do \\n  { :noreply , push_patch(socket,  to:   ~ p \"  /search?q=  #\\n{ query }  \" )}\\nend \\nFor simplicity, validate_search is a no-op. But you can implement any\\nvalidation logic you’d like. search_for_books pushes a query to the URL', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 656}),\n",
       " Document(page_content='parameters.\\nNow, implement handle_params/3, like so:\\n@impl true\\ndef  handle_params(%{ \"  q\"  => query}, _uri, socket)  \\ndo \\n  results = Library.search(query)\\n  socket =\\n    socket\\n    |> assign( :results , results)\\n    |> assign( :query , query)\\n  { :noreply , socket}\\nend \\nYou also need a clause for when no parameters are present:\\ndef  handle_params(_params, _uri, socket)  do \\n  { :noreply , assign(socket,  :query , nil)}\\nend \\nhandle_params/3 conducts a search and assigns the results to :results and the\\nquery to :query, which will both be rendered in the LiveView.\\nhandle_params/3 dispatches to Library.search, which you still need to', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 657}),\n",
       " Document(page_content='implement. To do so, open up book_search/library.ex and add the following\\nsearch/1 method:\\n@doc  \"\"\" \\nSearches for books. \\n\"\"\" \\ndef  search(query)  do \\n  %{ embedding:  embedding} = \\nBookSearch.Model.predict(query)\\n        Book\\n        |> order_by([b], l2_distance(b.embedding, \\n^embedding))\\n        |> limit(5)\\n        |> Repo.all()\\nend \\nYou’ll also need to add the following import to book_search/library.ex:\\nimport  Pgvector.Ecto.Query\\nThis function uses your model to embed the query and then searches for the\\nfive closest vectors in the search index to your query. Notice that you can\\nembed pgvector query functions such as l2_distance directly in your Ecto\\nqueries. The l2_distance function will compute the distance between your', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 658}),\n",
       " Document(page_content='query embedding and every embedding in your database and order the\\nresults by ascending distance. The lower the distance, the more similar the\\nquery and book embedding are. You limit your results to only five books\\nand return those to your application front-end.\\nAnd that’s it. You can now navigate to http://localhost:4000/search and look\\nfor a specific book. After running a search, you’ll see the following page:\\nCongratulations! You’ve successfully created a semantic search application\\nwith Phoenix, Bumblebee, and pgvector.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 659}),\n",
       " Document(page_content='Wrapping Up\\nIn this chapter, you implemented a semantic search application from scratch\\nusing Phoenix, Bumblebee, and Elixir. You went through the process of\\ndeciding when to use and not use machine learning in an application. You\\nlearned a bit about some of the important questions to ask when designing\\nmachine learning applications. You used Nx.Serving to wrap machine\\nlearning pipelines into a performant, fault-tolerant inference pipeline. You\\nlearned about the significance of dynamic batching to machine learning\\nperformance, and you saw how Nx.Serving can help you achieve dynamic\\nbatching.\\nIn addition, you implemented your own vector index and search\\nfunctionality using pgvector. In doing so, you learned a bit about vector\\nsearch.\\nFinally, you integrated all of your machine learning components into a\\nregular Phoenix application. As you might have seen, integrating machine\\nlearning into Phoenix applications doesn’t require any special dependencies\\nor tricks. All you need to do is add Elixir’s machine learning libraries to\\nyour application, and it just works.\\nWith this chapter, you’ve come full circle as an Elixir machine learning\\nengineer. You now know how to use many of the libraries in Elixir’s\\nmachine learning ecosystem to solve challenging problems on various data', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 660}),\n",
       " Document(page_content='[41]\\n[42]\\n[43]\\n[44]\\n[45]\\n[46]\\n[47]\\nmodalities, and you know how to deploy your models to a production\\napplication with Phoenix.\\nIn the next chapter, you’ll conclude your journey and discover some of the\\nnext steps to take in your machine learning journey.\\nFOOTNOTES\\nhttps://phoenixframework.org\\nhttps://github.com/pgvector/pgvector\\nhttps://github.com/pgvector/pgvector#installation-notes\\nhttps://huggingface.co/intfloat/e5-small-v2\\nhttps://people.ischool.berkeley.edu/~dbamman/\\nhttps://hexdocs.pm/oban/Oban.html\\nhttps://fly.io/blog/rethinking-serverless-with-flame/\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 661}),\n",
       " Document(page_content='Chapter 14\\nThat’s a Wrap\\n\\xa0\\nThroughout this book, you’ve implemented a number of different machine\\nlearning models in Nx, Axon, Bumblebee, and Scholar. You learned how to\\nmanipulate data with Nx, train models with Axon, and integrate them into\\nyour Phoenix applications. You discovered how to take advantage of the\\nvast ecosystem of pre-trained models with Bumblebee. You know\\neverything you need to know to create your own models on your own data\\nand integrate them into your own applications. You’re ready to take on the\\nchallenges of machine learning in the real world.\\nWhile this book covered a breadth of topics, it’s by no means\\ncomprehensive. Machine learning is a vast field. You can’t possibly learn\\neverything about machine learning from a single book. In this chapter,\\nyou’ll get up to speed with the current trends in machine learning and\\ndiscover some of the interesting machine learning topics this book wasn’t\\nable to cover.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 662}),\n",
       " Document(page_content='Learning from Experience\\nOne of the largest subfields in machine learning this book didn’t cover is\\nreinforcement learning. Reinforcement learning is concerned with creating\\nagents capable of making intelligent actions and learning from reward\\nsignals in an environment. Reinforcement learning is fundamentally\\ndifferent from both supervised and unsupervised learning. In a\\nreinforcement learning problem, there’s no concept of labeled or unlabeled\\ndata. Instead, the learning process is entirely online as an agent interacts\\nwith its environment.\\nThe most common training ground for reinforcement learning agents is\\nvideo games. For example, the Arcade Learning Environment is an\\nenvironment designed to allow researchers and hobbyists to create AI\\nagents capable of interacting with Atari 2600 games. Reinforcement\\nlearning can be credited with some of the superhuman demonstrations of\\nartificial intelligence in the last decade. For example, AlphaGo, the model\\nfrom Deep Mind which is capable of playing Go at superhuman levels, was\\ntrained with reinforcement learning.\\nIn addition to video games, reinforcement learning has proven to be a\\nfruitful paradigm in robotics. As it’s difficult to collect data and train robots\\non performing proper locomotion in a supervised manner, researchers often\\ntrain locomotion via reinforcement learning. The success of reinforcement\\nlearning has dramatically increased with a turn toward methods that\\n[48]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 663}),\n",
       " Document(page_content='combine reinforcement learning with deep learning. Most state-of-the-art\\nreinforcement learning algorithms today rely on deep models.\\nReinforcement learning is a rich field of research. If you’re interested in\\nexploring reinforcement learning more, I recommend reading\\nReinforcement Learning [SB92] by Richard S. Sutton and Andrew G. Barto.\\nAt the time of this writing, the hottest model in machine learning is\\nChatGPT.  ChatGPT is a large language model from OpenAI optimized\\nfor chat dialogue. ChatGPT is part of a growing trend of large language\\nmodels trained with reinforcement learning on human feedback (RLHF).\\nRLHF combines reinforcement learning with human ratings to optimize the\\nparameters of the model. With lots of human feedback, you end up with a\\nmodel capable of outputting text that aligns with what humans prefer.\\nLarge language models are already great at modeling text. As you learned in\\nChapter 11,  Model Everything with Transformers , large language models\\npre-trained on a causal language modeling task are great at predicting the\\nnext word in a sequence. Some research [HBDF19] has shown that as large\\nlanguage models approach the optimal likelihood for a text generation task,\\nthey tend to devolve into repetition. What that means is that large language\\nmodels are great at probabilistically modeling language. But being good at\\nmodeling language doesn’t mean you’re convincing to humans. ChatGPT\\nand related RLHF models are fine-tuned for human preference.\\n[49]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 664}),\n",
       " Document(page_content='The promise of RLHF is that we can collect enough human feedback so that\\nthese models are perfectly aligned with our preferences and capable of\\nperforming a wide range of tasks at the human level. Another promise of\\nRLHF is that large language models eventually become powerful enough to\\ngive feedback to themselves. This trend is already emerging out of research\\nlabs such as Anthropic. Recent works from Anthropic on Constitutional AI\\n[BKKA19] have shown that large language models are capable of\\ncriticizing and correcting themselves to align with a set of dictated\\npreferences. The trend towards reinforcement learning on artificial\\nintelligence feedback (RLAIF) could eliminate much of the intensive work\\nof collecting human annotations and rapidly accelerate AI research.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 665}),\n",
       " Document(page_content='Diffusing Innovation\\nAnother recent innovation this book hasn’t completely covered is the\\nsuccess of diffusion models in generative applications. As you briefly\\nlearned in  What Is the State of the Art? , latent diffusion is the process of\\nprogressively denoising over long time series. This process can be used to\\nstart from an image of random noise to generate incredibly realistic images.\\nAt their core, diffusion models are based on a process of iterative\\nrefinement. Initially, they start with a pattern of random noise. Over\\nsuccessive iterations, this noise is gradually shaped into a coherent output,\\nwhether that be an image, text, or another form of data. While their most\\npopular application is in generative art, diffusion models have been used\\nwith great success in video and speech generation. Recent work also shows\\nit’s possible to use diffusion models in text generation tasks, such as\\nmachine translation and code generation.\\nFor a more comprehensive introduction to the world of diffusion models,\\ncheck out Introduction to Diffusion Models for Machine Learning by\\nAssemblyAI.\\n[50]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 666}),\n",
       " Document(page_content='Talking to Large Language Models\\nAs large language models have become increasingly popular, an emerging\\nfield in machine learning and artificial intelligence is one that attempts to\\nimprove the performance of large language models at inference time\\nthrough prompt engineering. Prompt engineering is the process of\\nstrategically formulating input prompts to effectively guide a large language\\nmodel to generate the output you want. Large language models are great at\\nunderstanding text, but they need specific instructions to carry out a task.\\nOne of the most popular prompt engineering techniques is chain-of-thought\\nprompting. Chain-of-thought prompting involves prompting a large\\nlanguage model to break a problem into subtasks and chain them together\\ninto an output. In other words, you’re guiding a large language model to\\nshow its chain of thought before producing the output. It’s been shown that\\nchain-of-thought prompting increases the reasoning ability of large\\nlanguage models as measured on some specific benchmarks.\\nIn-context learning via retrieval augmented generation is another popular\\nmethodology for guiding large language models to provide correct\\nresponses to certain questions. Retrieval augmented generation, or RAG as\\nit’s often referred to, is the process of retrieving appropriate facts and\\ncontext for a large language model to then use in its response. The retrieval\\nprocess typically involves some sort of semantic search to take a question\\nand retrieve appropriate facts related to that question. RAG is effective, but', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 667}),\n",
       " Document(page_content='it requires a robust retrieval pipeline. For certain applications, it can be\\ndifficult to create a robust retrieval pipeline to provide sufficient context to\\nlarge language models.\\nA standing challenge with large language models is reliably deploying them\\nand using them within traditional software applications. The most popular\\napplications of large language models deploy them through chat interfaces.\\nHowever, they can be useful in many other applications as well.\\nGetting large language models to reliably interact with traditional,\\ndeterministic software is difficult. Recent work in structured prompting\\nattempts to address this issue. Structured prompting relies on some large\\nlanguage model’s ability to generate outputs according to a JSON Schema\\nspecification. OpenAI, for example, released a function-calling API that\\nallows you to specify a structured schema that their large language models\\nwould follow during generation. Open-source libraries implement similar\\nfunctionality through the use of constrained generation. This functionality\\nensures a large language model will output valid JSON that can then be\\nused in your application.\\nStructured prompting relies on the ability of large language models to\\ngenerate structured outputs. Basically, you define a schema you’d like your\\nlarge language model to follow, prompt the model, and receive a data\\nstructure that your application understands back from the model’s\\ngeneration. Libraries such as instructor_ex allow you to define Ecto[51]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 668}),\n",
       " Document(page_content='Schemas, prompt a large language model on some task, and receive a valid\\ndata structure following your defined schema as a result. This library is\\nbased on the original Instructor library from the Python ecosystem.\\nStructured prompting presents a potential bridge between Software 1.0 and\\nSoftware 2.0. It’s a way to reliably interact with large language models\\nwithin the context of your application. Rather than attempting to parse the\\nunstructured result of a large language model into something useful, you\\nreceive something useful directly from the large language model.\\n[52]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 669}),\n",
       " Document(page_content='Compressing Knowledge\\nAs a result of the large language model boom, there has been significant\\ninnovation in techniques for model compression at training and inference\\ntime. The two most prominent examples of this are quantization and low-\\nrank adaptation (LoRA).\\nQuantization is the process of converting a continuous range of values to a\\nfinite range of values. Quantization has been around for a very long time.\\nIt’s fundamental to the fields of digital signal processing. In the context of\\nmachine learning, quantization refers to the quantization of machine\\nlearning model weights. Models are typically trained in 32-bit, 16-bit (half),\\nor mixed (both) precision. Post-training quantization is the process of\\ntaking these 32-bit or 16-bit model weights and converting them to 8-bit or\\neven 4-bit integers. Quantization significantly reduces the storage and\\nmemory requirements of large language models such that they can run\\nefficiently on consumer hardware. Prior to large language models,\\nquantization was still a common technique for reducing the storage and\\nmemory requirements of neural networks on embedded hardware.\\nRecent innovations have also made it possible to train quantized\\nrepresentations of models. Training large language models is a memory-\\nhungry process. Quantization reduces the memory requirements of training\\nsuch that you can fine-tune large models on specific tasks on consumer\\nhardware.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 670}),\n",
       " Document(page_content='Low-rank adaptation is a technique for reducing the memory requirements\\nof fine-tuning pre-trained large language models. It works by reducing the\\nnumber of parameters required to fine-tune a pre-trained large language\\nmodel. Rather than updating a model’s full weights, you initialize and train\\na small number of adapters on specific layers. This significantly reduces\\nmemory requirements and facilitates fine-tuning large language models on\\nconsumer hardware.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 671}),\n",
       " Document(page_content='Moving Forward\\nAs you move forward into the wonderful world of machine learning, you\\nmight find it’s difficult to keep up with the intense pace of advancement. It\\nseems that every week there’s a new research paper or mind-blowing demo.\\nIt can be hard to determine what’s worth exploring and what’s worth\\nignoring. Fortunately, there are resources out there for keeping up.\\nIf you’re interested in keeping up with the state of the art in machine\\nlearning research, arxiv-sanity is an excellent tool for filtering through\\ntrending papers in machine learning. As you get your start in machine\\nlearning, I recommend spending some time trying to dissect research\\npapers. It can be a difficult task, but you’ll learn a ton in the process.\\nIn order to stay up-to-date with trending models and advancements in\\nimplementations, HuggingFace does an excellent job documenting\\ntrending models and staying at the cutting edge of machine learning that\\nworks. If you don’t want to dive into the academic details of a research\\npaper, I recommend at least following along with the open-source work\\ncoming out of HuggingFace and available on the HuggingFace hub.\\nFinally, no matter what machine learning work you choose to do in the\\nfuture, I hope you consider doing it in Elixir. As you’ve seen throughout\\nthis book, Elixir is an excellent language for building idiomatic, concurrent,\\nand performant machine learning applications. And because you’ve read\\n[53]\\n[54]', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 672}),\n",
       " Document(page_content='this book, you’re well on your way to becoming an expert on machine\\nlearning in Elixir.\\nThe Elixir machine learning ecosystem has made great strides in the past\\nthree years. The ecosystem now includes the following:\\nData Processing and Exploration (Explorer)\\nData Visualization (Vega, Tucan)\\nNumerical Computation (Nx)\\nTraditional Machine Learning (Scholar)\\nDecision Trees (EXGBoost)\\nNeural Networks (Axon)\\nPre-trained models and machine learning tasks (Bumblebee)\\nCode Notebooks (Livebook)\\nCombined with the incredible foundation of libraries and capabilities Elixir\\noffers out of the box, machine learning on the BEAM is incredibly\\npromising. With the emergence of libraries such as FLAME and the\\ncontinual growth of the machine learning ecosystem, Elixir will continue to\\ngrow in popularity as a choice for production machine learning\\napplications.\\nWhile Python certainly has a first-mover advantage, and a large ecosystem\\nof researchers and innovators to support it, the promise of Elixir is enticing\\nfor small teams looking to build and scale on proven technology.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 673}),\n",
       " Document(page_content='[48]\\n[49]\\n[50]\\n[51]\\n[52]\\n[53]\\n[54]\\nAdditionally, the ability to interop and take advantage of pre-trained models\\nfrom Python makes the decision to switch from Python to Elixir much\\neasier.\\nThe future of machine learning is bright—and so is your future in machine\\nlearning. Don’t stop here.\\nFOOTNOTES\\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\\nhttps://chat.openai.com/chat\\nhttps://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/\\nhttps://github.com/thmsmlr/instructor_ex\\nhttps://github.com/jxnl/instructor\\nhttps://arxiv-sanity-lite.com/\\nhttps://huggingface.co\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 674}),\n",
       " Document(page_content='', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 675}),\n",
       " Document(page_content='[Bis06]\\n[BKKA19]\\n[CMFg20]\\n[DFO20]\\nBibliography\\n\\xa0\\nChristopher M. Bishop. Pattern Recognition and Machine\\nLearning. Springer, New York, NY, 1, 2006.\\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda\\nAskell, Jackson Kernion, Andy Jones, Anna Chen, Anna\\nGoldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine\\nOlsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep\\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared\\nMueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamil\\nLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas\\nSchiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin\\nLarson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk,\\nStanislav Fort, Tamera Lanham, Timothy Tellen-Lawton, Tom Conerly,\\nTom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dobbs,\\nBen Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom\\nBrown, and Jared Kaplan. The Curious Case of Neural Text\\nDegeneration. International Conference on Learning Representations,\\n2020. 2019.\\nBeidi Chen, Tharun Medini, James Farwell, sameh gabriel,\\nCharlie Tai, and Anshumali Shrivastava. SLIDE : In Defense\\nof Smart Algorithms over Hardware Acceleration for Large-Scale Deep\\nLearning Systems. Proceedings of Machine Learning and Systems 2.\\n2020.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 676}),\n",
       " Document(page_content='[GBC16]\\n[Gon16]\\n[Gos21]\\n[GW08]\\n[HBDF19]\\n[Hoo21]\\n[Huy22]\\n[KSH12]\\n[Lef19]\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong.\\nMathematics For Machine Learning. Cambridge University Press,\\nCambridge, United Kingdom, 1, 2020.\\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep\\nLearning. MIT Press, Cambridge, MA, 1, 2016.\\nLovedeep Gondara. Medical image denoising using\\nconvolutional denoising autoencoders. Fourth Workshop on\\nData Mining in Biomedical Informatics and Healthcare at ICDM.\\n2016.\\nSvilen Gospodinov. Concurrent Data Processing in Elixir.\\nThe Pragmatic Bookshelf, Dallas, TX, 2021.\\nAndreas Griewank and Andrea Walther. Evalauting\\nDerivatives: Principles and Techniques of Algorithmic\\nDifferentiation. Society for Industrial and Applied Mathematics,\\nUniversity City, Philadelpha, PA, 2, 2008.\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\\nChoi. The Curious Case of Neural Text Degeneration.\\nInternational Conference on Learning Representations, 2020. 2019.\\nSara Hooker. The Hardware Lottery. Communications of the\\nACM. 64[12]:58--65, 2021.\\nChip Huyen. Designing Machine Learning Systems.\\nO’Reilly & Associates, Inc., Sebastopol, CA, 1, 2022.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.\\nImageNet Classification with Deep Convolutional Neural\\nNetworks. Conference on Neural Information Processing Systems.\\n2012.\\nMelanie Lefkowitz. Professor’s perceptron paved the way for\\nAI – 60 years too soon. Cornell Chronicle. 2019.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 677}),\n",
       " Document(page_content='[LTHC17]\\n[Mac16]\\n[Met21]\\n[Mit97]\\n[Mor21]\\n[Mur22]\\n[PP08]\\n[RDGF16]\\n[RFB15]\\nChristian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,\\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\\nAlykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-\\nRealistic Single Image Super-Resolution Using a Generative Adversarial\\nNetwork. IEEE Conference on Computer Vision and Pattern\\nRecognition. 2017.\\nDougal Maclaurin. Modeling, Inference, and Optimization\\nwith Composable Differentiable Procedures. Harvard\\nLibrary. 2016.\\nCade Metz. Genius Makers: The Mavericks Who Brought\\nAI to Google, Facebook, and the World. Dutton, New York,\\nNY, 1, 2021.\\nTom M. Mitchell. Machine Learning. McGraw-Hill,\\nEmeryville, CA, 1, 1997.\\nSean Moriarity. Genetic Algorithms in Elixir. The Pragmatic\\nBookshelf, Dallas, TX, 1, 2021.\\nKevin P. Murphy. Probabilistic Machine Learning. MIT\\nPress, Cambridge, MA, 1, 2022.\\nK.B. Petersen and M.S. Pedersen. The Matrix Cookbook.\\nTechnical University of Denmark. 2008.\\nJoseph Redmon, Santosh Divvala, Ross Girschick, and Ali\\nFarhadi. You Only Look Once: Unified, Real-Time Object\\nDetection. IEEE Conference on Computer Vision and Pattern\\nRecognition. 2016.\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\\nNet: Convolutional Networks for Biomedical Image\\nSegmentation. Medical Image Computing and Computer-Assisted\\nIntervention. 2015.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 678}),\n",
       " Document(page_content='[SB92]\\n[SFF19]\\n[Sha48]\\n[SHKS14]\\n[SSWS17]\\n[Str16]\\n[VSPU17]\\n[WR74]\\nRichard S. Sutton and Andrew Barto. Reinforcement\\nLearning: An Introduction. MIT Press, Cambridge, MA, 1,\\n1992.\\nEdirlei Soares de Lima, Bruno Feijó, and Antonio L.\\nFurtado. Procedural Generation of Quests for Games Using\\nGenetic Algorithms and Automated Planning. XVIII Brazilian\\nSymposium on Computer Games and Digital Entertainment. 2019.\\nClaude Shannon. A mathematical theory of communication.\\nThe Bell system technical journal. 1948.\\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\\nSutskever, and Ruslan Salakhutdinov. A Simple Way to\\nPrevent Neural Networks from Overfitting. Journal of Machine\\nLearning Research. 2014.\\nThomas Schlegl, Philipp Seeböck, Sebastian M. Waldstein,\\nUrsula Schmidt-Erfurth, and Georg Langs. Unsupervised\\nAnomaly Detection with Generative Adversarial Networks to Guide\\nMarker Discovery. Information Processing in Medical Imaging\\n(IPMI). 2017.\\nGilbert Strang. Introduction to Linear Algebra. Wellesley\\nCambridge Press, 7 Southgate Rd, Wellesley, MA 02482, 5,\\n2016.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\\nIllia Polosukhin. Attention is All You Need. Conference on Neural\\nInformation Processing Systems. 2017.\\nJ.H. Wilkinson and C. Reinsch. Handbook for Automatic\\nComputation: Linear Algebra. Springer, New York, NY, 1,\\n1974.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 679}),\n",
       " Document(page_content='Thank you!\\nWe hope you enjoyed this book and that you’re already thinking about what\\nyou want to learn next. To help make that decision easier, we’re offering\\nyou this gift.\\nHead on over to https://pragprog.com right now, and use the coupon code\\nBUYANOTHER2024 to save 30% on your next ebook. Offer is void where\\nprohibited or restricted. This offer does not apply to any edition of The\\nPragmatic Programmer ebook.\\nAnd if you’d like to share your own expertise with the world, why not\\npropose a writing idea to us? After all, many of our best authors started off\\nas our readers, just like you. With up to a 50% royalty, world-class editorial\\nservices, and a name you trust, there’s nothing to lose. Visit\\nhttps://pragprog.com/become-an-author/ today to learn more and to get\\nstarted.\\nWe thank you for your continued support, and we hope to hear from you\\nagain soon!\\nThe Pragmatic Bookshelf\\nCopyright © 2024, The Pragmatic Bookshelf.', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 680}),\n",
       " Document(page_content='Genetic Algorithms in Elixir\\nFrom finance to artificial intelligence, genetic algorithms\\nare a powerful tool with a wide array of applications. But\\nyou don’t need an exotic new language or framework to\\nget started; you can learn about genetic algorithms in a\\nlanguage you’re already familiar with. Join us for an in-\\ndepth look at the algorithms, techniques, and methods\\nthat go into writing a genetic algorithm. From introductory problems to\\nreal-world applications, you’ll learn the underlying principles of problem\\nsolving using genetic algorithms.\\nSean Moriarity\\n(242 pages) ISBN: 9781680507942 $39.95\\nGenetic Algorithms and Machine Learning for Programmers\\nSelf-driving cars, natural language recognition, and online recommendation\\nengines are all possible thanks to Machine Learning. Now you can create\\nyour own genetic algorithms, nature-inspired swarms, Monte Carlo\\nsimulations, cellular automata, and clusters. Learn how to test your ML\\nYou May Be Interested In…\\nSelect a cover for more information', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 681}),\n",
       " Document(page_content='code and dive into even more advanced topics. If you are\\na beginner-to-intermediate programmer keen to\\nunderstand machine learning, this book is for you.\\nFrances Buontempo\\n(234 pages) ISBN: 9781680506204 $45.95\\nProgramming Machine Learning\\nYou’ve decided to tackle machine learning — because\\nyou’re job hunting, embarking on a new project, or just\\nthink self-driving cars are cool. But where to start? It’s\\neasy to be intimidated, even as a software developer. The\\ngood news is that it doesn’t have to be that hard. Master\\nmachine learning by writing code one line at a time,\\nfrom simple learning programs all the way to a true deep learning system.\\nTackle the hard topics by breaking them down so they’re easier to\\nunderstand, and build your confidence by getting your hands dirty.\\nPaolo Perrotta\\n(340 pages) ISBN: 9781680506600 $47.95\\nCraft GraphQL APIs in Elixir with Absinthe', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 682}),\n",
       " Document(page_content='Your domain is rich and interconnected, and your API\\nshould be too. Upgrade your web API to GraphQL,\\nleveraging its flexible queries to empower your users,\\nand its declarative structure to simplify your code.\\nAbsinthe is the GraphQL toolkit for Elixir, a functional\\nprogramming language designed to enable massive\\nconcurrency atop robust application architectures. Written by the creators of\\nAbsinthe, this book will help you take full advantage of these two\\ngroundbreaking technologies. Build your own flexible, high-performance\\nAPIs using step-by-step guidance and expert advice you won’t find\\nanywhere else.\\nBruce Williams and Ben Wilson\\n(302 pages) ISBN: 9781680502558 $47.95\\nFunctional Web Development with Elixir, OTP, and Phoenix\\nElixir and Phoenix are generating tremendous\\nexcitement as an unbeatable platform for building\\nmodern web applications. For decades OTP has helped\\ndevelopers create incredibly robust, scalable applications\\nwith unparalleled uptime. Make the most of them as you\\nbuild a stateful web app with Elixir, OTP, and Phoenix.\\nModel domain entities without an ORM or a database. Manage server state', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 683}),\n",
       " Document(page_content='and keep your code clean with OTP Behaviours. Layer on a Phoenix web\\ninterface without coupling it to the business logic. Open doors to powerful\\nnew techniques that will get you thinking about web development in\\nfundamentally new ways.\\nLance Halvorsen\\n(218 pages) ISBN: 9781680502435 $45.95\\nMetaprogramming Elixir\\nWrite code that writes code with Elixir macros. Macros\\nmake metaprogramming possible and define the\\nlanguage itself. In this book, you’ll learn how to use\\nmacros to extend the language with fast, maintainable\\ncode and share functionality in ways you never thought\\npossible. You’ll discover how to extend Elixir with your\\nown first-class features, optimize performance, and create domain-specific\\nlanguages.\\nChris McCord\\n(128 pages) ISBN: 9781680500417 $17\\nFrom Ruby to Elixir', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 684}),\n",
       " Document(page_content='Elixir will change the way you think about\\nprogramming. Use your Ruby experience to quickly get\\nup to speed so you can see what all of the buzz is about.\\nGo from zero to production applications that are reliable,\\nfast, and scalable. Learn Elixir syntax and pattern\\nmatching to conquer the basics. Then move onto Elixir’s\\nunique process model that offers a world-class way to go parallel without\\nfear. Finally, use the most common libraries like Ecto, Phoenix, and Oban\\nto build a real-world SMS application. Now’s the time. Dive in and learn\\nElixir.\\nStephen Bussey\\n(222 pages) ISBN: 9798888650318 $48.95\\nProgramming Elixir 1.6\\nThis book is the introduction to Elixir for experienced\\nprogrammers, completely updated for Elixir 1.6 and\\nbeyond. Explore functional programming without the\\nacademic overtones (tell me about monads just one more\\ntime). Create concurrent applications, but get them right\\nwithout all the locking and consistency headaches. Meet\\nElixir, a modern, functional, concurrent language built on the rock-solid\\nErlang VM. Elixir’s pragmatic syntax and built-in support for', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 685}),\n",
       " Document(page_content='metaprogramming will make you productive and keep you interested for the\\nlong haul. Maybe the time is right for the Next Big Thing. Maybe it’s Elixir.\\nDave Thomas\\n(410 pages) ISBN: 9781680502992 $47.95', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 686})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcff2636-b89b-4241-90d5-bb5b685ca7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Machine Learning in Elixir\\nLearning to Learn with Nx and Axon\\nby Sean Moriarity\\nVersion: P1.0 (September 2024)', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 1}),\n",
       " Document(page_content=\"Copyright © 2024 The Pragmatic Programmers, LLC. This book is licensed to the individual who purchased it. We don't copy-\\nprotect it because that would limit your ability to use it for your own purposes. Please don't break this trust—you can use this\\nacross all of your devices but please do not share this copy with other members of your team, with friends, or via file sharing\\nservices. Thanks.\\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those\\ndesignations appear in this book, and The Pragmatic Programmers, LLC was aware of a trademark claim, the designations have\\nbeen printed in initial capital letters or in all capitals. The Pragmatic Starter Kit, The Pragmatic Programmer, Pragmatic\\nProgramming, Pragmatic Bookshelf and the linking g device are trademarks of The Pragmatic Programmers, LLC.\", metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 2}),\n",
       " Document(page_content='Programming, Pragmatic Bookshelf and the linking g device are trademarks of The Pragmatic Programmers, LLC.\\nEvery precaution was taken in the preparation of this book. However, the publisher assumes no responsibility for errors or\\nomissions, or for damages that may result from the use of information (including program listings) contained herein.\\nAbout the Pragmatic Bookshelf\\nThe Pragmatic Bookshelf is an agile publishing company. We’re here because we want to improve the lives of developers. We do\\nthis by creating timely, practical titles, written by programmers for programmers.\\nOur Pragmatic courses, workshops, and other products can help you and your team create better software and have more fun. For\\nmore information, as well as the latest Pragmatic titles, please visit us at http://pragprog.com.\\nOur ebooks do not contain any Digital Restrictions Management, and have always been DRM-free. We pioneered the beta book', metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 2}),\n",
       " Document(page_content=\"Our ebooks do not contain any Digital Restrictions Management, and have always been DRM-free. We pioneered the beta book\\nconcept, where you can purchase and read a book while it’s still being written, and provide feedback to the author to help make a\\nbetter book for everyone. Free resources for all purchasers include source code downloads (if applicable), errata and discussion\\nforums, all available on the book's home page at pragprog.com. We’re here to make your life easier.\\nNew Book Announcements\\nWant to keep up on our latest titles and announcements, and occasional special offers? Just create an account on pragprog.com (an\\nemail address and a password is all it takes) and select the checkbox to receive newsletters. You can also follow us on twitter as\\n@pragprog.\\nAbout Ebook Formats\\nIf you buy directly from pragprog.com, you get ebooks in all available formats for one price. You can synch your ebooks amongst\", metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 2}),\n",
       " Document(page_content=\"@pragprog.\\nAbout Ebook Formats\\nIf you buy directly from pragprog.com, you get ebooks in all available formats for one price. You can synch your ebooks amongst\\nall your devices (including iPhone/iPad, Android, laptops, etc.) via Dropbox. You get free updates for the life of the edition. And,\\nof course, you can always come back and re-download your books when needed. Ebooks bought from the Amazon Kindle store\\nare subject to Amazon's polices. Limitations in Amazon's file format may cause ebooks to display differently on different devices.\\nFor more information, please see our FAQ at pragprog.com/#about-ebooks. To learn more about this book and access the free\\nresources, go to https://pragprog.com/book/smelixir, the book's homepage.\", metadata={'source': 'machine-learning-elixir-nx-axon.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c28b5a-116c-4587-ab2c-463639d68967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Transformation, vector Embedding and Vector Store USING CHROMA DB\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents[:30], embedding = OpenAIEmbeddings())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2aa8f1-95ba-43ed-a9b2-0dbe59364486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1c0f84ee250>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc42f8d1-54d3-40ea-97f1-e0673faf3064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preface\\n\\xa0\\nMachine learning, specifically deep learning, is constantly pushing the\\nboundaries of what we thought was possible—and it’s doing it in every\\nindustry.\\nSeemingly every day, a company or research group releases a new model\\nthat pushes the state of the art even further ahead. In recent years, ChatGPT\\nand Stable Diffusion, among others, have taken the world by storm,\\nbringing artificial intelligence to the forefront and redefining what types of\\napplications are possible.\\nFor most of their existence, Elixir and the BEAM weren’t viable options for\\nmachine learning tooling. But the Nx ecosystem has changed that. Within\\nthis ecosystem, you can now write machine learning and numerical routines\\ndirectly in Elixir and achieve performance that is the same as or better than\\nan equivalent program in Python or Julia. Nx offers new capabilities to\\nElixir programmers. It also provides an off-ramp for existing machine\\nlearning engineers and researchers looking to explore ecosystems and build'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHROMA Vector database\n",
    "query = \"what is machine learning\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "266169c6-e843-4db2-ad49-6b0878279c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_validator' from 'pydantic' (C:\\Users\\Sur Ox\\anaconda3\\Lib\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Utilising Ollama for RAG\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load Ollama llama2 version model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\__init__.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, Type\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn_deprecated\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLLM\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_import_ai21\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Type[BaseLLM]:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai21\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AI21\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:24\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     16\u001b[0m     Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     cast,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigDict, Field, model_validator\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtenacity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     RetryCallState,\n\u001b[0;32m     27\u001b[0m     before_sleep_log,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     wait_exponential,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'model_validator' from 'pydantic' (C:\\Users\\Sur Ox\\anaconda3\\Lib\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "# Utilising Ollama for RAG\n",
    "from langchain_community.llms import Ollama\n",
    "# Load Ollama llama2 version model\n",
    "llm = Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97339e47-5798-4842-ac14-5f5d62b382aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\sur ox\\anaconda3\\lib\\site-packages (2.11.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sur ox\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sur ox\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\sur ox\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sur ox\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"pydantic>=2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fab79-4ed9-4530-b622-9496b6f4df57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4a250-13c2-4bf3-a150-8ab278e6c02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79ac15-6361-4ce0-b8bd-2ed72e09e9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829212ae-23db-4706-80bb-68798ad454cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669df0a-93e5-4fc9-84c1-ce5ea3bc70a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
